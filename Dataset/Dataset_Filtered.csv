No.,Company Name,Country,Industry,Published Year,Last Revised,Link,Document Name,Main Text,Comment
1,Accenture,Ireland,Consulting,03-30-2021,03-30-2021,https://www.accenture.com/content/dam/accenture/final/a-com-migration/pdf/pdf-149/accenture-responsible-ai-final.pdf#zoom=50,Responsible AI From principles to practice,"Responsible AI
From principles to practice
Contents
Responsible AI in practice—
essential but not easy
03
• A growing imperative
• Practitioners’ insights—the realities of Responsible AI
05 Moving from principles to practice
• The four pillars of Responsible AI
• Pain points, recommendations and case studies
• Organizational
• Operational
• Technical
• Reputational
14 The next step—from practice to proof
Artificial intelligence (AI) has the potential to add trillions to the
global economy. And the benefits it can deliver to the enterprise
are well documented. However, Accenture research found that to
deliver on the promise of this technology, AI must be scaled across
the organization.
Responsible AI 03
Successfully scaling AI for business value requires a number of key factors. These include a focus on
value, new-skilling and scaling to production. But often, a stumbling block that organizations struggle
to overcome is the significant uncertainty and risks associated with AI. Trust inside and outside of the
organization is a key component to getting to value from AI. And to get to trust, organizations must
move beyond defining Responsible AI principles and put those principles into practice.
Accenture has worked with organizations worldwide to build this trust. How? By defining and
implementing solutions across four Responsible AI pillars—moving from principles to practice.
In this report we share what we have learned—from practitioners’ pain points and how to address
them, to case studies of what good looks like in the real world.
From principles to practice
The potential value organizations can achieve through AI is clear. So is the danger of being left behind
by competitors if the technology is not leveraged. But many businesses feel overwhelmed trying to
determine how to address the risks associated with it. In a global survey of risk managers, 58% identify
AI as the biggest potential cause of unintended consequences over the next two years. Only 11%
describe themselves as fully capable of assessing the risks associated with adopting AI
organization-wide.
One of the main reasons for this crisis of confidence? The way AI shifts the risk landscape in new and
unexpected ways for organizations. As AI decisions increasingly influence and impact people’s lives at
scale, so responsibility on enterprises increases to manage the potential ethical and socio-technical
implications of AI adoption.
In particular, bias, discrimination and fairness have emerged as areas of paramount concern, alongside
explainability. Recent academic advances in algorithmic fairness focus on a variety of very specific
definitions of these problem areas, which can be highly contextual, mutually exclusive and
contradictory. Translating these into action is fraught with tough decisions, trade-offs and
application-specific constraints, requiring multiple cross-domain perspectives to reach consensus.
Responsible AI in practice—essential but
not easy
A growing imperative

Responsible AI From principles to practice 04
Shareholders, regulators, media, employees and the public are increasingly aware of the
positive impact that scaling AI can have. But it is also clear that the potential for significant
damage exists if Responsible AI isn’t included in an organization’s approach.
In response, many enterprises have started to act
(or in other words, to Professionalize their approach
to AI and data). Those that have put in place the right
structures from the start, including considering
Responsible AI, are able to scale with confidence,
achieving nearly three times the return on their AI
investments when compared to those that have not.
However, there are also multiple examples of
organizations which have taken initial steps to
establish AI ethics principles and well-meaning
proofs of concept but struggle to scale these
concepts into their live processes. Which begs
the questions: why is it so hard to go from
principles to practice? What are the common
pitfalls and how can we address them?
Responsible AI
We define Responsible
AI as the practice of
designing, building
and deploying
AI in a manner that
empowers employees
and businesses
and fairly impacts
customers and society.
To better understand why organizations struggle to move from principles to practice, Accenture
conducted a global Responsible AI Practitioners' Survey—26 in-depth interviews with Responsible AI
practitioners from 19 organizations, across four continents. We spoke with technological practitioners
(data scientists and AI engineers), lawyers, industrial/organizational psychologists, and project
managers, all of whom are charged with implementing Responsible AI practices in their organizations.
Our analysis indicates that given the embryonic nature of Responsible AI, some organizations
have struggled to develop a systematic internal approach to convert their principles into practice.
Our experience shows that this is because they underestimate the technical complexity and
scale of people and process change required.
Approximately 75% of the Responsible AI projects referred to in these interviews were more than a year
old. Yet, in almost all cases, efforts had stagnated or remained incomplete due to a range of problems.
In the report, we will analyze the problems we uncovered from the survey results, and provide
recommendations for how to move forward.
Practitioners’ insights—the realities of Responsible AI
Responsible AI From principles to practice 05
To successfully move from Responsible AI principles to practice, organizations need to tackle a central
challenge: translating ethical principles and academic theories like algorithmic fairness into practical,
measurable metrics and thresholds that are right for them. To embed these responsible approaches
and metrics into everyday practices, organizations will also need to put in place the necessary
organizational, technical, operational, and reputational scaffolding.
We’ve defined four pillars of successful Responsible AI implementations, based on what we’ve learned
delivering Responsible AI solutions to organizations across the globe.
Moving from principles to practice
e four pillars of Responsible AI
Pain points, recommendations and case studies
For each pillar of Responsible AI, we have summarized the key practitioner pain points from our survey,
provided recommendations to address these pain points based on our implementation experience,
and included a case study to demonstrate what good looks like.
In our experience, many organizations begin their journey by focusing on one issue, such as
algorithmic fairness or compliance. However, those that are most successful understand the
importance of investing in all four pillars that underpin Responsible AI from the very start.
Democratize the new way
of working and facilitate
human + machine
collaboration.
Organizational
01
Set up governance and
systems that will enable
AI to flourish.
Operational
02
Help ensure systems
and platforms are
trustworthy and
explainable by design.
Technical
Articulate the Responsible
AI mission and ensure
it’s anchored to your
company’s values,
ethical guardrails, and
accountability structure.
Reputational
03 04
The four pillars of Responsible AI
01 Organizational
Practitioner pain points
Practitioner interviewees regularly highlighted the need for
appropriate performance metrics and recognition of their
Responsible AI work. They indicated that little value was being
placed today on risk mitigation, including prevention of reputational
harm, while time pressures meant short-term product success
was prioritized ahead of the long-term benefits of Responsible AI.
This often came from the top, with interviewees highlighting an
unwillingness by leadership to recognize Responsible AI failures
and deeply engage with issues like lack of fairness, transparency,
and the potential for discrimination.
Interviewees felt that as organizations create new Responsible
AI duties and roles, a culture of confidence must be built across
the enterprise, empowering employees to raise concerns and act
ethically through the right combination of responsible product
and individual success metrics and incentive structures.
What we recommend
Strong leadership is pivotal in empowering employees, elevating
Responsible AI as a key business imperative. To democratize this
new way of working, successful organizations recognize the need
for new and changing roles, and actively upskill, re-skill, or hire.
In our experience, organizations should actively create and
encourage an organizational culture that empowers individuals to
raise doubts or concerns with AI systems, without stifling innovation.
Establishing clear success criteria, incentives and training helps to
nurture these new roles and skills, diffusing a Responsible AI culture
across the organization and cultivating trust in AI systems.
Responsible AI From principles to practice 06
Responsible AI From principles to practice 07
Leadership plays an essential role in the successful
adoption of Responsible AI. A leading European
financial services organization wanted to explore the
use of algorithmic fairness across their model
inventory. Strong buy-in and support at the executive
level enabled a culture of openness and
collaboration, where an internal, multidisciplinary
team were free to work with Accenture to learn and
explore algorithmic fairness, raising any concerns in
a safe environment.
A strong focus was initially placed on education
and upskilling, ensuring all team members fully
understood algorithmic fairness, the choices
and trade-offs that need to be made, and how to
utilize Accenture’s Algorithmic Assessment toolkit.
This foundation of support and knowledge removed
many of the fears and orthodoxies that often
exist around bias and fairness, creating an open,
collaborative culture. Individuals were empowered
to ask the hard questions, with potential sources
of bias thoroughly investigated. In undertaking this
process, the organization now has the training and
skills needed to implement the latest advances in
the algorithmic fairness space.
Putting organizational
principles into practice
Case study
Practitioner pain points
Practitioners interviewed as part of our research indicated that
companies consistently struggled with stakeholder misalignment,
frustrating bureaucracy, conflicting agendas and a lack of clarity
on processes or ownership. For example, individuals operated in an
ad hoc nature based on their own values and personal assessment
of relative importance. A lack of clarity on governance and
accountability structures, undue conflict and competing
incentives across groups ultimately led to Responsible AI
inertia and a reactive mindset.
What we recommend
Effective organizations establish transparent, cross-domain,
governance structures. These structures identify roles,
expectations and accountability to build internal confidence and
trust in AI technologies. Throughout the project lifecycle, these
organizations transform ethical principles into clear processes,
procedures and chains of command that respect the
context-specific needs of each application.
Accenture has found that the creation of a cross-domain ethics
committee at an early stage has been invaluable for some
organizations. In clearly defining roles and expertise, ways of
working and authority to govern, procedures can be maintained on
an ongoing basis, while also enabling on-demand responses as
issues arise.
02 Operational
Responsible AI From principles to practice 08
Responsible AI From principles to practice 09
Putting operational principles into practice
Following the release of EU AI Trustworthy Guidelines, one global communications vendor asked
Accenture to help develop their own internal ethical principles and translate them into operational
actions, activities, and structures. This would give them a practical Responsible AI risk management
foundation on which to build their future AI pipeline with confidence.
Over the course of seven weeks, Accenture helped to define a governance framework,
with four key deliverables:
In particular, the playbook enabled our client to navigate the difficult path of principles to practice.
For each key activity along a project lifecycle, the playbook makes clear the activities to be completed,
the team members required to take action and the stakeholders responsible for successful completion.
Each of these activities are aligned with one or more of the client’s ethical principles, ensuring that the
right principles are being considered and acted on accordingly, at each stage. The committee
supports day-to-day activity, resolving any alerts or escalations, while also running check-ins at each
phase of the AI lifecycle.
Responsible AI
principles defined
and detailed in an
actionable playbook.
Principles and
playbook
New committee
created with clearly
defined roles, ways of
working and authority
to govern.
Responsible AI
commiee
Personalized training
plans built for all team
members based on their
roles and Responsible AI
responsibilities.
Training
Structures and channels
established to safely
escalate risks and ways
to actively foster dissent.
Monitoring
Case study
Practitioner pain points
One of the biggest barriers that practitioner’s organizations faced
was a lack of expertise in defining and measuring ethical use and
algorithmic impact of data, models and model outcomes on an
ongoing basis. Without established technical methods to identify,
mitigate and monitor these risks, organizations cannot be confident
that the system is fair and safe.
Interviewees also indicated that their organizations struggled to
integrate academic metrics like algorithmic fairness, finding them
very different from traditional benchmarks and KPIs. Responsible AI
cannot be measured in revenue generation or click-through rates,
yet organizations still relied on performance metrics like these to
define the success or failure of Responsible AI practitioners.
What we recommend
Successful organizations architect and deploy AI models, systems
and platforms that are trustworthy, fair and explainable by design.
Using proven qualitative and quantitative techniques to assess
potential risks, they are better placed to reach cross-domain
consensus on mitigation strategies. They also clearly define
measurable performance metrics and establish techniques for
ongoing monitoring, control and re-assessment.
It is important for organizations to invest time in fully understanding
the sources of bias in their various systems. In our experience, this
process leads to better informed resolution strategies that match
the organization and the application. Having the right set of tools
to thoroughly investigate sources of bias and understand the
trade-offs and impacts of fairness decisions is invaluable in
reaching common cross-domain consensus.
Responsible AI From principles to practice 10
03 Technical
Responsible AI From principles to practice 11
Putting technical
principles into practice
Allied Irish Bank (AIB), wanted to ensure they were
ahead of the industry and bring their data-science
teams rapidly up to speed on the latest developments
in algorithmic fairness, while further enhancing the
integration of algorithmic fairness assessment in
the models used to aid their decision-making.
Working with the bank’s data science team, we leveraged
Accenture’s Algorithmic Assessment toolkit to assess
fairness and actions needed to mitigate bias in two
new models that were in development.
At relevant points in the model development workflow,
analyses were surfaced to a multidisciplinary group. By
breaking this complex problem down into manageable,
understandable “chunks”, the tools gave data scientists
and business executives a deeper understanding of their
data and model outcomes from a fairness perspective.
They then directed further investigation into areas of
potential bias. This methodology informed and improved
decision-making during the model-build process.
We enabled AIB to integrate a data-driven assessment
of the complex problem of algorithmic fairness in the
end-to-end model lifecycle. The bank’s data science
teams are now self-sufficient on the tool, and are able
to independently use it in their ongoing work to affirm
confidence and a deeper understanding of their models.
Case study
Practitioner pain points
Without the necessary organizational, operational, and technical
foundations in place, interviewees reported that organizations are
forced into a reactive approach to Responsible AI, leaving them
exposed to significant reputational damage. Interviewees indicated
that in the absence of clear legal requirements, they had used
reputational risk (in the form of catastrophic media attention)
as a way of incentivizing change.
What we recommend
Leaders in this field clearly articulate their Responsible Business
mission, anchored in their principles and informed by brand,
public risk assessments and guidance. Accurate, ongoing
measurement and monitoring of key Responsible AI metrics—
such as fairness—ensures they are managing risk and able to
communicate with confidence and transparency.
In some cases, internal stakeholders can be skeptical of the value of
ethical principles or risk averse in communicating these externally.
Successful organizations embrace these internal skeptics and the
fresh perspective they bring, encouraging the core team to
pressure-test the principles they’re defining. The result is often
a more considered, precise set of principles in which everyone
has confidence. Responsible AI is a cross-domain challenge
and consensus is vital to build a culture of confidence and
enable trust in the technology.
Responsible AI From principles to practice 12
04 Reputational
Responsible AI From principles to practice 13
Putting reputational principles into practice
Establishing a Responsible AI approach that is robust, fair and maintained on an ongoing
basis can also enable organizations to communicate and collaborate with confidence.
A major government agency wanted to apply an “ethics-by-design” approach to the algorithms
it utilizes to serve citizens. The agency wanted to have a clear view of potential bias present in
the data, model, and output to identify improvement points and make sure the algorithms it
used were treating all people in a fair way.
Working together, we established an end-to-end approach to Responsible AI, combining
our Algorithmic Assessment toolkit with strong governance structures, ensuring continued
monitoring and evaluation of fairness over time. A multidisciplinary team was also established
to ensure adherence to ethical principles, as well as serving as a mechanism for escalation of
ethical issues, should they ever arise. In taking a Responsible AI approach, the agency was also
able to establish new ethics roles to create internal and external awareness, foster trust and
encourage communication and collaboration.
Case study
Responsible AI From principles to practice 14
The value of AI is clear. But it can bring with it new, dynamic, ethical and social issues. A failure to
manage these issues can have a significant impact at a human and societal level, leaving organizations
exposed to financial, legal, and reputational repercussions.
While many organizations have taken the first step and defined AI principles, translating these into
practice is far from easy, especially with few standards or regulations to guide them. Our global
Responsible AI Practitioner's Survey identified a range of organizational, operational, technical and
reputational challenges that hold well-intentioned organizations back. While the initial focus is often on
ethical and legal requirements, success is also a function of an organization’s ability to modify its
traditional ways of working to support Responsible AI—and AI more broadly.
In our experience, successful organizations understand the importance of taking a systematic
approach from the start, addressing these challenges in parallel, while others underestimate the scale
and complexity of change required. A systematic approach requires proven tools, frameworks and
methodologies, enabling the organizations to move from principles to practice with confidence and
supporting the professionalization of AI.
In undertaking this process organizations also establish the structures needed to demonstrate the
long-term value of Responsible AI by scaling it across the organization, enabling the essential move
from “practice to proof.”
We use a set of 25 questions to help our clients to benchmark their motivators and challenges,
together with their maturity in terms of people, process and technology against their peers. Where are
you on your Responsible AI journey?
The next step? From practice to proof
About Accenture
Accenture is a global professional services company with leading
capabilities in digital, cloud and security. Combining unmatched
experience and specialized skills across more than 40 industries,
we offer Strategy and Consulting, Interactive, Technology and
Operations services—all powered by the world’s largest network of
Advanced Technology and Intelligent Operations centers. Our
514,000 people deliver on the promise of technology and human
ingenuity every day, serving clients in more than 120 countries.
We embrace the power of change to create value and shared
success for our clients, people, shareholders, partners and
communities. Visit us at www.accenture.com.
Follow @AccentureAI
Authors
This document is produced by consultants at Accenture as general guidance.
It is not intended to provide specific advice on your circumstances. If you
require advice or further details on any matters referred to, please contact
your Accenture representative.
This document makes descriptive reference to trademarks that may be
owned by others. The use of such trademarks herein is not an assertion of
ownership of such trademarks by Accenture and is not intended to represent
or imply the existence of an association between Accenture and the lawful
owners of such trademarks.
Copyright © 2021, Accenture. All rights reserved.
Accenture and its logo are trademarks of Accenture.
Ray Eitel-Porter
Managing Director, Applied Intelligence
Global Lead for Responsible AI
ray.eitel-porter@accenture.com
Medb Corcoran
Managing Director, Accenture Labs
Global Responsible AI Lead for Tech Innovation
medb.corcoran@accenture.com
Patrick Connolly
Research Manager
Responsible AI
patrick.connolly@accenture.com
Contributors
Bogdana Rakova
Artificial Intelligence Consultant, Responsible AI
Anika Mahajan
Responsible AI – Technical & ASEAN Lead
About Applied Intelligence
Applied Intelligence is Accenture’s approach to scaling AI for our
clients. We embed AI-powered data, analytics and automation
capabilities into business workflows to accelerate time to value. Our
expertise in defining end to-end strategy, combined with deep data
infrastructure capabilities, cognitive services and industrialized
accelerators help smooth clients’ path to AI adoption, extending
human capabilities and supporting clients in scaling AI responsibly.
Recognized as a leader by industry analysts, we collaborate with a
powerful global alliance, innovation and delivery network to help
clients deploy and scale AI within any market and industry.
Visit us at www.accenture.com/appliedintelligence.
About Accenture Labs
Accenture Labs incubates and prototypes new concepts through
applied R&D projects that are expected to have a significant impact
on business and society. Our dedicated team of technologists and
researchers work with leaders across the company and external
partners to imagine and invent the future. Accenture Labs is located
in seven key research hubs around the world: San Francisco, CA;
Sophia Antipolis, France; Washington, D.C.; Shenzhen, China;
Bangalore, India; Herzliya, Israel and Dublin, Ireland; and 25 Nano Labs
The Labs collaborates extensively with Accenture’s network of nearly
400 innovation centers, studios and centers of excellence located in
92 cities and 35 countries globally to deliver cutting-edge research,
insights and solutions to clients where they operate and live. For
more information, please visit www.accenture.com/labs.
About Accenture Research
Accenture Research shapes trends and creates data driven
insights about the most pressing issues global organizations face.
Combining the power of innovative research techniques with a deep
understanding of our clients’ industries, our team of 300 researchers
and analysts spans 20 countries and publishes hundreds of reports,
articles and points of view every year. Our thought-provoking
research—supported by proprietary data and partnerships with
leading organizations, such as MIT and Harvard—guides our
innovations and allows us to transform theories and fresh ideas
into real-world solutions for our clients. For more information,
visit www.accenture.com/research.",Addtional Details: https://www.accenture.com/us-en/insights/artificial-intelligence/responsible-ai-principles-practice
2,Adobe,United States of America,Software,,,https://www.adobe.com/content/dam/cc/en/ai-ethics/pdfs/Adobe-AI-Ethics-Principles.pdf,Adobe’s Commitment to AI Ethics,"Adobe’s Commitment to AI Ethics
At Adobe, our purpose is to serve the creator and respect the consumer, and our heritage is built on providing
trustworthy and innovative solutions to our customers. As our technology becomes more sophisticated, our products
and features have the potential to impact our customers in profound and exciting ways. However, we believe we have a
role that goes beyond creating the world’s best technology. We are committed to ensuring that our technology and the
use of our technology benefits society. At Adobe, as we innovate and harness the power of AI in our tools, we are
dedicated to addressing the harms posed by biased data in the training of our AI. AI Ethics is one of the core pillars of
our commitment to Digital Citizenship, a pledge from Adobe to address the consequences of innovation as part of our
role in society.
How AI is used in Adobe’s products
We believe AI will enhance human creativity and drive value from the complex global digital ecosystem. For the
Creative Cloud, we are focused on making it easier for everyone to tell their story with simpler and more intuitive tools.
As part of our Digital Experience offerings, Adobe’s enterprise customers use our AI to deliver relevant and meaningful
insights and personalized digital experiences to their end customers. And with Document Cloud, AI-enabled features
help understand the structure of PDFs to assist the user in viewing, searching, and editing documents on any platform.
However, we recognize the potential challenges inherent in this powerful technology. AI systems are based on data,
and that data can be biased. AI systems trained on biased data can unintentionally discriminate or disparage, or
otherwise cause our customers to feel less valued. Therefore, we are committed to maintaining a principled and
ethically sound approach to ensure our work stays aligned with our intended outcomes and consistent with our values.
And we are actively participating in government discussions around the world to shape AI Ethics regulation for the
good of the consumer and effectiveness in the industry.
AI Ethics Principles
At Adobe, we believe responsible AI development is based on the following three principles:
• Responsibility: We will approach designing and maintaining our AI technology with thoughtful evaluation and
careful consideration of the impact and consequences of its deployment. We will ensure that we design for
inclusiveness and assess the impact of potentially unfair, discriminatory, or inaccurate results, which might
perpetuate harmful biases and stereotypes. We understand that special care must be taken to address bias if a
product or service will have a significant impact on an individual’s life, such as with employment, housing,
credit, and health.
• Accountability: We take ownership over the outcomes of our AI-assisted tools. We will have processes and
resources dedicated to receiving and responding to concerns about our AI and taking corrective action as
appropriate. Accountability also entails testing for and anticipating potential harms, taking preemptive steps to
mitigate such harms, and maintaining systems to respond to unanticipated harmful outcomes.
Page 2 of 4
Adobe’s Commitment to AI Ethics
• Transparency: We will be open about, and explain, our use of AI to our customers so they have a clear
understanding of our AI systems and their application. We want our customers to understand how Adobe uses
AI, the value AI-assisted tools bring to them, and what controls and preferences they have available when they
engage with and utilize Adobe’s AI-enhanced tools and services.
AI development and AI ethical review is still in its infancy. With any such complex topic, errors may occur, but with the
commitment of our engineers and with help from our employees, our products and features will be best-in-class while
continuing to reflect Adobe’s values.
Adobe’s AI Ethics Principles
Responsibility
We at Adobe place a high value on taking responsibility for the impact of our company and the innovation we deliver
to the world. It stems from taking pride in our work and our dedication to the best possible outcomes. Therefore, we’ve
determined that responsibility is the critical foundational principle to underpin Adobe’s commitment and efforts toward
developing AI. We must understand and address the impact of introducing new technologies such as AI.
Responsible development of AI encompasses the following: designing an AI system thoughtfully, evaluating how it
interacts with end users, exercising due diligence to avoid unwanted kinds of bias, and vetting the AI system to
determine when the behavior of the system is unacceptable. This entails anticipating potential harms, taking
preemptive steps to mitigate such harms, measuring and documenting system performance throughout the technology
lifecycle, and establishing systems to monitor and respond to unanticipated harmful outcomes.
Responsibility and Bias
The behavior of AI features is strongly dependent on the data used to train them. We understand that AI
models can result in unintended biases in training data, whether produced by the selection of data to include
or by the customer actions that produce the data, can result in correspondingly biased behavior in the system.
Therefore, we are committed to building and curating AI training sets in order to avoid harmful bias for the
instances where bias perpetuates societal stereotypes which in turn negatively impact people’s lives. However,
we understand that all data has bias; therefore, we are committing to ensuring that the output of our AI
systems are remediated for bias, regardless of the input.
As part of developing and deploying its AI systems, Adobe will seek to mitigate unintended bias related to
human attributes (e.g. race, gender, color, ethnic or social origin, genetic or identity preservation features,
religion or belief, political belief, geography, income, disability, age, sexual orientation or vocation), and will
apply a special focus, and strict standards of fairness and inclusiveness, to situations where the outcome would
have an outsized impact on an individual’s life, such as access to information about employment, housing,
credit, and health.
Our ultimate goal is to design for inclusiveness rather than exclusion or discrimination. We will also determine
whether the advantages of using AI outweigh the risk of harm of using AI at all.
This notion of fairness, however, does not imply a rigid uniformity of experience across customers, as some of
the most typical AI use cases distinguish between individuals in ordinary and acceptable ways, as in
demographic marketing or personalized product recommendations. Responsible development of AI means
using AI in reasonable ways that accommodate the norms and values of our society.
Page 3 of 4
Adobe’s Commitment to AI Ethics
Responsibility and Adobe’s Digital Media Tools
It is possible, despite reasonable prevention efforts, that an outside party (customer or otherwise) might make
use of Adobe’s AI technology, such as with our video and photo tools, in a way that results in questioning the
authenticity of content. Adobe feels a responsibility to support the creative community, and society at large,
and is committing to contributing to solutions that address the issues of manipulated media.
However, we believe it is important to be clear about actions to which Adobe is not committing. We do not believe
that we will develop AI that:
• Ensures egalitarian fairness, in the sense of providing identical experiences to all users. The value of AI is in its
ability to differentiate, and as long as the AI is free of unfair bias, enabling customization and personalization of
tools, recommendations, and technologies is valuable, important, and welcomed by our users.
• Certification that inbound training data is ""unbiased"". All training data encodes bias in one direction or another,
as humans create the data point, and humans have inherent bias. We do not believe it is reasonable or useful
to commit to a zero-bias starting point; rather, it is more important and effective to commit to having an
outcome that mitigates against bias.
Accountability
Accountability means the commitment to take ownership for the outcomes of our actions. At Adobe, while anyone
involved with AI has an obligation to help ensure it’s being managed responsibly, business leaders are held accountable
for the ethical operation of Adobe’s AI technologies. We are ensuring processes are in place and resources are
dedicated to meet Adobe’s AI Ethics commitments, including to develop and implement the necessary engineering
practices to achieve our responsibility goals, receive and respond to internal and external concerns, and to take
corrective action as required.
How We Ensure Accountability:
• Establishing governance processes to evaluate and track the performance of AI algorithms, data and designs,
including labeling datasets and models for any identified bias to ensure remediation can occur at product
design stage;
• Requiring an AI Impact Assessment (as part of our services development process) to ensure an AI ethics review
happens before deployment of new AI technologies;
• Creating an Ethics Advisory Board to oversee the promulgation of AI development requirements and be a
place where any AI ethics concerns can be heard, while safeguarding ethical whistleblowers;
• Processes to ensure remediation of any negative AI impacts that are discovered after deployment;
• Education of engineers and product managers via mandatory training courses on AI ethics issues.
Page 4 of 4
Adobe’s Commitment to AI Ethics
Transparency
Transparency is the reasonable public disclosure, in clear and simple language, of how we responsibly develop and
deploy AI within our tools. Adobe values our trusted relationship with our customers and feels that transparency is
integral to that relationship. This includes sharing information on how or whether Adobe collects and uses customer
assets and usage data to improve our products and services.
Transparency includes the disclosure of the following:
• Our data collection practices:
o When and if individual’s data will be collected for AI training, and what controls a user will have over
the collection
o Providing notice prior to and if human-review of customer data for AI training will be implemented
• Model development:
o How datasets are used in building AI models
• Accountability processes:
o How Adobe is testing for and resolving issues related to unfair bias.
• General disclosure of how data and AI are used in Adobe’s tools and services;
• Provide external and internal feedback mechanisms to report concerns on our AI practices",Addtional Details: https://www.adobe.com/content/dam/cc/en/ai-ethics/pdfs/Adobe-AI-Ethics-Principles.pdf
3,Alphabet,United States of America,Software,,,https://ai.google/responsibilities/responsible-ai-practices/,Responsible AI practices,"Responsible AI practices
The development of AI is creating new opportunities to improve the lives of people around the world, from business to healthcare to education. It is also raising new questions about the best way to build fairness, interpretability, privacy, and security into these systems.

These questions are far from solved, and in fact are active areas of research and development. Google is committed to making progress in the responsible development of AI and to sharing knowledge, research, tools, datasets, and other resources with the larger community. Below we share some of our current work and recommended practices. As with all of our research, we will take our latest findings into account, work to incorporate them as appropriate, and adapt as we learn more over time.

Explore our responsible practices:
General recommended practices for AI
Fairness
Interpretability
Privacy
Security
Back to top ↑
General recommended practices for AI
Reliable, effective user-centered AI systems should be designed following general best practices for software systems, together with practices that address considerations unique to machine learning. Our top recommendations are outlined below, with additional resources for further reading.

Recommended practices
Use a human-centered design approach
The way actual users experience your system is essential to assessing the true impact of its predictions, recommendations, and decisions.

Design features with appropriate disclosures built-in: clarity and control is crucial to a good user experience.
Consider augmentation and assistance: producing a single answer can be appropriate where there is a high probability that the answer satisfies a diversity of users and use cases. In other cases, it may be optimal for your system to suggest a few options to the user. Technically, it is much more difficult to achieve good precision at one answer (P@1) versus precision at a few answers (e.g., P@3).
Model potential adverse feedback early in the design process, followed by specific live testing and iteration for a small fraction of traffic before full deployment.
Engage with a diverse set of users and use-case scenarios, and incorporate feedback before and throughout project development. This will build a rich variety of user perspectives into the project and increase the number of people who benefit from the technology.
Identify multiple metrics to assess training and monitoring
The use of several metrics rather than a single one will help you to understand tradeoffs between different kinds of errors and experiences.

Consider metrics including feedback from user surveys, quantities that track overall system performance and short- and long-term product heath (e.g., click-through rate and customer lifetime value, respectively), and false positive and false negative rates sliced across different subgroups.
Ensure that your metrics are appropriate for the context and goals of your system, e.g., a fire alarm system should have high recall, even if that means the occasional false alarm.
When possible, directly examine your raw data
ML models will reflect the data they are trained on, so analyze your raw data carefully to ensure you understand it. In cases where this is not possible, e.g., with sensitive raw data, understand your input data as much as possible while respecting privacy; for example by computing aggregate, anonymized summaries.

Does your data contain any mistakes (e.g., missing values, incorrect labels)?
Is your data sampled in a way that represents your users (e.g., will be used for all ages, but you only have training data from senior citizens) and the real-world setting (e.g., will be used year-round, but you only have training data from the summer)? Is the data accurate?
Training-serving skew—the difference between performance during training and performance during serving—is a persistent challenge. During training, try to identify potential skews and work to address them, including by adjusting your training data or objective function. During evaluation, continue to try to get evaluation data that is as representative as possible of the deployed setting.
Are any features in your model redundant or unnecessary? Use the simplest model that meets your performance goals.
For supervised systems, consider the relationship between the data labels you have, and the items you are trying to predict. If you are using a data label X as a proxy to predict a label Y, in which cases is the gap between X and Y problematic?
Data bias is another important consideration; learn more in practices on AI and fairness.
Understand the limitations of your dataset and model
A model trained to detect correlations should not be used to make causal inferences, or imply that it can. E.g., your model may learn that people who buy basketball shoes are taller on average, but this does not mean that a user who buys basketball shoes will become taller as a result.
Machine learning models today are largely a reflection of the patterns of their training data. It is therefore important to communicate the scope and coverage of the training, hence clarifying the capability and limitations of the models. E.g., a shoe detector trained with stock photos can work best with stock photos but has limited capability when tested with user-generated cellphone photos.
Communicate limitations to users where possible. For example, an app that uses ML to recognize specific bird species might communicate that the model was trained on a small set of images from a specific region of the world. By better educating the user, you may also improve the feedback provided from users about your feature or application.
Test, Test, Test
Learn from software engineering best test practices and quality engineering to make sure the AI system is working as intended and can be trusted.

Conduct rigorous unit tests to test each component of the system in isolation.
Conduct integration tests to understand how individual ML components interact with other parts of the overall system.
Proactively detect input drift by testing the statistics of the inputs to the AI system to make sure they are not changing in unexpected ways.
Use a gold standard dataset to test the system and ensure that it continues to behave as expected. Update this test set regularly in line with changing users and use cases, and to reduce the likelihood of training on the test set.
Conduct iterative user testing to incorporate a diverse set of users’ needs in the development cycles.
Apply the quality engineering principle of poka-yoke: build quality checks into a system, so that unintended failures either cannot happen or trigger an immediate response (e.g., if an important feature is unexpectedly missing, the AI system won’t output a prediction).
Continue to monitor and update the system after deployment
Continued monitoring will ensure your model takes real-world performance and user feedback (e.g., happiness tracking surveys, HEART framework) into account.

Issues will occur: any model of the world is imperfect almost by definition. Build time into your product roadmap to allow you to address issues.
Consider both short- and long-term solutions to issues. A simple fix (e.g., blocklisting) may help to solve a problem quickly, but may not be the optimal solution in the long run. Balance short-term simple fixes with longer-term learned solutions.
Before updating a deployed model, analyze how the candidate and deployed models differ, and how the update will affect the overall system quality and user experience.",Addtional Information: https://ai.google/principles/
4,Amazon,United States of America,Software,,,https://d1.awsstatic.com/responsible-machine-learning/responsible-use-of-machine-learning-guide.pdf,Responsible Use of Machine Learning,"Responsible Use of Machine Learning
At AWS, we are proud to support our customers as they invent, build, and use machine learning (ML) systems
to solve real-world problems. We see the transformational nature of ML technology across industries every
day. ML techniques can make tasks easier, safer, and more efficient. For example, ML has been used to develop
transcription and translation services, fraud detection software, search and recommendation engines, and tools
that monitor and help protect our environment.
Given the breadth and depth of ML, many customers are asking for perspectives on how to responsibly develop
and use ML systems. This document shares some recommendations, examples, and tools that can be used
across three major phases of ML life cycles: (1) design and development; (2) deployment; and (3) ongoing use.
An important preliminary note is that we believe all use of ML must respect the rule of law, human rights,
and values of equity, privacy, and fairness. The field of responsible ML is a rapidly developing area, so these
recommendations should be viewed as a starting point and not the final answer. We encourage readers to
consider the spirit and intent behind the recommendations. They should be considered along with third party
and AWS tools and resources on responsible development and use of ML systems, such as the ones listed below.
We are also eager to receive feedback, and appreciate the opportunity to contribute to this important topic
while continuing to learn from the broader community.
Phase 1: Design and Development
This phase includes establishing requirements of the ML system, defining performance criteria, exploring the
potential impact of the system on users and other parties, collecting and curating training data, and building and
testing models and other system components.
Evaluating Use Cases: There are a wide variety of use cases that may incorporate ML, with different goals,
characteristics, user bases, and potential impacts. Developers should consider the benefits and potential risks of
their specific use case. Given the broad nature and applicability of ML, many applications may pose limited or no
risk (e.g., movie recommendation systems), while others could involve significant risk, especially if used in a way
that impacts human rights or safety. Examples of risks worth carefully evaluating include: technical limitations
of an ML system, over-reliance on limited data or inaccurate output, the potential for bias in training data or
the model itself, and intentional or unintentional misuse, as well as the likelihood and impact of those risks
and possible solutions. Potential mitigation options may include detailed documentation, explicit warnings or
contractual restrictions, technical restrictions, or mechanisms to receive and act on feedback.
ML Capabilities and Limitations: Developers and users should understand the nature, capabilities, and limitations
of ML systems, including important concepts like the probabilistic nature of ML, confidence levels, and human
review. Many ML systems predict a possible or likely answer, not the answer itself. The probabilistic nature of
ML means that use cases that require definitive answers (as opposed to possible or likely answers) may benefit
from additional guardrails. Consider providing a numeric or other indicator of the confidence level associated
with system output to help users evaluate the output for their use case. Also consider whether human review
or oversight of the system may be appropriate, and when it should be required. As an example, if an ML system
helps predict the risk of fraud in online transactions, it may not be appropriate to take output from the system as
the sole indicator of fraud, but as one factor to be analyzed in connection with the overall transaction. In certain
cases it may be appropriate for a trained person to review the ML prediction and transaction before any action is
taken. In cases where human review is needed, consider how to provide reviewers the necessary training, context,
and interface to take action.
Building and Training Diverse Teams: It is important to have diverse backgrounds, perspectives, skills, and
experiences on teams that are developing ML systems. Assess whether teams include a wide array of genders,
races, ethnicities, abilities, ages, religions, sexual orientations, military status, backgrounds, and political views.
Further assess whether teams may have gaps and consider adding underrepresented perspectives to fill those
gaps to enhance performance. Successful teams will likely have cross-functional expertise (e.g., technologists,
academics, industry experts, lawyers, and other stakeholders) and diverse characteristics to help ensure important
perspectives are taken into account. Consider resources such as user testing, focus groups, and/or third party
advocacy groups to obtain additional perspectives from outside parties. There are many public resources, such as
the EU Assessment List for Trustworthy Artificial Intelligence, that can enable deeper analyses on these
subjects.
Be Mindful of Overall Impact: Consider the potential impact of an ML system on parties that are not customers or
direct users of the system, but may still be affected. For example, if an autonomous vehicle is not operating as
expected, it could have an impact on passengers, other drivers, pedestrians, or property. Similarly, consider
guidelines or restrictions for use of ML systems that determine whether users are eligible for certain services or
benefits, if those users may lose eligibility based on how the ML system’s output is used. 
Data Collection: Consider how you will acquire data to develop and test ML models. For example, data may
be available through open source repositories, through licenses from third party data providers, or already in
your possession. Involve your legal and procurement teams as appropriate to assess the impact of any privacy
considerations or other relevant laws, license, or contractual requirements that may impact your collection or use
of the data. Consider any necessary processes for handling data securely and safely, and ways to mitigate risk.
For example, if certain portions of a data set are sensitive, but are not necessary for development of the model,
consider whether you can discard that content.
Training and Testing Data: When collecting and evaluating data to develop and test models, consider its
completeness, representativeness, and breadth. Diversity of data is often important for use cases that involve
personal characteristics like race and gender, but can also apply in non-obvious contexts. Develop mechanisms to
evaluate whether the data appropriately represents real world use, and collect and test additional data to address
underrepresented attributes. For example, an audio transcription system may need data with different accents,
speech speeds, vernacular, and background environments, and autonomous transport systems may need data
from different terrains and obstacles (e.g., cobblestones, dirt, and cracked sidewalks). Review data for freshness
(data may be outdated and in need of replacement), potential sources of error (inherent to the data itself, in its
structure and organization, or introduced during annotation), and bias (discussed below). It is important to have
separate sets of data for training and testing, but both sets should be complete and representative.
Bias: Consider ways to maximize accuracy and reduce bias in data, algorithms, and system design. Some
suggestions include:
• Staffing development and annotation teams with a diverse set of backgrounds, perspectives, skills,
experiences, and demographics as appropriate for your system’s use case and performance.
• Understanding perspectives and potential biases of data annotators and developers, and having processes to
mitigate human error (for example, by using annotators familiar with the subject matter, being thoughtful
when using labels that require subjective versus objective judgment, and checking annotation samples for
accuracy). Consider using multiple data annotators and developers to help identify discrepancies.
• Creating fairness goals and metrics (including potential minimum acceptable thresholds) to measure
performance across different subgroups, communities, and demographics applicable to the use case, and
testing and measuring progress against those metrics. For example, a speech recognition system may be
evaluated for accuracy across different speaker groups by running statistical studies to determine the
correlation between speaker demographic variables (such as regional accents) and error rates. Consider
whether and how bias is measured in existing processes that do not use ML, and how to use that information to
evaluate the effectiveness of the ML system.
• Having independent teams help test the system for bias, and considering whether it may be appropriate or
feasible to have external parties perform such evaluations.
• Developing plans to remediate potential inaccuracies and bias, which may include evaluating root causes,
developing new requirements, acquiring more data, and re-training models.
• Considering mechanisms to allow users to evaluate system performance and bias/accuracy using their own
data for their specific use case.
Explainability of ML systems: Consider the need to explain the methodology and important factors that influence
the ML system’s output. Mechanisms to help explain complex ML models are still being researched and there is
currently no “silver bullet” for explainability, but some areas (like explainability of models that use structured
tabular data) have progressed further than others and can be used to help explain certain predictions today. The
importance of explainability will vary depending on the use case: many systems that have low or no risk may not
require explainability, while ML systems whose output may be used in a manner that could impact human rights
or safety will likely need a method for determining how the system performed its analysis. If explainability is not
technically feasible, consider whether other mechanisms, such as human review, auditability (next section), and
re-focusing or limiting the scope of the use case might serve as an appropriate alternative.
Auditability: Consider the need for implementing mechanisms to track and review steps taken during
development and operation of the ML system, e.g., to trace root causes for problems or meet governance
requirements. Evaluate the need to document relevant design decisions and inputs to assist in such reviews.
Establishing a traceable record can help internal or external teams evaluate the development and functioning of
the ML system.
Legal Compliance: Engage with legal advisors to assess requirements for and implications of building your ML
system. This may include vetting legal rights to use data and models, and determining applicability of laws
around privacy, biometrics, anti-discrimination, and other use-case specific regulations. Be mindful of differing
legal requirements across states, provinces, and countries, as well as new AI/ML regulation being considered
and proposed around the world. Re-visit legal requirements and considerations through future deployment and
operations phases. 
Phase 2: Deployment
This phase includes preparing and deploying ML systems for use, including understanding and accounting for
capabilities, limitations, and risks associated with deployment.
Education, Documentation and Training: Consider whether users and other stakeholders should be educated
on topics like the predictive nature of ML, confidence indicators and thresholds, capabilities and limitations of
the system, recommended or prohibited uses, and best practices. As an example, if deploying a conversational
chatbot, users should be informed that they are interacting with a computer system and not a real person,
to avoid misunderstandings about the nature of the interaction. If using a facial recognition system to assist
personnel in making decisions that could impact a person’s civil liberties or human rights, include appropriate
training for human reviewers on the nature and proper use of such systems. Consider appropriate mechanisms
and processes for carrying out trainings or communications, and the need to provide more educational resources
around issues like privacy, safety, transparency, accessibility, inclusiveness, and bias.
Confidence Levels and Human Review: As noted earlier, it’s important to understand that many ML systems
generate predictions of a possible or likely answer, not the answer itself. If confidence indicators are available,
take them into account (or instruct your users to take them into account) when reviewing and taking action using
output provided by the system. For higher risk use cases, be mindful of situations where confidence indicators
may not be appropriately considered and may instead be used as a shortcut to make decisions, and whether
such behavior can be mitigated. Regardless of confidence levels, consider whether human review or oversight
over the operation of the system may be appropriate or necessary (e.g., in situations where ML systems may be
used in a manner that impact human rights or safety), and if so, how to best incorporate such human input into
the overall operation of the system. Human reviewers should be appropriately trained on real world scenarios,
including examples where the system fails to properly process inputs or cannot handle edge cases, and have ways
to exercise meaningful oversight.
Use Case Evaluation and Testing: Consider whether a particular ML model is appropriate for the use case,
including any benefits, limitations, and risks. This should be reassessed if the model is used for new use cases, or
beyond the system for which it was designed. It is important to test ML systems in the operational environments
and on the data on which they will be deployed before live deployment. Develop metrics and a test plan to
measure performance of the system against production uses, and consider ongoing tests against a frequently
updated “gold standard” dataset. Testing should include not just the ML system itself but also the overall process
it is a part of, including decisions or actions that might be taken based on system output. In some situations, it
may not be appropriate to use the system if testing does not reach a specified accuracy level. In other cases, such
as where the system is used for entertainment purposes or to “narrow the field” for additional review or human
judgment, accuracy is one variable that should be balanced with other factors, such as the need to generate a
large number of results. Deployers should also factor in localization requirements when deploying an ML system
into a new use case, region, or geography different from the one for which it was designed and tested -- for
example, real estate pricing models in different geographic areas, or voice recognition systems deployed in areas
with different dialects or accents. 
Notice and Accessibility: Consider whether to notify end users about the use of ML in the system they are
interacting with, such as the earlier example about notifying users that they are interacting with a chatbot
and not a live human. Consider whether it is appropriate or feasible to allow end users to bypass interacting
with the system and offer an alternate method to accomplish the use case -- for example, some users may
prefer not to use a facial recognition authentication system and request a different method of authentication.
Consult accessibility resources to ensure that the system is actually usable by the target audience and provides
appropriate access options to all intended users.
Operational Data: Consider the sources of any data used with the ML model once it is deployed. As with data
used for training and testing, involve your legal and procurement teams as appropriate to assess the impact of
any relevant laws or contractual requirements on operational data. Consider any necessary processes for handling
data securely and safely, and ways to mitigate risk.
Safety, Security, and Robustness: Use of the ML system must be safe for both users and third parties. As with
any technology, deployers should implement appropriate mechanisms to protect the ML system and associated
data (both inputs and outputs) from loss, attack, vulnerabilities, or unexpected or malicious user behavior. These
mechanisms may include limiting potential access to the system, putting in place legal or technical restrictions
on use, and/or implementing warnings, notices, education and trainings that educate users about risks and
consequences of improper use. Consider how potential inaccuracies in results produced by the ML system may
impact users and relevant stakeholders, and prepare a plan for addressing these inaccuracies, which may involve
narrowing the scope of use, relying on human review or oversight, or altering dependencies on the system.
Legal Compliance: As noted in the development phase, it is important to engage your legal advisors to assess
legal requirements arising from your deployment and use of the system. 
Phase 3: Operation
This phase deals with ongoing operation of the system after it is developed and deployed. Note that many
considerations and questions from Phases 1 and 2 continue to be relevant.
Provide and Use Feedback Mechanisms: Since ML systems can continue to “learn” and improve throughout their
lifecycle, an important aspect of improvement involves receiving and incorporating feedback from users and
stakeholders. Consider soliciting feedback through programmatic and manual methods, including in-system
mechanisms or third party outreach through surveys and focus groups. Keep in mind that not all feedback will be
relevant or actionable, and it may be appropriate to develop and communicate expectations for acknowledging
and addressing feedback. If appropriate for the use case (such as if an ML system might be used to help make
decisions on eligibility for important services), consider mechanisms for users or stakeholders to request more
information about, or obtain remediation for, negative impact arising from how system output is used.
Continuous Improvement and Validation: ML is an iterative science. Consider the issues raised in previous phases
about monitoring and testing of your system. ML models can be subject to “concept drift,” where model behavior
changes as a result of changes in users, environments, or data over time. There are multiple ways that models
in ML systems can drift, including changes to the use case, operating environment, or types and quality of data.
Develop and run ongoing performance tests, and use these test results and feedback to identify areas where
additional data or development may improve your system’s performance. Be thoughtful about the data being
used as inputs and for any further training or tuning of the ML system. Continue to monitor for potential bias
and accuracy, including that your models perform as expected across different segments. Consider appropriate
adjustments to both the system and overall processes that involve the system, such as updated training, new
notices or restrictions, or optimizing the ways system output is evaluated and used.
Ongoing Education: ML is a constantly evolving landscape, and new techniques, technologies, laws, and social
norms will continue to be developed and refined over time. It is critical that all parties involved with building and
using ML systems stay educated on these issues and account for them in the design, deployment, and operation
of their systems. We encourage all stakeholders in the field, and other interested parties, to contribute knowledge
and relay their experiences and learnings to the broader community. 
Tools and Resources
AWS offers a large number of tools and resources to help you responsibly develop and use ML systems, including
methods to help address some of the issues above.
Amazon SageMaker Clarify provides ML developers with greater visibility into their training data and models so
they can identify and limit bias and explain predictions. Amazon SageMaker Clarify detects potential bias during
data preparation, after model training, and in a deployed model by examining attributes you specify. For instance,
you can check for bias related to age in your initial dataset or in your trained model and receive a detailed report
that quantifies different types of possible bias. SageMaker Clarify also helps you to look at the importance of
model inputs to explain why models make the predictions they do. SageMaker Clarify includes feature importance
graphs that help you explain model predictions and produces reports which can be used to support internal
presentations or to identify issues with your model that you can take steps to correct.
Amazon Augmented AI makes it easy to build the workflows required for human review of ML systems. Amazon
A2I brings human review to all developers, removing the difficult tasks of building custom human review systems
or managing large numbers of human reviewers. As mentioned above, in some situations it may be appropriate
to have human oversight over ML systems to help ensure accuracy, provide continuous improvements, or retrain
models with updated predictions. Amazon A2I streamlines building and managing human reviews for ML
applications. Amazon A2I provides built-in human review workflows for common ML use cases, such as content
moderation and text extraction from documents. You can also create your own workflows for ML models built
on SageMaker or any other tools. Using Amazon A2I, you can allow human reviewers to step in when a model is
unable to make a high-confidence prediction or to audit its predictions on an ongoing basis.
Amazon SageMaker Model Monitor helps maintain high quality ML models by detecting model and concept drift
in real-time, and sending alerts to enable immediate action. Model and concept drift are detected by monitoring
the quality of the model based on independent and dependent variables. Independent variables (also known as
features) are the inputs to an ML model, and dependent variables are the outputs of the model. For example, with
an ML model predicting a bank loan approval, independent variables could be age, income, and credit history of
the applicant, and the dependent variable would be the actual result of the loan application. SageMaker Model
Monitor constantly monitors model performance characteristics such as accuracy, which measures the number of
correct predictions compared to the total number of predictions, so you can take action to address anomalies.
Amazon SageMaker Data Wrangler gives you better control of your training and testing data by simplifying
the process of data preparation and feature engineering. You can complete each step of the data preparation
workflow, including data selection, cleansing, exploration, and visualization from a single visual interface. It
also helps you identify potential errors, extreme values, and inconsistencies in your data preparation workflow
through visualization templates. Using SageMaker Data Wrangler’s data selection tool, you can choose the data
you want from various data sources and import it with a single click. SageMaker Data Wrangler contains over 300
built-in data transformations so you can quickly normalize, transform, and combine features without having to
write any code.
Training and Professional Services. AWS offers the latest in ML education through the Machine Learning
University, Training and Certification program, and AWS ML Embark. You can also work with experts in responsible
ML within our AWS Professional Services organization to create an operational approach encompassing people,
processes, and technology to develop and operationalize responsible ML principles based on a proven framework.
This can helps you look around corners, uncover potential unintended impacts, and mitigate risks related to the
development, deployment, and operationalization of ML systems.
Research, Innovation, and External Collaboration. AWS collaborates with academia and other stakeholders
through strategic partnerships with universities including University of California, Berkeley, MIT, California
Institute of Technology, the University of Washington, and others. We are also active members of multistakeholder organizations relating to AI, including OECD AI working groups and The Partnership on AI. We also
provide research grants through Amazon Research Awards and the joint Amazon and National Science Foundation
Fairness in AI Grants program. See some of our recent videos and publications related to Responsible AI:
Amazon scientist Dr. Nashlie Sephus focuses on ensuring accuracy in machine learning
Amazon Scholars Michael Kearns and Aaron Roth discuss the ethics of machine learning
How a paper by three Oxford academics influenced AWS bias and explainability software
Nine videos about explainable AI in industry
Related Publications From Amazon Science:
Correcting exposure bias for link recommendation
Fair Bayesian optimization
General Fair Empirical Risk Minimization
Learning Deep Fair Graph Neural Networks
Bias preservation in machine learning: The legality of fairness metrics under EU non-discrimination law
Learning fair and transferable representations with theoretical guarantees
Exploiting MMD and Sinkhorn divergences for fair and transferable representation learning
Towards unbiased and accurate deferral to multiple experts
Amazon SageMaker Clarify: Machine learning bias detection and explainability in the cloud
Learning to rank in the position based model with bandit feedback
Decoding and diversity in machine translation
Fairness measures for machine learning in finance
Fair Bayesian optimization
Mixed-privacy forgetting in deep networks
Continuous compliance
© 2022, Amazon Web Services, Inc. or its affiliates. All rights reserved.",
5,Atos,France,Consulting,,,https://atos.net/en/lp/cybersecurity-magazine-ai-and-cybersecurity/the-atos-blueprint-for-responsible-ai,The Atos Blueprint for Responsible AI,"AI is a broad topic encompassing many different families of algorithms and techniques. However, we currently live in a narrow AI era where AI is used for very specific tasks. Some AI techniques such as machine learning have proven to be more efficient than humans in areas such as computer vision, automated translation, predictions and anomaly detection.

Despite the progress, a number of technical challenges remain. There is a huge need for diverse and qualitative data and — in some cases — large volumes of historical data. Furthermore, we need more efficient computing, algorithms and DevOps scalability (e.g. parallelization for algorithms and MLOps tooling for DevOps).
Bringing the best out of AI
Atos defines responsible AI with four key dimensions: fair and ethical, robust and secure, industrialized and eco-sustainable. Atos enables you to boost your business by keeping these foundational elements of responsible AI in mind.
The proposed way in which Atos can boost your business are:
Enable AI – design and deliver cost-efficient and secured infrastructure for your AI needs
Augment with AI – leverage AI to optimize existing business processes and operations
Grow the business with AI – leverage AI to create new business models
The four horsemen of the apocalyptic AI
A failure to follow the four dimensions of responsible AI can have a serious business impact. Below are some of the problems and consequences of (non)responsible AI:
(Un)fair and (un)ethical — Compliance fines, reputation damage, reduced talent attraction, negative corporate social responsibility impacts
(Non)robust and (un)secure — Reputation risk, user acceptability, revenue impacts, compliance fines
(Non)industrialized — Impact on gross margin, lower revenue, productization and scalability challenges
(Non)sustainable — Ambiguous decarbonization goals, reputation damage, higher cost for customers, reduced talent attraction, lower revenue, compliance fines
Ethical considerations are important in AI solutions. There has been a shift from algorithms written by humans to algorithms that learn their behaviour from data. This implies that humans need to be in the loop to control outputs of the AI algorithms, which can be biased by input data or potentially compromised. Full delegation of human-controlled tasks to AI solutions implies greater responsibility in the way AI solutions are designed and maintained in a secure and explainable way.
Putting customer’s purposes at the heart
Finally, it’s important to position AI ethics within a customer relationship. Ethics is relative to every purpose and company. Our goal is to support our customers in building responsible AI. We do this by first defining Atos ethics, which explain how we want to drive our AI projects from an ethical point of view. In addition, they must be aligned with the Atos’s enterprise purpose. Then, we define the AI ethics specificity driven by a particular project that requires a custom view on the situation (e.g. a self-driving car that needs to adapt to the principles and cultures of a particular country).
We establish processes and provide rules and tools to make sure that AI solutions take ethical concerns into account up-front, from their creation to their retirement. In a nutshell, we advise our clients to include AI concerns in their ethics.
Only by taking this broad array of factors into account and designing solutions that align with our priorities, client priorities and the specific business, cultural and legal parameters can we develop AI solutions that are a true win-win for all parties.",
6,Capgemini,France,Consulting,,,https://www.capgemini.com/wp-content/uploads/2021/03/Capgemini_Code_of_Ethics_for_AI_2021_EN.pdf,Our Code of Ethics for AI,"Our Code of Ethics for AI
AI is a general-purpose technology that can affect entire
economies, and which is spreading very visibly beyond
the business area to areas of daily life. A challenge
facing both business and society today is how to
optimize the opportunities offered by AI technology,
whilst addressing the risks and fears that AI may generate.
Since its foundation, Capgemini places ethics at the center of
its activity. As a leader in digital transformation, we are committed
to the adoption of AI in a way that delivers clear benefits from AI
technologies within a trusted framework, by building a Code of Ethics
for AI.
Our ethical culture drives our vision of AI, guided notably by 5 of
our core Values: Honesty, Trust, Boldness, Freedom, and Modesty.
These Values work together to inform our approach. Boldness drives
us to act as entrepreneurs, identifying and pursuing the opportunities
presented by innovation in this field. We aspire to increase Freedom
by empowering, complementing and augmenting human cognitive,
social and cultural skills, giving people more say over how they
live their lives. Modesty keeps us mindful of the need to mitigate
risks, building solutions that are robust, safe, and human-centric.
Honesty underpins our commitment to transparency, and to creating
solutions that are accountable and controllable. We consider
Trust to be an essential basis for long-standing interdependent
relationships with clients, users, and all members of our ecosystem;
the value we place on Trust drives our efforts to create AI that
protects privacy and ensures equal access rights and fair treatment.
Our Code of Ethics for AI guides our organization on how to
embed ethical thinking in our business. It is illustrated by concrete
examples from projects or solutions that we deliver. Reference
to its principles stimulates ethical reasoning and is intended
to launch an open-ended process of discussion within the company,
with our clients, and with all stakeholders.
Our Code of Ethics for AI concerns both the intended purpose
of the AI solution, and the way we embed ethical principles in
the design and delivery of AI solutions and services to our clients.
It should be read in combination with applicable legislations
with which Capgemini is, of course, committed to comply.
Our Code of Ethics for AI
AI definition
Artificial Intelligence (AI) is a
collective term for the capabilities
shown by learning systems
that are perceived by humans
as representing intelligence.
These intelligent capabilities
typically can be categorized
into Machine Vision & Sensing,
Natural language processing,
Predicting & Decision-making,
and Acting & Automating.
Various applications of AI include
speech, image, audio and video
recognition, autonomous vehicles,
natural language understanding and
generation, conversational agents,
prescriptive modelling, augmented
creativity, smart automation,
advanced simulation, as well as
complex analytics and predictions.
Technologies that enable these
applications include automation,
big data systems, deep learning,
reinforcement learning
and AI acceleration hardware.
22 Capgemini Capgemini Our Code of Ethics for AI Our Code of Ethics for AI
At Capgemini, we believe that human ethical values should
never be undermined by the uses made of AI by business. We want
AI solutions to be human-centric, which we define as follows:
1. AI with carefully delimited impact – designed for human benefit,
with a clearly defined purpose setting out what the solution will deliver,
to whom.
2. Sustainable AI – developed mindful of each stakeholder, to benefit
the environment and all present and future members of our ecosystem,
human and non-human alike, and to address pressing challenges such
as climate change, CO₂ reduction, health improvement, and sustainable
food production.
3 Fair AI – produced by diverse teams using sound data for unbiased
outcomes and the inclusion of all individuals and population groups.
4. Transparent and explainable AI – with outcomes that can be
understood, traced and audited, as appropriate.
5. Controllable AI with clear accountability – enabling humans
to make more informed choices and keep the last say.
6. Robust and safe AI – including fallback plans where needed.
7. AI respectful of privacy and data protection – considering
data privacy and security from the design phase, for data usage that is
secure, and legally compliant with privacy regulations.
& Sensin
g
Machine
Visio
n
Dec si
oi n
a M i k gn
Pred ci t ni g na d
uA
ot mat ni
g
c A t ni g and
Processing
N
atural Language
LEARNING
SYSTEMS
Augmented
Creativity
Autonomous
Vehicles
AI APPLICATIONS
Speech Synthesis Image Recognition
Audio & Video
Analytics
Natural
Language
Understanding
Conversational
Agents
Prescriptive
Modelling
Smart Automation
Advanced
Simulation
Reinforcement
Learning
Deep Learning
AI Acceleration
Hardware
AI ENABLERS
Analytics &
Predictions
Big Data
Systems
INTELLIGENCE
CAPABILITIES
AI Taxonomy
Building on our core Values,
we believe that the design
and delivery of AI solutions
should be guided by these
seven principles, aligned
with the “Ethics Guidelines
for Trustworthy AI” issued
in 2019 by the independent
High-Level Expert Group
on AI set up by the European
Commission.
3
 ETHICAL CHALLENGE
The very first and fundamental ethical question to be considered
is the intended purpose of the AI solution and its impact
on humans. Like any general-purpose technology, AI solutions
can equally enable and negatively affect human fundamental
rights.
1. AI with carefully
delimited impact
 CAPGEMINI’S RESPONSE
Capgemini cares about the intended
purpose of AI solutions; our solutions
must be mindful of the impact on humans
and respect universal fundamental rights,
principles and values, in particular the
Universal Declaration of Human Rights
and the UN Global Compact. AI must
focus on improving life for humans and
should neither exacerbate existing harm
nor create new harm for individuals.
The intended purpose of an AI application
– what the AI solution will deliver,
for whom, and to whom – must be clearly
defined, and AI should be used according
to its intended purpose. To this end,
we are transparent about the intended
purpose with our various stakeholders,
notably the end users, and include
appropriate provisions in our agreements,
clearly describing the use for which the
technology is intended.
As AI is a highly evolutive technology,
we believe that assessing the impact
of AI solutions, notably on individuals,
is important to help identify the overall
impact, i.e. the likely benefits against
the foreseeable risks, such as social impact
or potential risk deriving from inadequate
or inappropriate use. Assessing the
potential impact that a new technology
can have before adopting it helps identify
undesired side-effects and consequent
ethical risks and helps mitigate them.
In situations where there is any doubt about
a potential risk of affecting fundamental
rights, a fundamental-rights impact
assessment will be undertaken to ensure
that such a risk is eliminated.
4 Capgemini Our Code of Ethics for AI
CASE
Global demand for food is anticipated to increase
by 60% by 2050. Today, much of the world’s
population is fed by small-scale farmers, primarily
in developing countries, using rudimentary
farming practices. This agricultural inefficiency
is exacerbated by a complex value chain and a lack
of resources and connectivity, so there is a strong
need for a wider package of yield-optimizing
and risk-decreasing services for these small-scale
farmers. Project FARM, created at Capgemini’s
Applied Innovation Exchange (AIE) Collaboration
Zone (CoZone) in the Netherlands, aims to address
these issues.
The Project FARM platform uses AI to determine
farming patterns through big data, generating
insights from the data to make recommendations.
It uses machine learning to make the platform
applicable at scale, by connecting it with cell
phones. This solution has been built in collaboration
with Agrics, a social enterprise operating in
East Africa, which provides local farmers with
agricultural products and services on credit.
 ETHICAL CHALLENGE
Beyond the direct impact on humans and human society, other beings
and the environment can be impacted by AI solutions. The challenge
goes beyond guiding “human-friendly AI”, to ensuring “Earth-friendly
AI”. As the scale and urgency of the economic and health impacts
from our deteriorating natural environment grows, we have an
opportunity to look at how AI can help transform traditional sectors
and systems to address climate change, deliver food and water security,
build sustainable cities, and protect biodiversity and human wellbeing.
Furthermore, AI cannot support a sustainable future if it is not itself
sustainable by design.
2. Sustainable AI
 CAPGEMINI’S RESPONSE
AI systems should benefit all human
beings. This means that their design and
development should take into careful
consideration the social and societal
impacts. Design and development must
also be mindful of future generations,
the environment, and all beings – human
and nonhuman alike – that make up
our ecosystem. They must be considered
as stakeholders throughout the AI
solution’s life cycle, so that AI solutions are
sustainable and environmentally friendly.
We support AI to address challenges
in societal areas as diverse as climate
change and CO₂ reduction, digital literacy
and inclusion, environmental protection,
health improvement, and sustainable
food production.
5
 CAPGEMINI’S RESPONSE
We embed diversity and inclusion principles
throughout the entire AI system’s life cycle:
• AI design and development teams
must be built as diverse teams,
with diversity in terms such as gender
and ethnicity, but also discipline,
for multiple perspectives during AI design,
and sensitivity to the fullest spectrum of
ethical issues.
• We seek to identify any unfair bias likely
to lead to discrimination and inappropriate
results in the context of decision making,
and present possible correction scenarios
to remove them.
• We will advise clients to put in place an
oversight process to analyze and address
the system’s purpose, constraints,
requirements, and decisions in a clear
and transparent manner.
• AI systems must entail and ensure equal
access rights and treatment by people
(regardless of ethnicity, disability,
age, religious belief, sexual orientation,
or other personal characteristics).
• As an alternative to – potentially biased –
historical training data, generated
(i.e. synthetic) data or off-the-shelf
industry data should be considered.
CASE
Capgemini Invent has developed SAIA – Sustainable
Artificial Intelligence Assistance – a demonstrator
designed to show our approach to prevent
discrimination and make AI decisions transparent
throughout the AI life cycle. It identifies potential
biases and analyzes bias behavior. It also provides
recommendations on ways to correct algorithm
biases and simulates the impact of these corrections.
 ETHICAL CHALLENGE
In order to be effective, AI needs to learn from historical data. The more data,
the more accurate an AI system will be in terms of categorizing, predicting,
prescribing, and overall decisioning. However, training data for machines,
notably statistics, may reflect an organizational or individual perspective
on a given subject matter, or a historical picture of reality. This perspective
may be biased or incorrect, as data can include various forms of bias,
resulting in extrapolations that can conflict with or undermine current trends
and desired evolutions, gradually building up over time. This can result in
discrimination against certain population groups based on gender, ethnicity,
or similar social factors.
Likewise, unfair biases and discrimination can be built in the algorithms
themselves, by design and development teams lacking appropriate diversity.
3. Fair AI
6 Capgemini Our Code of Ethics for AI
CASE
A world-leading bank for which AI is expected
to become a significant part of operations needed
to understand and compare the most popular AI
explainability methods. Capgemini designed a 4-step
approach, comparing the explainability models
on several axes (quality of results, smoothness,
source dataset impact, and consistency between the
results of each method). By putting explainability at
the heart of the project, the client will ensure that
innovation through AI is properly understood and
actionable for its teams..
Working with technologies that we
understand and control, we will provide
documentation and training to users
to explain the logic behind the functioning
of the AI and to indicate the limits
of understanding and testing scenarios,
in a manner adapted to the different
stakeholders potentially concerned.
 CAPGEMINI’S RESPONSE
AI must be transparent: its capabilities and
purpose should be openly communicated.
Decisions based on AI solutions should be
explainable, with the degree of explicability
dependent on the context and severity
of the consequences if the output is
erroneous. The data sets and processes
used for the AI solution should also be
documented to allow for traceability and,
if required, for auditability.
When interacting with an AI interface,
individuals should be aware that they are
communicating with a machine, and should
not be misled into thinking otherwise, while
being informed of the AI capabilities and
limits. Individuals interacting with AI should
be made clearly aware about the purposes
of the AI system, how it works, with whom
the information may be shared, the impact
of the AI solution and any potential impact
on their rights, if any, in relation to the AI
system at stake.
We will advise our clients to ensure with us
that systems developed are explainable,
especially regarding data selection and
treatment, notably weightings. We will
endeavor to indicate the limits that can exist
in the understanding of their functioning.
 ETHICAL CHALLENGE
The complexity of AI may amplify the “black box” concern. A “black box” is
a device, system, or program that allows input and output to be seen but gives
no view of the processes or workings between the two. For example, in tools
using artificial neural networks, hidden layers of nodes process the input
and pass their output to subsequent layers of nodes, while in deep learning,
an artificial neural network “learns” autonomously by pattern recognition.
As with a human brain, one cannot see the output between layers, how data
has been analyzed, or what has been “learnt” – one sees only the conclusion.
Where a conclusion needs to be checked and justified, because it is unexpected,
incorrect, or problematic, it can therefore be highly challenging to understand.
This is of greater concern where AI functionality plays a role in high-stakes
decision-making areas, such as banking, justice or health, where the potential
impact of decisions is more serious.
4. Transparent
and explainable AI
7
CASE
A major energy supplier wanted to improve
the management of inbound requests
from customers to enhance its service quality.
Capgemini developed and integrated semantic
analysis for emails into the information system,
allowing to automatize dispatches to the right
team. Accountability was secured by a reference
book setting out the business rules and AI
processing guidelines. It will be regularly updated
to allow for clear accountability over time.
 ETHICAL CHALLENGE
While the control responsibilities in any IT system depend on organizing
accountability, several aspects complexify this with regard to AI. The production
environment itself often involves many discrete contributors, including highly
specialized third parties, rendering in-built controllability and oversight
more difficult. Moreover, in a legal environment largely based on the
assumption of human agents, AI systems depend to a greater or lesser degree
on AI-driven intelligence, autonomous agents, and autonomous decisionmaking. Determining responsibility for AI outcomes is further complexified
by techniques such as deep learning, which can make systems hard to control
and outputs difficult to explain. While individuals need to know who will
be answerable in case of malfunction, or should a system have unintended
consequences or cause harm, tying an AI’s actions or decisions to a human,
or group of humans, can therefore present a considerable challenge.
5. Controllable AI
with clear accountability
 CAPGEMINI’S RESPONSE
Humans should keep the last say on AI,
and we should design AI solutions in such
a way that AI cannot learn to circumvent
the controls and voluntary interruptions
by humans.
The design of AI solutions should protect
the human’s autonomy and decision making.
As such, AI solutions should help humans
make more informed choices.
To ensure such respect, humans should
be part of the AI governance mechanism
in such a way that they always keep control
over AI. Appropriate measures should be
implemented from the AI solution’s design
phase, with the appropriate level of human
oversight depending on the AI solution
application area and potential risks.
AI systems cannot be the subject “per se”
of legal responsibility for their own
functioning, so the AI system design should
embed accountability rules (to identify
who is responsible for what) and trackability
principles, allowing the AI-based decisionmaking process to be explained and audited,
thus helping to identify and prevent future
mistakes or bias.
To achieve this objective, we will
advise our clients that it implies to
define and clearly identify together
roles and responsibilities amongst
the different actors involved in the design,
manufacturing, integration, deployment
and operation chain, including the
designer of the AI solution, the data
provider, and the company that adopts
the AI solution or the final user. This would
enable appropriate allocation of liability
and effective recourse when needed.
8 Capgemini Our Code of Ethics for AI
CASE
Sogeti, part of Capgemini, was requested to help
a public social insurance agency to better leverage
the sensitive data at its disposal in order to provide
a secure, efficient service to the agency’s clients.
A 6-step proof of concept was implemented
to design, train, and control machine-learning
algorithms. This ensured the robustness and
safety of the model, by allowing original data
to be preserved and data set performances to be
evaluated, for referential integrity.
 ETHICAL CHALLENGE
Like any tools or systems, those utilizing AI must be fit for their
intended purposes, and resilient and secure from a technical
perspective. As AI uptake increases, so does the scope for potential
impact, and the need to also consider the broader social and
environmental context in which AI-based tools and systems operate.
From this arises the challenge to foresee measures to safeguard
against any risks, such as unlikely mishaps or malevolent intent,
that might prevent the AI from delivering the desired benefits.
6. Robust and safe AI
 CAPGEMINI’S RESPONSE
Robustness should be embedded
throughout the life cycle of the AI systems,
from the design and development
to the deployment and use over their
lifetime. AI systems should include,
when achievable, fallback plans in case of
failure of the AI system itself (e.g. allowing
to adjust rule-based logic or even switch
to human control, to avoid any wrong
output), as well as being accurate,
reliable and having reproducible results,
to the extent allowed by applicable laws.
9
 ETHICAL CHALLENGE
With AI taking off, the need for data is greater than ever, much of it driven by consumers.
The opportunity for greater freedom presented by easily accessible data brings with
it a related risk to data protection and informational privacy. Lengthy user agreements
can tempt consumers to click “accept” without checking what rights they are giving
away, while companies can be tempted to feed consumer and vendor data into advanced,
AI-fueled algorithms, without the awareness and approval of the affected consumers
and employees. Facial recognition, voice identification systems and smart homeappliances collect data about when we come and go; while many such functions provide
a helpful service, the potential risks they carry are not trivial: Seemingly anonymized
data can be de-anonymized by AI. Data collected can also enable tracking, monitoring,
people profiling, and behavior prediction. By raising analysis of personal information
to new levels of power and speed, AI magnifies our ability to use – and misuse – personal
information, presenting a challenge for privacy and data protection.
7. AI respectful of privacy
and data protection
 CAPGEMINI’S RESPONSE
In agreement with the client, we must
ensure, that we will put in place all
the necessary means for our current
perimeter of responsibility, to contribute
to the Clients’ global AI objectives
in terms of compliance with privacy
regulations, data protection and proper
data governance.
AI and data protection are compatible
as long as data protection and
cybersecurity are taken into account
from the design phase of any AI project.
Besides ensuring full respect for privacy
and data protection laws and regulations,
adequate data governance mechanisms
should also be put in place. In practice,
this means that any AI project would need
to ensure that only data that are strictly
necessary are collected and processed.
Indeed, the data collected and used shall
be proportionate, accurate, and processed
in a secure manner.
Individuals will be provided with the relevant
level of information on how their data
is processed and they should be provided
with appropriate means to exercise their
rights as may be required by law.
10 Capgemini Our Code of Ethics for AI
1111
The information contained in this document is proprietary.
Copyright ©2021 Capgemini. All rights reserved.
Graphic design: Avant Midi. January 2021 – Version 1 – EN
The above ethical principles on AI aim to create an
ethical culture for AI and represent a commitment
to trustworthiness and ethical quality in services relating
to AI.
AI solutions developed by Capgemini are based on
ecosystems that may be composed of several
third parties. Capgemini is committed to selecting
and working with third-party providers that commit
to comply with ethical principles.
Our Code of Ethics for AI defines areas for attention
of ethical importance. These are being completed
operationally by the development of tools integrated
into service offerings and methodologies that comply
with the ethical principles set out in the present
code. Our professionals (such as AI solution architects
and project managers) are trained to fully apply the code
“by design” in all their AI-related engagements.",
7,Cisco,United States of America,Software,,,https://www.cisco.com/c/dam/en_us/about/doing_business/trust-center/docs/cisco-responsible-artificial-intelligence-framework.pdf,The Cisco Responsible AI Framework,"The Cisco Responsible
AI Framework
Security by Design / Human Rights by Design / Privacy by
Design for personal data and consequential decisions
At Cisco, we appreciate that Artificial Intelligence (AI) can be leveraged to power an inclusive future for all.
We also recognize that by applying this technology, we have a responsibility to mitigate potential harm. That
is why we have developed a Responsible AI Framework based on six principles of Transparency, Fairness,
Accountability, Privacy, Security and Reliability.
We translate these principles into controls that can be applied to model creation and the selection of
training data with Security by Design, Privacy by Design, and Human Rights by Design processes embedded
throughout the model’s lifecycle and its application in products, services, and enterprise operations. 
The Responsible AI Framework
Guidance and Oversight • Establishes a Responsible AI Committee of senior executives across Cisco business
units, sales, privacy, security, human rights, legal, government affairs, human
resources, and other functions.
• Advises Cisco on responsible AI practices and oversees Responsible AI Framework
adoption.
• Reviews sensitive or high-risk uses of AI proposed by our business units and
incident reports of bias or discrimination.
Controls • Embeds security, privacy, and human rights processes into AI design as part of
the existing Cisco Secure Development Lifecycle.
• Assesses AI functions for models and data directly involved in decisions that could
have adverse legal or human rights impact. 
• Applies controls to reduce risk of AI harm by focusing on areas like unintended bias
mitigation, model monitoring, fairness, and transparency.
Incident Management • Leverages security, data breach, and privacy incident response system to manage
reported AI incidents involving bias and discrimination.
• Assigns and reports incidents to the Responsible AI Incident Response Team to
analyze and engage relevant team for resolution.
• Tracks and reports AI incidents to governance board and reports findings and
remediation steps to the original submitter or a broader group of stakeholders,
customers, employees, and partners.
 Industry Leadership • Embeds Responsible AI as a focus area for incubation of new technology across Cisco.
• Engages with industry innovation providers focused on delivering Responsible AI.
• Participates proactively in industry forums to advance Responsible AI, including the
Centre for Information Policy Leadership, Equal AI, and the Business Roundtable on
Human Rights and AI.
 External Engagement • Works with governments to understand global perspectives on AI’s benefits and risks.
• Monitors, tracks, and influences AI-related legislation, emerging policy, and
regulations.
• Partners with and sponsors cutting-edge research institutions, exploring the
intersection of ethics and AI from technical, organizational, social, and design
perspectives.
Cisco is committed to continuing internal focus and collaboration with our external partners and stakeholders
to improve our collective understanding of the societal and human rights impact of AI. We work to continuously
improve our framework to support fair, explainable, and transparent results of the AI systems we develop and use.
Learn more about our approach to Responsible AI at the Cisco Trust Center.",
8,Facebook,United States of America,Software,06-22-2022,06-22-2022,https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/,Facebook’s five pillars of Responsible AI,"Facebook’s five pillars of Responsible AI
AI today is a core component of a vast range of technology used by billions of people around the world. Here at Facebook, it is part of systems that do everything from ranking posts in your News Feed to tackling hate speech and misinformation to responding to the COVID-19 pandemic. But, as with other emerging technologies, AI also raises many hard questions around issues such as privacy, fairness, accountability, and transparency.

The challenges raised by AI are new territory for everyone, so standards are still emerging and we admittedly don’t yet have all the answers to these important questions. We at Facebook — and we as a society — are at the beginning rather than the end of our Responsible AI journey, and finding the right approaches and then scaling them across Facebook’s large family of products and features will take time. But we recognize how important these issues are, and we’re committed to addressing them in an open and collaborative way.

That’s why Facebook created a dedicated, cross-disciplinary Responsible AI (RAI) team within its AI organization, much like we have previously invested in a dedicated Privacy team to lead our data privacy efforts and Integrity teams to enforce the policies that keep people safe on our platforms. Along with many others working across the company’s different product organizations, the RAI team is building and testing approaches to help ensure that our machine learning (ML) systems are designed and used responsibly. We are doing so with the benefit of regular consultation and collaboration with outside experts and regulators, and in line with RAI’s overall goal of ensuring that AI at Facebook benefits people and society.

It’s encouraging to see the growing momentum among governments, industry experts, and others to answer these challenges collaboratively. Most notably, the European Commission in April debuted its proposal for a risk-based approach to regulating AI. We look forward to engaging with EU lawmakers, and we welcome this proposal as a first step toward AI regulation that we hope will protect people’s rights while ensuring continued innovation and economic growth. As with other emerging technologies, we believe proactive regulation of AI is necessary. And it’s particularly important to ensure that AI governance is based on foundational values of respect for human rights, democracy, and the rule of law, particularly when countries such as China are pursuing global AI superiority unrestrained by those rights and values.

Those foundational values are at the root of the wide range of principles statements that have been released around responsible AI development, most especially the European Commission’s High-Level Expert Group’s Ethics Guidelines for Trustworthy AI and the Organization for Economic Cooperation and Development’s Principles on Artificial Intelligence, which Facebook helped develop. Facebook, in turn, has organized its Responsible AI efforts around five key pillars that were heavily influenced by those principles: Privacy & Security, Fairness & Inclusion, Robustness & Safety, Transparency & Control, and Accountability & Governance.

We’re sharing new details here on our efforts inside Facebook in this crucial area, including work from our RAI team and from across the company. While much of our work is still in early stages and we have much to do, these five pillars will guide our efforts to help ensure that Facebook uses AI responsibly.

Privacy & Security
At Facebook, we believe protecting the privacy and security of people’s data is the responsibility of everyone at the company.

That’s why we have built our cross-product Privacy Review process, by which we assess privacy risks that involve the collection, use, or sharing of people’s information. The process is also designed to help identify and mitigate the privacy risks we identify, including features and products driven by AI. We recently published an in-depth progress update on our company-wide privacy efforts, going into greater detail about our review process and the eight core Privacy Expectations that serve as its foundation.

Over the next several years, we’ll leverage the centralized tools we’ve developed to help manage the Privacy Review process to support continued investment in infrastructure improvements that will systematize the process of enforcing our privacy decisions. These changes will move more human-driven processes to automated ones that will make it easier to consistently enforce our privacy commitments across our products and services, including those driven by AI.

Additionally, in order to standardize the data flow and the development of AI for production purposes, we are unifying AI systems into one platform and ecosystem that will support all teams across the company. With a single platform, we can quickly and responsibly evolve models that perform countless inference operations every day for the billions of people that use our technologies.

Of course, AI can raise novel privacy and security concerns that go beyond questions of data infrastructure. In particular, face and speech recognition and other AI-driven technologies that make use of sensitive information have driven a great deal of privacy concern among policymakers and the public. That’s why we ask people eligible for Face Recognition to affirmatively turn it on before we recognize them, and provide clear controls to turn it off. Similarly, we allow people to turn off storage of voice interactions on their Portal devices. When stored, voice interactions improve our speech recognition algorithms, and we take strict steps to protect people’s privacy when our review team transcribes those interactions.

AI can also create new opportunities for protecting privacy, which is why we are heavily investing in research around privacy-preserving machine learning technologies like differential privacy, federated learning, and encrypted computation — and teaching the AI community how to deploy them. Our goal is to leverage this research to make our products work better for people while collecting less data and better protecting the data that we do collect. For example, the computer vision processing that allows Portal’s Smart Camera to accurately focus on people in the camera frame happens on the device. We are also publicly sharing the fruits of our research in the form of AI privacy resources like CrypTen, a tool to help researchers who aren’t cryptography experts easily experiment with ML models using secure computing techniques, and Opacus, an open source library for training ML models with differential privacy, to help advance the state of the art and improve AI privacy across the industry.

Fairness & Inclusion
At Facebook, we believe that our products should treat everyone fairly and work equally well for all people, which is why Fairness is one of the core Privacy Expectations that help guide the above mentioned Privacy Review process.

In the context of AI, our Responsible AI team has developed and is continually improving our Fairness Flow tools and processes to help our ML engineers detect certain forms of potential statistical bias in certain types of AI models and labels commonly used at Facebook, as described in our recent academic paper, with a goal of eventually scaling similar measurement to all our AI products.

The RAI team has also developed a framework for evaluating the fairness maturity of our products that is now beginning to be incorporated into the goals of all the product teams in our Facebook AI organization, and we aim to eventually require similar goals for all our product teams across the company. To help product teams formulate and meet those goals, we’ve established a multidisciplinary team of experts to offer targeted Fairness Consultations about specific products.

One fairness effort that we’re particularly proud of is our work to help ensure that the AI-driven Portal Smart Camera accurately focuses on people on-camera regardless of apparent skin tone or gender presentation. This foundational fairness work has helped inform computer vision efforts across a range of our products, and we recently released our Casual Conversations data set, composed of over 45,000 videos designed to similarly help researchers evaluate their computer vision and audio models for accuracy across a diverse set of ages, genders, apparent skin tones, and ambient lighting conditions.

Measurement is important, but fairness in AI can’t simply be reduced to a number or a checklist or a mathematical definition. What “fairness” means will often be unclear and contested, and can differ based on the particular product or context at issue. To help us consider these issues from a broad range of perspectives, Facebook’s Responsible Innovation team and Diversity, Equity & Inclusion team both facilitate input from a wide range of external experts and voices from underrepresented communities. They similarly solicit input from employees with diverse lived experiences, through both a Diversity Advisory Council and an Inclusive Product Council with a diverse range of stakeholders, to advise teams about how particular communities may be impacted by their new products and how existing products might be improved.

We are also prioritizing AI Diversity & Inclusion education efforts for our AI team when hiring and training employees, and setting clear D&I expectations for our AI managers. We aim to better ensure that the people making our AI products are from as diverse a range of backgrounds and perspectives as the people using them, and that we are inclusive of a broad range of voices in our decision-making.

Robustness & Safety
At Facebook, we believe that AI systems should meet high performance standards, and should be tested to ensure they behave safely and as intended even when they are subjected to attack.

That’s why we’ve established an AI Red Team, which partners with our product teams to test how robust our AI-powered integrity systems are against adversarial threats. We are also developing new software tools for testing and improving robustness — and then sharing them with the AI research and engineering community. For example, our open source Captum library provides state-of-the-art algorithms to understand more easily and effectively which features of an AI model built with our open source PyTorch ML framework contribute to that model’s outputs. Captum helps AI developers interpret their models, benchmark their work, and improve and troubleshoot unexpected model outputs. Captum will soon include tools to help simulate adversarial attacks, much as our Red Team adversarially tests our own models. And, just last week, we released another robustness tool called AugLy, which helps teach models to be more robust in the face of perturbations of unimportant attributes of data and focus more on the important attributes of data.

In addition to contributing to outside research on robustness, we are also learning from it. For example, leveraging methodologies proposed by external experts, we’ve developed a new framework and tools to better detect and mitigate “model drift” — that is, the degradation of a model’s predictive power due to changes in the environment.

Ensuring robustness and safety is an important challenge for all companies offering AI-driven services, not just Facebook. We believe industry needs to tackle that challenge collectively, which is why we’ve released the research and tools that we have, and why we’ve invested so much in collaborative efforts around adversarial AI testing. For example, we came together with the Partnership on AI, Microsoft, leading academics, and others to create the Deepfake Detection Challenge. That competition drew more than 2,000 participants who trained their deepfake detection models using a unique new data set created by Facebook. Likewise, we created and shared a data set of more than 10,000 new multimodal examples for the Hateful Memes Challenge, a first-of-its-kind competition to help build AI models that better detect multimodal hate speech. And most recently, to further speed up innovation across the field of AI robustness testing, we hosted a virtual cross-industry event in May — the “AI Red Table” — to facilitate the sharing of best practices and learnings between AI industry leaders.

Transparency & Control
At Facebook, we believe that the people who use our products should have more transparency and control around how data about them is collected and used, which is why it is one of our eight core Privacy Expectations.

Beyond privacy, in the context of Responsible AI, we are striving to be more transparent about when and how AI systems are making decisions that impact the people who use our products, to make those decisions more explainable, and to inform people about the controls they have over how those decisions are made.

That’s why we’ve introduced a number of tools over the years to increase transparency around why people see which News Feed content and ads (Why Am I Seeing This, or WAIST, tools), and to provide additional transparency and control over the data and off-Facebook activity that may impact how we select the News Feed items and ads that we think will be most relevant to them. We are also giving people more control over how our AI systems rank content in your News Feed, including giving you more control over which favorite friends or Pages should influence your ranking, or even letting you turn off AI-driven News Feed personalization altogether.

Transparency isn’t just important for the people using our products, though. We also look forward to discussing more about how our models work with outside experts and regulators — and doing so in a way that preserves privacy and doesn’t reveal trade secrets. That is why, for example, we launched in May a new online Transparency Center, where we’ll be publishing information about the News Feed ranking process, including more information about some of the important signals in that process, as well as providing updates on significant changes to those ranking algorithms.

The Responsible AI team is also collaborating with other product teams and building on previous academic research and industry efforts to develop Facebook’s own method for creating simple, standardized documentation of our models, in a form commonly known as model cards. The Instagram Equity team has made the most progress in this effort so far, already using model cards across Instagram’s integrity systems and aiming to apply model cards to all Instagram models before the end of next year.

We are developing these approaches to model documentation in dialogue with other related efforts across the industry. For example, we support the About ML initiative at the Partnership on AI, which aims at developing industry standards in this area. We also held two workshops this spring through TTC Labs to bring together AI designers and AI policy experts to help chart the future of AI model documentation.

We understand that experts, regulators, and everyday people all are eager to more easily understand why AI systems make the decisions they make. There are many challenges in explaining the predictions of complex AI systems, but we are working to push the limits, including with interpretability software such as Captum. Although work in this area is still in its infancy, our hope is that ultimately we will be able to build an integrated transparency solution that can automatically feed information from internal documentation efforts like model cards into new transparency features and controls for the people using our products.

Accountability & Governance
At Facebook, we believe in building reliable processes to ensure accountability for our AI systems and the decisions they make.

This requires building governance systems to ensure that our AI systems are performing to high standards, to satisfy external expectations and internal best practices, and to identify and mitigate any potential negative impacts those systems might pose. It also means making sure that wherever necessary or appropriate, humans are able to monitor these systems and intervene when necessary.

That’s why we’re doing the work described in this blog post, from investing in our Privacy Review efforts, to developing approaches and tools to improve our understanding and ability to address concerns about our AI systems, to increasing transparency and control around our AI products and features. And that’s why, except for removals of some content posing extreme safety concerns, we give people a way to appeal and seek additional human review of a broad range of content-takedown decisions, which are sometimes first made with the assistance of AI systems. We hold ourselves accountable through quarterly Community Standards Enforcement Reports and through an independent Oversight Board that considers further appeals of both removal and nonremoval of content and makes binding rulings around our most difficult and important content decisions.

We also recognize that quickly evolving technologies like AI can raise new and unanticipated issues, so we must continually improve our processes for identifying and mitigating negative impacts. To help address potential harms early in the product cycle, Facebook’s Responsible Innovation team provides foresight workshops, office hours, and extended collaborations to help product teams identify and brainstorm solutions to a wide range of individual and societal risks. Meanwhile, the Responsible AI team has begun developing its own AI-specific impact assessment framework that we hope will complement our existing launch review processes. We understand the importance of thinking holistically about how our AI products affect society, and we’re continually experimenting with new ways to educate and inspire our engineers to consider the big picture and weigh the long-term impact of their work.

Collaborating on the future of Responsible AI
Because this is still a relatively new field, there are not yet clearly defined standards and processes for AI governance and for assessing potential negative AI-related impacts. We — not just Facebook but also the tech industry, the AI research community, policymakers, advocacy groups, and others — need to collaborate on figuring out how to make AI impact assessment work at scale, based on clear and reasonable standards, so that we can identify and address potential negative AI-related impacts while still creating new AI-powered products that will benefit us all. We must similarly collaborate on developing basic practical standards around AI fairness, privacy, robustness, and transparency before they are codified into law.

That’s why we are prioritizing a broad range of collaborative research around AI best practices. For example, we’re actively participating in efforts to establish clear AI principles and best practices, including collaborating with the OECD’s new AI Observatory project to study and disseminate emerging best practices that are in line with its 2019 AI principles. Through our recently launched Open Loop partnership, we’re building innovative “policy prototyping” projects for testing new potential AI policy requirements with regulators and startups before they become law, to ensure that they are both practical and impactful; we have already launched projects in Europe, Asia, and Latin America, with more to come. We’re funding a global effort to solicit diverse academic research on AI ethics issues, supporting projects in Asia, Africa, and Latin America and providing foundational support for an independent Institute for Ethics in Artificial Intelligence at the Technical University of Munich. And we’re a founding partner in the Partnership on AI, the premier cross-industry, cross-civil-society multistakeholder forum for collaboratively developing AI best practices.

Of course, the people who use our products and services are another key stakeholder in how we chart our course around Responsible AI, and their lived experiences are another critical form of feedback to factor into our thinking. That’s why we have an integrated user research practice that allows us to better understand the core needs of the people using our products and help ensure that we are building technology and experiences that benefit people, society, and the world.

Maintaining a focus on the needs of the people using our products while working together across governments, industries, and broader AI expert communities in academia and civil society, Facebook aims to help chart a course for the future of Responsible AI development that leads to a safer, fairer, and more prosperous society for all. We look forward to that journey and to sharing more about our own Responsible AI practices as they evolve and grow.",
9,FUJIFILM,Japan,Hardware,12-01-2022,12-01-2022,https://holdings.fujifilm.com/en/sustainability/vision/policy/ai,Fujifilm Group AI Policy,"Fujifilm Group AI Policy
The Fujifilm Group’s corporate philosophy is to contribute to the advancement of culture, science, technology and industry, as well as improved health and environmental protection in society, thereby helping enhance the quality of life of people worldwide. In line with this philosophy, we are combining AI (Artificial Intelligence) with our advanced proprietary technologies, nurtured in a variety of business fields, to create new value, bringing greater comfort and convenience to people’s lives and resolving various issues faced by society.
We recognize that AI, which is still under development, is capable of delivering a wide range of benefits, but contains a number of issues that must also be taken into account, including ethical issues. The rapid advancement of AI technology is also altering social norms and values. Under Fujifilm’s open, fair and clear corporate culture, as defined in the Group’s Vision, we continue to focus on earnest discussions and collaborations with internal and external stakeholders to build mutual trust, and will take on challenges to deliver values that can be created with AI. The Fujifilm Group aims to leverage AI technology to contribute to building a better society, based on the policy set forth below.
1．Accelerating New Value Creation
In its efforts to bring about safe, robust and easy-to-use products and services, the Fujifilm Group will actively promote the use of AI to ensure that more and more people can enjoy the benefits of AI technology. By promoting the integration of AI into the advanced proprietary technologies the Group has developed, and through co-creation with external partners, the Fujifilm Group will generate new values in a speedy manner to lead the world in resolving societal issues in healthcare and a wide range of other fields. We will also actively utilize AI in various work processes including R&D and production in company-wide efforts to streamline operations and rapidly deliver values to meet societal expectations.
2．Respecting Human Rights
The Fujifilm Group recognizes not only the potential benefits that the advancement of AI technology could bring but also bias, lack of fairness, discrimination and other risks that could occur through the use of AI. We believe that the use of AI be strictly monitored to ensure that dependence on AI does not lead to restrictions or denial of human dignity, ability and potential, or pose a threat to human lives or physical health and wellbeing. The Fujifilm Group will work on developing and supplying products and services that respect basic human rights, preventing the abuse or misuse of AI that could lead to such risks, be it intentional or incidental.
3．Ensuring Fair and Appropriate Use of AI
The Fujifilm Group recognizes that the use of AI has the capacity to create bias on data and algorithms that are used as sources of information for AI. In order to ensure fairness and appropriate application of the scope and methods involved in the use of AI, we will anticipate possible scenarios that could occur through diverse use of AI in the development and supply of products and services that employ AI, while developing an internal structure for appropriate verification. We will also constantly review the scope of our use of AI in line with the advancement of AI technology.
4．Managing Information Security
The Fujifilm Group recognizes the possibility that data collected and utilized in AI use could affect the rights and interests of individuals and organizations. We therefore continue to work on securing privacy and security to ensure that data is used appropriately. With respect to the handling of personal information, the Fujifilm Group will apply its privacy policy to ensure appropriate management and administration.
5．Ensuring Transparency
The Fujifilm Group endeavors to maintain appropriate communication in good faith to fulfill its accountability in relation to the use of AI. Through such communications to ensure transparency, we will strive to gain a high level of trust from our stakeholders, which will lead to further enhancing the accuracy of data to be obtained and thus developing more desirable products and services that utilize AI.
6．Developing Human Resource
To actively promote the use of AI, the Fujifilm Group provides in-house AI literacy education and training for human resource development, targeting researchers, developers and a wide range of other staff. We aim to foster human resources that possess a comprehensive understanding of the benefits and risks of AI use, and are capable of using AI to take on new challenges to enable the Fujifilm Group to deliver products and services that prove to be truly useful to society.",
10,Fujitsu Ltd.,Japan,Hardware,03-01-2019,03-01-2019,https://www.fujitsu.com/global/documents/about/csr/humanrights/fujitsu-group-ai-commitment-201903_en.pdf,Fujitsu Group AI Commitment,"Fujitsu Group AI Commitment
Progress and innovation in the realm of advancing information and communication technologies
(ICT), especially artificial intelligence (AI), are dramatically changing the way and the society we
live in. By analyzing enormous amount of data arising continuously, we are finding new ways of
helping to transform industries such as manufacturing, finance, healthcare, transportation, logistics
and agriculture, and working to resolve environmental issues such as water shortage, global
warming and desertification.
Meanwhile, there are some questions and concerns about unanticipated side effects including
discrimination. Fujitsu Group (Fujitsu) desires to build a more prosperous and better tomorrow
where human dignity is respected. We, as a developer and provider of AI solutions, firmly believe
it crucial to find the way to use AI not only for convenience in life but also safety and security in use.
In 2009, Fujitsu has promoted a concept: “Human Centric”, where ICT is centered on people.
Moreover, in 2015, Fujitsu enhanced its concept for AI: Human Centric AI which autonomously
collaborates with people. The concept already included ethical value around the use of AI. In order
to turn Fujitsu‘s vision and concept of Human Centric AI into action, we hereby establish the “Fujitsu
Group AI Commitment” as our core principles. With this commitment, Fujitsu, as a developer and
a provider of AI solutions, continues to be the most reliable business partner to support enterprises
business transformation. We will continue the dialogue with our customers, their customers (enduser) and other stakeholders including external experts and then make prosperity brought by AI
spread widely to the world. Furthermore, we will establish a new special committee including
external experts to survey this commitment. We believe its objective opinion will strengthen our
corporate governance.
1. Provide value to customers and society with AI:
Fujitsu and its whole global group companies respect co-creation with customers by using
emerging technologies. We are working together with customers for their prosperous tomorrow.
At the same time, we consider impacts to end-users and society brought by continuously
evolving AI.
2. Strive for Human Centric AI:
Fujitsu, advocating “Human Centric AI”, treats people as ends in themselves, not as means.
To achieve this objective, Fujitsu, respecting diversity and inclusion, commits to use AI as a
tool to support people’s desire to seek prosperity and contribution for society. As part of this
effort, Fujitsu will seek trustworthy AI through considering fairness and safety to prevent
discrimination and harm.
3. Strive for a sustainable society with AI:
Fujitsu has been strongly committed to Sustainable Development Goals (SDGs). Fujitsu
challenges various social issues and environmental issues and thus contribute for building
better society and long term business success of our customers.
4. Strive for AI that respects and supports people’s decision making:
Fujitsu believes it is crucial to protect the intrinsic value of human choice on suggestions and
results brought by AI. To this end, Fujitsu will strive for designing and developing AI that can
explain key reasons for why it makes specific recommendations so that humans can make
informed decisions based on such AI.
5. As corporate responsibility, emphasize transparency and accountability for AI:
As an information and communication technology provider responsible for reliability of
infrastructure systems, Fujitsu understands it is critical to avoid serious consequences which
can be caused by AI operated in unexpected conditions. To this end, Fujitsu commits to
leverage its accumulated experience and know how to develop and constantly improve
reliability of AI. Moreover, preparing for the unlikely event of serious consequences occurring,
Fujitsu will seek appropriate measures to track the causes and effects of any such situation.",Relevant Page: https://www.fujitsu.com/global/about/research/technology/aiethics/
11,Hewlett Packard Enterprise,United States of America,Hardware,,,https://www.hpe.com/us/en/solutions/artificial-intelligence/ethics.html,HPE AI ETHICS AND PRINCIPLES,"HPE SUPPORTS AI ETHICS FOR GOOD
Artificial Intelligence (AI) is a powerful, transformative technology that can amplify human capabilities, but also presents risks. Investing in AI ethics and its principles is a responsibility that HPE takes seriously, and we believe in ethical and responsible AI principles:
AI privacy-enabled security
Respect individuals privacy, be secure and minimize the risk of errors and unintended, malicious use.
AI human focused principle
Respect human rights and be designed with mechanisms and safeguards, such as human oversight to prevent misuse.
AI inclusivity principle
Be inclusive, minimize harmful bias, ensure fair and equal treatment and access for individuals.
AI robust principle
Be engineered to build in quality testing, include safeguards to maintain functionality, and minimize misuse and impact of failure.
Responsible AI
Be designed to enable responsible and accountable use, allow an understanding of AI and for outcomes to be challenged.",Blog: https://www.hpe.com/us/en/newsroom/blog-post/2021/04/innovation-in-the-ethics-of-ai-at-hpe.html
12,HP,United States of America,Computer,,,https://www.hpe.com/us/en/solutions/artificial-intelligence/ethics.html,HPE AI ETHICS AND PRINCIPLES,"HPE SUPPORTS AI ETHICS FOR GOOD
Artificial Intelligence (AI) is a powerful, transformative technology that can amplify human capabilities, but also presents risks. Investing in AI ethics and its principles is a responsibility that HPE takes seriously, and we believe in ethical and responsible AI principles:
AI privacy-enabled security
Respect individuals privacy, be secure and minimize the risk of errors and unintended, malicious use.
AI human focused principle
Respect human rights and be designed with mechanisms and safeguards, such as human oversight to prevent misuse.
AI inclusivity principle
Be inclusive, minimize harmful bias, ensure fair and equal treatment and access for individuals.
AI robust principle
Be engineered to build in quality testing, include safeguards to maintain functionality, and minimize misuse and impact of failure.
Responsible AI
Be designed to enable responsible and accountable use, allow an understanding of AI and for outcomes to be challenged.",
13,IBM,United States of America,Consulting,,,https://www.ibm.com/policy/wp-content/uploads/2022/04/IBM-2020-Public-Policy-and-Legislative-Priorities-2.pdf,IBM's Principles of Trust and Transparency here.,"innovation and protect consumers by regulating AI based on use-cases and end users, rather than the underlying
technology. We support policies that advance the five business imperatives that we have called for in our IBM Policy
Lab piece - having an AI ethics official; different rules for different risks; don’t hide your AI, and test your AI. That is
why we supported the National AI Initiative Act which directs NIST to develop an accountability framework to
advance explainable fair and trustworthy AI.
Contact: Ryan Hagemann, Ryan.Hagemann@ibm.com & Kevin Walsh, kevin.j.walsh@ibm.com",
14,Infosys,India,Consulting,,,https://www.infosys.com/content/dam/infosys-web/en/techcompass/responsible-ai.html,Responsible AI,"Trend 13: AI ethics throughout the development lifecycle
Responsible AI concepts should be factored in from the beginning to ensure the business stays out of any AI ethics and bias issues. Explainability is one such critical concept. The design and development teams should be aware and informed of every step in the AI lifecycle to answer any related questions, providing all information AI users would seek to understand how and why the system made a decision. This way, an organization can remain clear of adverse ethical issues and maintain customer trust.

These scenarios demand efficient tools to make AI systems more transparent and interpretable, ensuring trust, fairness, transparency, reliability, and auditability. AI models should adhere to the following principles:

Purposeful: An AI system should be designed with empathy and follow a human-centric approach with socially responsible use cases. For example, consider user preferences and behavior to provide recommendations.
Ethical: Models should comply with legal and social structures and be designed with high-cost functions that prevent unethical behavior. There should be transparency in data and models.
Human reviewed: Although AI models are built to operate independently without human interference, human dependency is a necessity in some cases. For example, in fraud detection or cases where law enforcement is involved, human supervision is required to review decisions made by AI models.
Bias detection: An unbiased dataset is an important prerequisite for reliable and nondiscriminatory predictions. AI models are being used for credit scoring by banks, resume shortlisting, and in some judicial systems. However, some datasets were found with an inherent bias toward color, age, and/or sex.
Explainable: Models should enable easy interpretation of results such as predictions, recommendations, etc. Explainable AI helps understand the decision-making process of AI systems and recognize which features of the given input are emphasized while making predictions.
Accountable: Models should use telemetry for auditing all human and machine actions. There should be data lineage for traceability, and all models/datasets should be version controlled.
Reproductive: The ML model should be consistent when giving predictions. Many practitioners think that explainable AI (XAI) is applied only at the output stage, but the role of XAI is throughout the whole AI lifecycle.
Thus, consistent and continuous governance can make AI systems understandable and resilient in various situations.",Relevant Page: https://www.infosys.com/ai/ai-applied/responsible-adoption.pdf
15,Intel,United States of America,Hardware,,,https://www.intel.com/content/www/us/en/artificial-intelligence/responsible-ai.html,Responsible AI Pillars,"Intel is committed to advancing AI technology responsibly. We do this by utilizing rigorous, multidisciplinary review processes throughout the development lifecycle, establishing diverse development teams to reduce biases, and collaborating with industry partners to mitigate potentially harmful uses of AI. We are committed to implementing leading processes founded on international standards and industry best practices. AI has come a long way but there is still so much more to be discovered, as technology evolves. We are continuously finding ways to use this technology to drive positive change and better mitigate risks. We continue to collaborate with academia and industry partners to advance research in this area while also evolving our platforms to make responsible AI solutions computationally tractable and efficient.",Relevant Page: https://www.intel.com/content/www/us/en/artificial-intelligence/ai4socialgood.html
16,Leidos,United States of America,Software,,,https://www.leidos.com/enabling-technologies/artificial-intelligence-machine-learning,Trusted AI/ML,"We deliver trusted artificial intelligence and machine learning (AI/ML) solutions that provide real and immediate value. Our cutting-edge technology makes AI/ML algorithms trustworthy, resilient, and secure. When combined with tools that provide transparency and eliminate bias, we offer mission-focused AI/ML solutions that are trusted and valued by customers and built on our deep experience.",
17,Microsoft,United States of America,Software,06-01-2022,06-01-2022,https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl,The Microsoft Responsible AI Standard ,"Preview – Microsoft Responsible AI Standard v2 – Introduction
1
Microsoft
Responsible AI
Standard, v2
GENERAL REQUIREMENTS
FOR EXTERNAL RELEASE
June 2022

About this release
When we embarked on our effort to operationalize Microsoft’s six AI principles, we knew there was a policy
gap. Laws and norms had not caught up with AI’s unique risks or society’s needs. Yet, our product development
teams needed concrete and actionable guidance as to what our principles meant and how they could uphold
them. We leveraged the expertise on our research, policy, and engineering teams to develop guidance on how
to fill that gap.
The Responsible AI Standard is the product of a multi-year effort to define product development requirements
for responsible AI. We are making available this second version of the Responsible AI Standard to share what
we have learned, invite feedback from others, and contribute to the discussion about building better norms
and practices around AI.
While our Standard is an important step in Microsoft’s responsible AI journey, it is just one step. As we make
progress with implementation, we expect to encounter challenges that require us to pause, reflect, and adjust.
Our Standard will remain a living document, evolving to address new research, technologies, laws, and
learnings from within and outside the company.
There is a rich and active global dialog about how to create principled and actionable norms to ensure
organizations develop and deploy AI responsibly. We have benefited from this discussion and will continue to
contribute to it. We believe that industry, academia, civil society, and government need to collaborate to
advance the state-of-the-art and learn from one another. Together, we need to answer open research
questions, close measurement gaps, and design new practices, patterns, resources, and tools.
As we continue our journey, we welcome feedback on our approach and insights on other ways forward:
https://aka.ms/ResponsibleAIQuestions
Microsoft Responsible AI Standard v2
Accountability Goals
Goal A1: Impact assessment
Microsoft AI systems are assessed using Impact Assessments.
Applies to: All AI systems.
Requirements
A1.1 Assess the impact of the system on people, organizations, and society by completing an Impact Assessment
early in the system’s development, typically when defining the product vision and requirements. Document the
effort using the Impact Assessment template provided by the Office of Responsible AI.
Tags: Impact Assessment.
A1.2 Review the completed Impact Assessment with the reviewers identified according to your organization’s
compliance process before development starts. Secure all required approvals from those reviewers.
Tags: Impact Assessment.
A1.3 Update and review the Impact Assessment at least annually, when new intended uses are added, and before
advancing to a new release stage.
Tags: Impact Assessment.
Microsoft Responsible AI Standard v2
Goal A2: Oversight of significant adverse impacts
Microsoft AI systems are reviewed to identify systems that may have a significant adverse impact on people,
organizations, and society, and additional oversight and requirements are applied to those systems.
Applies to: All AI systems.
Requirements
A2.1 Review defined Restricted Uses to determine whether the system meets the definition of any Restricted Use.
If it does, document this in the Impact Assessment, and follow the requirements for the Restricted Use.
Tags: Impact Assessment.
A2.2 Answer prompts in the Impact Assessment template to determine whether the system meets the definition of
a Sensitive Use. If it does, report it to the Office of Responsible AI, and follow any additional requirements resulting
from a Sensitive Uses review.
Tags: Impact Assessment.
A2.3 Review your systems at least annually against the definitions for Sensitive Uses and Restricted Uses. If there
are systems that meet the criteria for Sensitive Uses, report them to the Office of Responsible AI. If there are
systems that meet the criteria for Restricted Uses, notify the Office of Responsible AI.
Microsoft Responsible AI Standard v2
Goal A3: Fit for purpose
Microsoft AI systems are fit for purpose in the sense that they provide valid solutions for the problems they are
designed to solve.
Applies to: All AI systems.
Requirements
A3.1 Document in the Impact Assessment how the system’s use will solve the problem posed by each intended use,
recognizing that there may be multiple valid ways in which to solve the problem.
Tags: Impact Assessment.
A3.2 Define and document for each model in the AI system:
1) the model’s proposed inputs and how well they represent the concepts they are intended to represent; include
analysis of the limitations of this representation,
2) the model’s proposed output and how well it represents the concept it is intended to represent; include analysis
of the limitations of this representation, and
3) limitations to the generalizability of the resulting model based on the training and testing data that will be used.
A3.3 Define and document Responsible Release Criteria for this Goal. Include:
1) a concise definition of the problem being solved in the intended use,
2) performance metrics and their Responsible Release Criteria, and
3) error types and their Responsible Release Criteria.
A3.4 Document an evaluation plan for each of the performance metrics and error types.
Tags: Ongoing Evaluation Checkpoint.
A3.5 Use the methods defined in requirement A3.4 to conduct evaluations. Document the pre-release results of the
evaluations. Determine and document how often ongoing evaluation should be conducted to continue supporting this
Goal.
Tags: Ongoing Evaluation Checkpoint.
A3.6 Provide documentation to customers which describes the system’s:
1) intended uses, and
2) evidence that the system is fit for purpose for each intended use.
When the system is a platform service made available to external customers or partners, include this information in the
required Transparency Note.
Tags: Transparency Note.
A3.7 If an intended use is not supported by evidence, or if evidence comes to light that refutes that the system is fit for
purpose for the intended use at any point in the system’s use:
1) remove the intended use from customer-facing materials and make current customers aware of the issue, take
action to close the identified gap, or discontinue the system,
2) revise documentation related to the intended use, and
3) publish the revised documentation to customers.
When the system is a platform service made available to external customers or partners, include this information in the
required Transparency Note.
A3.8 Communicate with care about system benefits; follow any applicable guidance from your attorney.
Microsoft Responsible AI Standard v2
Goal A4: Data governance and management
Microsoft AI systems are subject to appropriate data governance and management practices.
Applies to: All AI systems.
Requirements
A4.1 Define and document data requirements with respect to the system’s intended uses, stakeholders, and the
geographic areas where the system will be deployed. Document these requirements in the Impact Assessment.
Tags: Impact Assessment.
A4.2 Define and document procedures for the collection and processing of data, to include annotation, labelling,
cleaning, enrichment, and aggregation, where relevant.
A4.3 If you plan to use existing data sets to train the system, assess the quantity and suitability of available data
sets that will be needed by the system in relation to the data requirements defined in A4.1. Document this
assessment in the Impact Assessment.
Tags: Impact Assessment.
A4.4 Define and document methods for evaluating data to be used by the system against the requirements
defined in A4.1.
A4.5 Evaluate all data sets using the methods defined in requirement A4.4. Document the results of the evaluation. 
Microsoft Responsible AI Standard v2
8
Goal A5: Human oversight and control
Microsoft AI systems include capabilities that support informed human oversight and control.
Applies to: All AI systems.
Requirements
A5.1 Identify the stakeholders who are responsible for troubleshooting, managing, operating, overseeing, and
controlling the system during and after deployment. Document these stakeholders and their oversight and control
responsibilities using the Impact Assessment template.
Tags: Impact Assessment.
A5.2 Identify the system elements (including system UX, features, alerting and reporting functions, and
educational materials) necessary for stakeholders identified in requirement A5.1 to effectively understand their
oversight responsibilities and carry them out. Stakeholders must be able to understand:
1) the system’s intended uses,
2) how to effectively execute interactions with the system,
3) how to interpret system behavior,
4) when and how to override, intervene, or interrupt the system, and
5) how to remain aware of the possible tendency of over-relying on outputs produced by the system
(“automation bias”).
Document the system design elements that will support relevant stakeholders for each oversight and control
function.
A5.3 When possible, design the system elements identified in A5.2. When this is not possible (for example, when
Microsoft is not responsible for the system UX), provide guidance on human oversight considerations to the third
party responsible for implementing the system elements identified in A5.2.
A5.4 Define and document the method to be used to evaluate whether each oversight or control function can be
accomplished by stakeholders in realistic conditions of system use. Include the metrics or rubrics that will be used
in the evaluations. When this is not possible (for example, when Microsoft is not responsible for oversight and
control functions), provide guidance on evaluating oversight and control functions to the third party responsible
for evaluating oversight or control functions.
A5.5 Define and document Responsible Release Criteria to achieve this Goal.
A5.6 Conduct evaluations defined by requirement A5.4 using a near-release version of the system. Document the
results.
A5.7 If there are Responsible Release Criteria for metrics or rubrics that have not been met, consult with the
reviewers named in the Impact Assessment, and in the case of Sensitive Uses, with the Office of Responsible AI, to
develop a plan detailing how the gap will be managed until it can be closed. Document that plan.
Tools and practices
Recommendation A5.3.1 Follow the Guidelines for Human-AI Interaction when designing the system.
Recommendation A5.4.1 Assign user researchers to design these evaluations.
Microsoft Responsible AI Standard v2
9
Transparency Goals
Goal T1: System intelligibility for decision making
Microsoft AI systems that inform decision making by or about people are designed to support stakeholder needs for
intelligibility of system behavior.
Applies to: All AI systems when the intended use of the generated outputs is to inform decision making by or
about people.
Requirements
T1.1 Identify:
1) stakeholders who will use the outputs of the system to make decisions, and
2) stakeholders who are subject to decisions informed by the system.
Document these stakeholders using the Impact Assessment template.
Tags: Impact Assessment.
T1.2 Design the system, including, when possible, the system UX, features, reporting functions, and educational
materials, so that stakeholders identified in requirement T1.1 can:
1) understand the system’s intended uses,
2) interpret relevant system behavior effectively (i.e., in a way that supports informed decision making), and
3) remain aware of the possible tendency of over-relying on outputs produced by the system (""automation
bias"").
For the two categories of stakeholders identified in requirement T1.1, document:
1) how the system design will support their understanding of the system’s intended uses, and
2) how the system aids their ability to interpret relevant system responses, and
3) how the system design discourages automation bias.
T1.3 Define and document the method to be used to evaluate whether each stakeholder who will make decisions
or be subject to decisions based on the behavior of the system can interpret the relevant system responses
reasonably well. Include the metrics or rubrics that will be used in the evaluations.
Tags: Ongoing Evaluation Checkpoint.
T1.4 Define and document a Responsible Release Plan, to include Responsible Release Criteria to achieve this
Goal.
Tags: Ongoing Evaluation Checkpoint.
T1.5 Conduct evaluations defined by requirement T1.3. Document the pre-release results of the evaluations.
Determine and document how often ongoing evaluation should be conducted to continue supporting this Goal.
Tags: Ongoing Evaluation Checkpoint.
T1.6 If there are Responsible Release Criteria for metrics or rubrics that that have not been met, consult with the
reviewers named in the Impact Assessment, and in the case of Sensitive Uses, with the Office of Responsible AI, to
develop a plan detailing how the gap will be managed until it can be closed. Document that plan.
Microsoft Responsible AI Standard v2
10
Tools and practices
Recommendation T1.2.1 Follow the Guidelines for Human-AI Interaction when designing the system.
Recommendation T1.2.2 Use one or more techniques available as part of the Interpret ML toolkit to understand
the impact of features on system behavior. This may help stakeholders who need to understand model predictions.
Recommendation T1.3.1 Assign user researchers to define, design, and prioritize evaluations in appropriately
realistic contexts of use.
Microsoft Responsible AI Standard v2
11
Goal T2: Communication to stakeholders
Microsoft provides information about the capabilities and limitations of our AI systems to support stakeholders in
making informed choices about those systems.
Applies to: All AI systems.
Requirements
T2.1 Identify:
1) stakeholders who make decisions about whether to employ a system for particular tasks, and
2) stakeholders who develop or deploy systems that integrate with this system.
Document these stakeholders in the Impact Assessment template.
Tags: Impact Assessment.
T2.2 Publish documentation for the system so that stakeholders defined in T2.1 can understand the system.
Include:
1) capabilities,
2) intended uses,
3) uses that require extra care or guidance,
4) operational factors and settings that allow for effective and responsible system use,
5) limitations, including uses for which the system was not designed or evaluated, and
6) evidence of system accuracy and performance as well as a description of the extent to which these results
are generalizable across use cases that were not part of the evaluation.
When the system is a platform service made available to external customers or partners, a Transparency Note is
required.
Tags: Transparency Note.
T2.3 Review and update documentation annually or when any of the following events occur:
1) new uses are added,
2) functionality changes,
3) the product moves to a new release stage,
4) new information about reliable and safe performance becomes known as defined by requirement RS3.3, or
5) new information about system accuracy and performance becomes available.
When the system is a platform service made available to external customers or partners, include this information in
the required Transparency Note.
Tags: Transparency Note.
Microsoft Responsible AI Standard v2
12
Goal T3: Disclosure of AI interaction
Microsoft AI systems are designed to inform people that they are interacting with an AI system or are using a system
that generates or manipulates image, audio, or video content that could falsely appear to be authentic.
Applies to: AI systems that impersonate interactions with humans, unless it is obvious from the circumstances
or context of use that an AI system is in use. AI systems that generate or manipulate image, audio, or video
content that could falsely appear to be authentic.
Requirements
T3.1 Identify stakeholders who will use or be exposed to the system, in accordance with the Impact Assessment
requirements. Document these stakeholders using the Impact Assessment template.
Tags: Impact Assessment.
T3.2 Design the system, including system UX, features, reporting functions, educational materials, and outputs so
that stakeholders identified in T3.1 will be informed of the type of AI system they are interacting with or exposed
to. Ensure that any image, audio, or video outputs that are intended to be used outside the system are labelled as
being produced by AI.
T3.3 Define and document the method to be used to evaluate whether each stakeholder identified in T3.1 is
informed of the type of AI system they are interacting with or exposed to.
Tags: Ongoing Evaluation Checkpoint.
T3.4 Define and document Responsible Release Criteria to achieve this Goal.
Tags: Ongoing Evaluation Checkpoint.
T3.5 Conduct evaluations defined by requirement T3.3. Document the pre-release results of the evaluations.
Determine and document how often ongoing evaluation should be conducted to continue supporting this goal.
Tags: Ongoing Evaluation Checkpoint.
Microsoft Responsible AI Standard v2
13
Fairness Goals
Goal F1: Quality of service
Microsoft AI systems are designed to provide a similar quality of service for identified demographic groups,
including marginalized groups.
Applies to: AI systems when system users or people impacted by the system with different demographic
characteristics might experience differences in quality of service that Microsoft can remedy by building the system
differently.
Requirements
F1.1 Identify and prioritize demographic groups, including marginalized groups, that may be at risk of
experiencing worse quality of service based on intended uses and geographic areas where the system will be
deployed. Include:
1) groups defined by a single factor, and
2) groups defined by a combination of factors.
Document the prioritized identified demographic groups using the Impact Assessment template.
Tags: Impact Assessment.
F1.2 Evaluate all data sets to assess inclusiveness of identified demographic groups and collect data to close gaps.
Document this process and its results.
F1.3 Define and document the evaluation that you will perform to support this Goal. Include:
1) any system components to be evaluated, in addition to the whole system,
2) the metrics to be used to evaluate the system components and the whole system, and
3) a description of the data set to be used for this evaluation.
Tags: Ongoing Evaluation Checkpoint.
F1.4 Define and document Responsible Release Criteria to achieve this Goal, as follows:
For each metric, document:
1) any target minimum performance level for all groups, and
2) the target maximum (absolute or relative) performance difference between groups.
Tags: Ongoing Evaluation Checkpoint.
F1.5 Evaluate the system according to the defined Responsible Release Criteria.
Tags: Ongoing Evaluation Checkpoint.
F1.6 Reassess the system design, including the choice of training data, features, objective function, and training
algorithm, to pursue the goals of:
1) improving performance for any identified demographic group that does not meet any target minimum
performance level, and
2) minimizing performance differences between identified demographic groups, paying particular attention
to those that exceed the target maximum, while recognizing that doing so may appear to affect system
performance and that it is seldom clear how to make such tradeoffs.
Consult with your attorney to determine your approach to this, including how you will identify and document
tradeoffs.
Microsoft Responsible AI Standard v2
14
Tags: Ongoing Evaluation Checkpoint.
F1.7 Identify and document any justifiable factors, such as circumstantial and other operational factors (e.g.,
“background noise” for speech recognition systems or “image resolution” for facial recognition systems), that
account for:
1) any inability to meet any target minimum performance level for any identified demographic group, and
2) any remaining performance differences between identified demographic groups.
Tags: Ongoing Evaluation Checkpoint.
F1.8 Document the pre-release results from requirements F1.4, F1.5, and F1.6. Determine and document how often
ongoing evaluation should be conducted to continue supporting this Goal.
Tags: Ongoing Evaluation Checkpoint.
F1.9 Publish information for customers about:
1) identified demographic groups for which performance may not meet any target minimum performance
level,
2) any remaining performance disparities between identified demographic groups that may exceed the target
maximum, and
3) any justifiable factors that account for these performance levels and differences.
When the system is a platform service made available to external customers or partners, include this information in
the required Transparency Note.
Tags: Transparency Note.
Microsoft Responsible AI Standard v2
15
Tools and practices
Recommendation F1.1.1 For identifying people by age, gender identity, and ancestry in North America, use Best
Practices for Age, Gender Identity, and Ancestry.
Recommendation F1.1.2 Work with user researchers to understand variations in demographic groups across
intended uses and geographic areas.
Recommendation F1.1.3 Work with domain-specific subject matter experts to understand the factors that impact
performance of your system and how they vary across identified demographic groups in this domain.
Recommendation F1.1.4 Work with members of identified demographic groups to understand the risks of and
impacts associated with differences in quality of service. Consider using the Community Jury technique to conduct
these discussions.
Recommendation F1.2.1 Use Analysis Platform to understand the representation of identified demographic
groups in data sets that you plan to use for training and evaluating your system, respecting privacy controls for
working with sensitive data.
Recommendation F1.2.2 Document the representation of identified demographic groups in a Datasheet.
Recommendation F1.5.1 Use the Fairlearn Python toolkit’s assessment and mitigation capabilities, if appropriate
for the system.
Recommendation F1.5.2 Use Error Analysis to help understand factors that may account for performance levels
and differences, if appropriate for the system.
Recommendation F1.5.3 Use one or more techniques available as part of the Interpret ML toolkit to help
understand factors that may account for performance levels and differences, if appropriate for the system.
Recommendation F1.6.1 Use the Fairlearn Python toolkit’s assessment and mitigation capabilities, if appropriate
for the system.
Recommendation F1.6.2 Be prepared to collect additional training data for identified demographic groups.
Recommendation F1.7.1 Use Error Analysis to help understand factors that may account for performance levels
and differences, if appropriate for the system.
Recommendation F1.7.2 Use one or more techniques available as part of the Interpret ML toolkit to help
understand factors that may account for performance levels and differences, if appropriate for the system.
Microsoft Responsible AI Standard v2
16
Goal F2: Allocation of resources and opportunities
Microsoft AI systems that allocate resources or opportunities in essential domains are designed to do so in a manner
that minimizes disparities in outcomes for identified demographic groups, including marginalized groups.
Applies to: AI systems that generate outputs that directly affect the allocation of resources or opportunities relating
to finance, education, employment, healthcare, housing, insurance, or social welfare.
Requirements
F2.1 Identify and prioritize demographic groups, including marginalized groups, that may be at risk of being
differentially impacted by the system based on intended uses and geographic areas where the system will be
deployed. Include:
1) groups defined by a single factor, and
2) groups defined by a combination of factors.
Document the prioritized identified demographic groups using the Impact Assessment template.
Tags: Impact Assessment.
F2.2 Evaluate all data sets to assess inclusiveness of identified demographic groups and collect data to close any
gaps. Document this process and its results.
F2.3 Define and document the evaluation that you will perform to support this Goal. Include:
1) any system components to be evaluated, in addition to the whole system,
2) the metrics to be used to evaluate the system components and the whole system, and
3) the data set to be used for this evaluation.
Tags: Ongoing Evaluation Checkpoint.
F2.4 Define and document Responsible Release Criteria to achieve this Goal, as follows:
For each metric, document the target maximum difference (absolute or relative) between the rates at which
resources and opportunities are allocated to groups.
Tags: Ongoing Evaluation Checkpoint.
F2.5 Evaluate the system according to the defined Responsible Release Criteria.
Tags: Ongoing Evaluation Checkpoint.
F2.6 Reassess the system design, including the choice of training data, features, objective function, and training
algorithm, to pursue the goal of minimizing differences between the rates at which resources and opportunities
are allocated to identified demographic groups, paying particular attention to those that exceed the target
maximum difference, while recognizing that doing so may appear to affect system performance and it is seldom
clear how to make such trade-offs.
Consult with your attorney to determine your approach to this, including how you will identify and document
trade-offs.
Tags: Ongoing Evaluation Checkpoint. 
Microsoft Responsible AI Standard v2
17
F2.7 Identify and document any justifiable factors that account for any remaining differences between the rates at
which resources and opportunities are allocated to identified demographic groups.
Tags: Ongoing Evaluation Checkpoint.
F2.8 Document the pre-release results for the evaluation described by requirements F2.4, F2.5, and F2.6.
Determine and document how often ongoing evaluation should be conducted to continue supporting this goal.
Tags: Ongoing Evaluation Checkpoint.
F2.9 Publish information for customers about:
1) any remaining differences between the rates at which resources and opportunities are allocated to
identified demographic groups, and
2) any justifiable factors that account for these differences. When the system is a platform service made
available to external customers or partners, include this information in the required Transparency Note.
Tags: Transparency Note.
Microsoft Responsible AI Standard v2
18
Tools and practices
Recommendation F2.1.1 For North America, use Best Practices for Age, Gender Identity, and Ancestry to help
identify demographic groups and methods for collecting demographic information.
Recommendation F2.1.2 Work with user researchers to understand variations in demographic groups across
intended uses and geographic areas.
Recommendation F2.1.3 Work with domain-specific subject matter experts to understand the facts that impact
performance of your system and how they vary across identified demographic groups in this domain.
Recommendation F2.1.4 Work with members of identified demographic groups to understand risks of and
impacts associated with differences between the rates at which resources and opportunities are allocated.
Recommendation F2.2.1 Use Analysis Platform to understand the representation of identified demographic
groups, respecting privacy requirements for using sensitive data.
Recommendation F2.2.2 Document the representation of identified demographic groups in a Datasheet.
Recommendation F2.5.1 Use the Fairlearn Python toolkit’s assessment and mitigation capabilities, if appropriate
for the system.
Recommendation F2.5.2 Use Error Analysis to help understand factors that may account for differences between
the rates at which resources and opportunities are allocated to the identified demographic groups, if appropriate
for the system.
Recommendation F2.5.3 Use one or more techniques available as part of the Interpret ML toolkit to help
understand factors that may account for differences between the rates at which resources and opportunities are
allocated to the identified demographic groups, if appropriate for the system.
Recommendation F2.6.1 Use the Fairlearn Python toolkit’s assessment and mitigation capabilities, if appropriate
for the system.
Recommendation F2.7.1 Use Error Analysis to help understand factors that may account for differences between
the rates at which resources and opportunities are allocated to the identified demographic groups, if appropriate
for the system.
Recommendation F2.7.2 Use Interpret ML to help understand factors that may account for differences between
the rates at which resources and opportunities are allocated to the identified demographic groups, if appropriate
for the system.
Microsoft Responsible AI Standard v2
19
Goal F3: Minimization of stereotyping, demeaning, and erasing outputs
Microsoft AI systems that describe, depict, or otherwise represent people, cultures, or society are designed to
minimize the potential for stereotyping, demeaning, or erasing identified demographic groups, including
marginalized groups.
Applies to: AI systems when system outputs include descriptions, depictions, or other representations of people,
cultures, or society.
Requirements
F3.1 Identify and prioritize demographic groups, including marginalized groups, that may be at risk of being
subject to stereotyping, demeaning, or erasing outputs of the system. Include:
1) groups defined by a single factor, and
2) groups defined by a combination of factors.
Document the prioritized identified demographic groups using the Impact Assessment template.
Tags: Impact Assessment.
F3.2 Define and document any system components to be evaluated, in addition to the whole system.
F3.3 Define and document a plan to evaluate the system components and the whole system for risks of
stereotyping, demeaning, and erasing the prioritized identified demographic groups.
Tags: Ongoing Evaluation Checkpoint.
F3.4 Evaluate the system according to the plan defined in requirement F3.3.
Tags: Ongoing Evaluation Checkpoint.
F3.5 Reassess the system design, including the choice of training data, features, objective function, and training
algorithm, to pursue the goal of minimizing the potential for stereotyping, demeaning, and erasing the identified
demographic groups.
Tags: Ongoing Evaluation Checkpoint.
F3.6 Document the pre-release results from requirements F3.4 and F3.5. Determine and document how often
ongoing evaluation should be conducted to continue supporting this goal.
Tags: Ongoing Evaluation Checkpoint.
F3.7 Publish information for customers about these risks involving identified demographic groups. When the
system is a platform service made available to external customers or partners, include this information in the
required Transparency Note.
Tags: Transparency Note.
Microsoft Responsible AI Standard v2
20
Tools and practices
Recommendation F3.1.1 Work with user researchers, subject matter experts, and members of identified
demographic groups to understand these risks and their impacts.
Recommendation F3.4.1 Use CheckList to help evaluate these risks involving identified demographic groups, if
appropriate for the system.
Recommendation F3.4.2 Use red teaming exercises to evaluate these risks involving identified demographic
groups.
Recommendation F3.5.1 Mitigate any risks of these types of harms that you can. In addition, establish feedback
mechanisms and a plan for addressing problems, in alignment with Reliability and Safety Goal RS3. Note that this
approach is recommended in acknowledgment of the fact that the state-of-the-art in mitigating these risks is less
advanced than the state-of-the-art in mitigating differences in quality of service or allocative harms.
Microsoft Responsible AI Standard v2
21
Reliability & Safety Goals
Goal RS1: Reliability and safety guidance
Microsoft evaluates the operational factors and ranges within which AI systems are expected to perform reliably and
safely, remediates issues, and provides related information to customers.
Applies to: All AI systems.
Requirements
RS1.1 Document how:
1) reliable and safe behavior is defined for this system and,
2) what acceptable error rates are for overall system performance in the context of intended uses.
Tags: Ongoing Evaluation Checkpoint.
RS1.2 Evaluate training and test data sets to ensure that they include representation of the intended uses,
operational factors, and an appropriate range of settings for each factor. Document the evaluation.
Tags: Ongoing Evaluation Checkpoint.
RS1.3 Determine and document the operational factors, including quality of system input, use, and operational
context that are critical to manage for reliable and safe use of the system in its deployed context.
Tags: Ongoing Evaluation Checkpoint.
RS1.4 Define and document acceptable ranges for each operational factor important to support reliable and safe
system use. Define and document an acceptable error rate for the system when operating within these ranges.
Tags: Ongoing Evaluation Checkpoint.
RS1.5 Define intended uses, if any, where additional operational factors, more narrow or different acceptable
ranges, or lower acceptable error rates (including false positive and false negative error rates), are advised to
ensure reliability and safety. Document your conclusions.
Tags: Ongoing Evaluation Checkpoint.
RS1.6 Define and document an evaluation plan based on requirements RS1.1, RS1.3, RS1.4, and RS1.5, to include
the environment in which the system will be evaluated.
Tags: Ongoing Evaluation Checkpoint.
RS1.7 Evaluate the system according to the evaluation plan defined in requirement RS1.6 to ensure reliable and
safe system behavior. Document the pre-release results of the evaluation. Determine and document how often
ongoing evaluation should be conducted to continue supporting this goal.
Tags: Ongoing Evaluation Checkpoint.
RS1.8 In the event of failure cases within operational factors and defined ranges, work to resolve the issues. If the
Responsible Release Criteria established in requirements RS1.1, RS1.3, RS1.4, and RS1.5 cannot be met, a
reassessment of intended uses and updated documentation is required.
Tags: Ongoing Evaluation Checkpoint.
RS1.9 Provide documentation to customers and potential customers of the system that includes the outputs of
requirements RS1.2, RS1.7 and RS1.8, and any unsupported uses defined in the Impact Assessment and in RS1.8.
When the system is a platform service made available to external customers or partners, include this information in
the required Transparency Note.
Tags: Impact Assessment, Transparency Note.
Microsoft Responsible AI Standard v2
22
Tools and practices
Recommendation RS1.1.1 Interview safety experts and review relevant literature for domains where the system
may impact the safety of people.
Recommendation RS1.4.1 Interview customers to understand operational factors and their variations.
Microsoft Responsible AI Standard v2
23
Goal RS2: Failures and remediations
Microsoft AI systems are designed to minimize the time to remediation of predictable or known failures.
Applies to: All AI systems.
Requirements
RS2.1 Define predictable failures, including false positive and false negative results for the system as a whole
and how they would impact stakeholders for each intended use. Use the Impact Assessment template to
document any adverse impacts of these failures on stakeholders.
Tags: Impact Assessment.
RS2.2 For each case of a predictable failure likely to have an adverse impact on a stakeholder, document the
failure management approach:
1) When possible, design and build the system to avoid this failure. Describe the design solution.
Estimate the time range for resolving predictable failures for each designed solution or indicate that
the failure will be prevented by design.
2) When a failure cannot be prevented by design, build a fallback option that may be used when this
failure occurs. Describe the fallback option and document the estimated time required to invoke and
use the fallback option.
3) Provide training and documentation for stakeholders accountable for system oversight that supports
their resolution of the failure. Describe the documentation and training.
RS2.3 Document your plan for managing previously unknown failures that come to light once the system is
in use:
1) Describe the system’s rollback plan and document the time that may elapse until the entire system,
across all endpoints can be rolled back.
2) Describe support for turning features off and document the time that may elapse until the feature can
be turned off across all endpoints.
3) Describe the process for updating and releasing updates to each model and document the time that
may elapse until the system has been updated across all endpoints.
4) Describe how customers, partners, and end users will be notified of changes to the system, updated
understandings of failures, and their best mitigations.
RS2.4 Provide training and documentation for system owners, developers, customer support and other
stakeholders responsible for managing the system to support their remediation and mitigation of predictable
failures identified in requirement RS2.1. Document the training and documentation provided.
Tools and practices
Recommendation RS2.1.1 Conduct Failure Mode and Effects Analysis.
Recommendation RS2.2.1 Follow the Guidelines for Human-AI Interaction when designing the system to help
manage failures.
Microsoft Responsible AI Standard v2
24
Goal RS3: Ongoing monitoring, feedback, and evaluation
Microsoft AI systems are subject to ongoing monitoring, feedback, and evaluation so that we can identify and review
new uses, identify and troubleshoot issues, manage and maintain the systems, and improve them over time.
Applies to: All AI systems.
Requirements
RS3.1 Establish and document a detailed inventory of the system health monitoring methods to be used, to
include:
1) data and insights generated from data repositories, system analytics, and associated alerts,
2) processes by which customers can submit information about failures and concerns, and
3) processes by which the general public can submit feedback.
RS3.2 Define and document a standard operating procedure and system health monitoring action plan for each
monitoring channel for the system, to include:
1) processes for reproducing system failures to support troubleshooting and prevention of future failures,
2) which events will be monitored,
3) how events will be prioritized for review,
4) the expected frequency of those reviews,
5) how events will be prioritized for response and timing to resolution,
6) how high priority issues related to supporting the Standard and its goals will be escalated to the Office of
Responsible AI, and
7) engaging customer service to ensure that they are aware of how to respond to issues for the system.
RS3.3 When new uses, critical operational factors, or changes in the supported range of an operational factor are
identified, determine whether any new use or operational factor can be supported with the existing system, will be
supported but require additional work, or will not be supported.
• When new uses or operational factors identified are to be supported, evaluate the updated system in
accordance with requirement RS1.6, add the new intended use to the Impact Assessment, and publish
updated communication in accordance with requirement RS1.9.
• When these new uses or operational factor range changes cannot or will not be accommodated to ensure
reliable and safe performance of the system update customer documentation described in RS1.9 to include
the new use as an unsupported use.
When the system is a platform service made available to external customers or partners, include this information in
the required Transparency Note.
Tags: Impact Assessment, Transparency Note.
RS3.4 When a system is to be used for a Sensitive Use that imposes qualification or quality control requirements
beyond the intended uses and/or operational factor ranges, conduct an evaluation specific to this use. If the
required Responsible Release Criteria cannot be met, the Office of Responsible AI will review the results and decide
how to proceed. Document any changes to the Responsible Release Criteria and document the results of
evaluation.
RS3.5 Conduct all evaluations tagged as Ongoing Evaluation Checkpoints in other Goals on an ongoing basis.
Microsoft Responsible AI Standard v2
25
RS3.6 If there are targets in Ongoing Evaluation Checkpoints that are no longer satisfied, consult with named
reviewers, and in the case of Sensitive Uses, with the Office of Responsible AI, to develop and implement a plan to
close any gaps. Document the process, its results, and conclusions.
RS3.7 If evidence comes to light that refutes the system is fit for purpose for an intended use at any point in the
system’s use:
1) remove the intended use from customer-facing materials and make current customers aware of the issue,
take action to close the identified gap, or discontinue the system,
2) revise documentation related to the intended use, and
3) publish the revised documentation to customers.
When the system is a platform service made available to external customers or partners, include this information in
the required Transparency Note.
Tags: Transparency Note.
RS3.8: Review and update documentation required by Goal T2 when any of the following events occur:
1) new uses are added,
2) functionality changes,
3) new information about reliable and safe performance becomes known as defined by requirement RS3.3, or
4) new information about system accuracy and performance becomes available.
When the system is a platform service made available to external customers or partners, include this information in
the required Transparency Note.
Tags: Transparency Note.
RS3.9: Escalate unresolved issues related to supporting the Standard and its requirements to the Office of
Responsible AI.
Microsoft Responsible AI Standard v2
26
Privacy & Security Goals
Goal PS1: Privacy Standard compliance
Microsoft AI systems are designed to protect privacy in accordance with the Microsoft Privacy Standard.
Applies when: Microsoft Privacy Standard applies.
Goal PS2: Security Policy compliance
Microsoft AI systems are designed to be secure in accordance with the Microsoft Security Policy.
Applies when: Microsoft Security Policy applies.
Microsoft Responsible AI Standard v2
27
Inclusiveness Goal
Goal I1: Accessibility Standards compliance
Microsoft AI systems are designed to be inclusive in accordance with the Microsoft Accessibility Standards.
Applies when: Microsoft Accessibility Standards apply.
Scan this code to access responsible AI resources from Microsoft:
© 2022 Microsoft Corporation. All rights reserved. This document is provided “as-is.” It has been edited for external release to
remove internal links, references, and examples. Information and views expressed in this document, including URL and other
Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only
and are fictitious. No real association is intended or inferred. This document does not provide you with any legal rights to any
intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.",Relevant Page: https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6
18,Motorola Solutions,United States of America,Telecommunication,,,https://www.motorolasolutions.com/content/dam/msi/images/artificialintelligence/responsible-ai-whitepaper-final.pdf,"PRINCIPLES AND PRACTICES FOR THE
RESPONSIBLE APPLICATION OF ARTIFICIAL
INTELLIGENCE AT MOTOROLA SOLUTIONS","PRINCIPLES AND PRACTICES FOR THE
RESPONSIBLE APPLICATION OF ARTIFICIAL
INTELLIGENCE AT MOTOROLA SOLUTIONS
WHITE PAPER
WHITE PAPER | AI AT MOTOROLA SOLUTIONS
Artificial Intelligence (AI) will improve the efficiency, effectiveness
and safety of the Motorola Solutions user community. However, as a
powerful, multi-faceted emerging technology, AI can have far-reaching
unintended consequences if applied inappropriately, or executed with
insufficient rigor and discipline.
This paper summarizes Motorola Solutions’ policies and practices for
responsibly applying AI in its public safety products and applications.
Our goal is to ensure unbiased, fair, understandable, secure and reliable
operation. We apply a human-centered design philosophy that is informed
by copious customer research to guide our application of AI based on the
foundational tenets of “human in the loop” for all consequential actions,
and an emphasis on the use of human-centered design in the application
of AI for focused, purpose-built solutions.
HOW ARTIFICIAL INTELLIGENCE BENEFITS OUR
PUBLIC SAFETY USERS
To a layperson, AI is generally presumed to be computer technology that
can broadly mimic a human in performing complex tasks. In practice,
today’s technology is not that advanced. Currently, AI can only emulate
aspects of human intelligence, focused on narrowly defined tasks such as
the ability to recognize faces, or perform a translation of audible speech
to text. In the context of responsibly applying AI to the public safety
workflow, these specific task applications are, in fact, more beneficial.
Motorola Solutions has been creating mission critical communications
hardware and software for nearly our entire 91-year history. Through
our acquisitions, strategic venture investments, and evolving research
and development profile, we are assembling a comprehensive operating
platform for public safety. This platform is the foundation upon which
we can bring AI-driven outcomes to our customers. As the world creates
exponentially increasing quantities of structured and unstructured data
that characterizes conditions and events, we are applying AI to interpret
this data, improving the efficiency, effectiveness and safety of first
responders and command center personnel around focused tasks.
Our objective is to support humans throughout the public safety incident
workflow by augmenting their decision-making with appropriate AI
assistance. Some ways that we are applying AI in this regard include:
• Using AI to transcribe, translate, interpret and summarize
speech and text during human interactions with a public safety
professional or emergency call center.
• Utilizing AI to power natural language interactions (voice bots),
enabling first responders to access critical information while
allowing them to remain aware and attentive (“eyes up and hands
free”) at all times. Voice applications could include running a
license plate check with a voice query/response; populating a
form verbally rather than manually; or a periodic autonomous wellbeing check by asking a first responder to verbally acknowledge
their condition. A key aspect of successfully executing voice
interaction - informed by extensive experience building missioncritical voice solutions - is building the vocabulary of our user
groups, e.g. 10-codes and the NATO alphabet, into the experience.
• Leveraging AI to “watch” the exponentially increasing video
sources available to public safety, identifying unusual occurrences
in real time and alerting a human analyst. These use cases could
include noting the appearance of smoke, the formation of a crowd,
a person crossing a security boundary, or identifying an individual
matching a description such as a missing person or AMBER Alert
subject of interest. AI also excels at searching historical video for
items or persons of interest.
• Applying AI in the form of biometric identification (such as facial
recognition) in public safety applications to identify individuals
without identification or who are incapable of identifying
themselves.
In all cases, these are tasks that humans perform manually today. We
are simply using AI to automate the labor-intensive aspects for each.
WHITE PAPER | AI AT MOTOROLA SOLUTIONS
THE NEED FOR RESPONSIBLE AI
Artificial Intelligence is new, complex, powerful technology. Applied
carelessly, it can generate surprising and unintended results. Applied
improperly or maliciously, the technological scaling of AI can amplify
or institutionalize undesirable outcomes - a risk of particular import for
public safety, given its societal impact.
Responsible AI simply means that when applying AI in products and
services, one must consider the ramifications (good, bad, or unexpected)
and constrain the outcomes to ensure proper results. Fundamentally, AI
powered solutions must be unbiased, secure and trustworthy. Motorola
Solutions approaches responsible AI based on our guiding policies and
principles, supplanted by the underpinning human-centered design,
development, validation, and performance monitoring processes that
ensure our AI solutions adhere to the policies.
POLICY AND PRINCIPLES FOR APPLYING AI
RESPONSIBLY
Over decades of designing mission critical systems, Motorola Solutions
has developed substantial design practices expertise founded in user
research, iterative user-inclusive design validation, and domain-specific
human factors engineering. We annually accumulate thousands of hours
of in-field and in-situ observational user research (ethnography). We
apply this intimate understanding of user needs to a human-centered
design process that results in purpose-built, focused solution designs
specifically optimized for the tasks at hand. We then develop prototypes
and verification frameworks that we test and validate iteratively with
users to perfect the design and, ultimately, its implementation. Finally, we
apply specific human factors disciplines to our products to ensure that the
technology is well-tuned to the needs of first responders operating across
their daily workflows.
We are applying the same focused research and design-led approach in the
application of ‘purpose built’ AI solutions to ensure that we are aligning the
capabilities with specific human needs (no more, no less).
WHITE PAPER | AI AT MOTOROLA SOLUTIONS
Our policy for applying AI starts with three foundational tenets that are
fundamentally focused on our users and fields of use:
• Human in the Loop
• Focused application resulting in purpose-built solutions
• Applying mature AI
HUMAN IN THE LOOP
Fundamentally, our approach is to augment human decision making while
never displacing or disintermediating human judgement. In this sense,
our systems are advisory and will never take AI-generated consequential
actions on their own. In other words, there is always a “human in the loop”
to make the final determinations on substantial decisions. Studies indicate
that the best results are achieved with a combination of AI and human
experts1.
Our user-centered research methodologies have led us to a specific set of
human factors disciplines that we refer to as High Velocity Human Factors
(HVHF). This recognizes that the more stress and duress an individual is
experiencing, the less cognitive capacity they have to apply to anything
other than the specific event.
For example, if a police officer has deployed his or her weapon, there is
little else they can (or should be) focused on other than the threat at hand.
The paradox of HVHF is that the more a user could benefit from technology
- including AI recommendations, for example - the less mental capacity they
have available to leverage it.
We are investigating the selective application of AI to detect these
situations and understand the context so that we can automatically
adapt the operation of the technology on behalf of the user. For example,
detecting the condition via biometric data, voice inflection, and/or
background audio event detection that leads to activation of streaming body
worn video, prioritization of communications, alerting the command center
and nearby officers, or similar. 
WHITE PAPER
| AI AT MOTOROLA SOLUTIONS
FOCUSED APPLICATION AND MATURE AI
Our two remaining foundational tenets - focused
application, and mature AI - are connected.
Many respected and capable companies across a
variety of industries have applied substantial thought
and effort into the considerations of how to best
apply AI in a general fashion, across a diverse set of
applications, users, and environments2,3,4. Applying
AI in a general manner is extremely difficult, due to
the wide variety of possible circumstances and user
personas.
Given our emphasis on public safety, Motorola
Solutions can focus this particular AI challenge. We
are creating “purpose-built” AI solutions focused on
specific tasks within the public safety workflow. Our
immersive human-centered design practices result in
a thorough understanding of the end-to-end process,
and awareness of bottlenecks within the system
where AI can accelerate or improve the process.
We are not creating general AI utilities, tools
and frameworks for others to leverage in building
their solution. Rather, we are applying proven AI
technology in a focused and collaboratively developed
way. We are placing it in the hands of a known
population (e.g., first responders, or dispatchers). We
are augmenting their decision-making, not displacing
them. Finally, we are applying the technology to a
narrowly defined and well-understood use case.
Critically, the narrow scope of the application allows
us to choose the simplest, most specific, and most
mature underlying AI technologies. We can manage
and curate training data for the specific applications
and environments implied by the solution, and to
constrain the scenarios and use cases across which
the solutions must be validated.
Motorola cannot ensure or enforce our customer’s
compliance with legal and ethical standards in the
application of AI, but our products provide access
controls based upon user authentication credentials
and access permissions to allow our customers to
enforce user compliance to operational policies.
Our products will maintain an audit trail of user
operations and where possible, constrain the use
of AI powered capabilities within the context of a
workflow. For example, submitting an image of
a face (probe) for search against a gallery may
only be accomplished in the context of an active
investigation by authorized individuals. When a
‘match’ is surfaced, this must be verified and signed
off by one (and sometimes two, depending upon local
policy) human experts. Audit trails will not only keep
track of the specific operations and user identity but
will also retain any data used in the operation (e.g.,
introduction of a probe-image for search).
FUNDAMENTALS OF
RESPONSIBLE AI
Underlying our foundational
tenets designed for the unique
needs of public safety, there are
generally acknowledged universal
considerations when applying AI that
Motorola Solutions addresses:
• Bias and fairness
• Understandability and transparency
• Privacy
• Reliability and security
BIAS AND FAIRNESS
Bias, in general, is a prejudice for or against
one thing, person, or group compared with
another, usually in a way considered to be unfair.
For humans, bias can result from a number
of factors, including a person’s environment,
training, condition, or experiences. In AI, bias
occurs when the output of the AI process results
in inconsistent treatment across a group. For
example, being able to identify faces more
accurately for one demographic (characterized
by race, gender, age, and physiology) than
another, or making decisions about how a person
should be treated (such as a hiring decision)
inconsistently across demographics.
Much like humans learn iteratively through
experience, AI algorithms learn by iteratively
‘training’ with representative data, developing
the ability to identify discriminating
characteristics across this data set. For
example, in a facial recognition application, the
AI system is exposed to a series of pictures of
different and identical faces that are tagged
so the AI can know whether given images
are of the same individual or not. In this way,
the AI effectively learns what characteristics
distinguish different people. (This is very
similar to how humans develop their ability to
recognize and differentiate patterns in the senses
of sight, sound, smell, taste, and touch.) Voice
interaction systems are essentially trained in
the same manner using tagged verbal data. The
completeness and accuracy of the AI system’s
‘learning’ is a function of the quality, diversity,
correctness, and volume of the training dataset.
In general, data needs to be completely
representative of the context that the AI system
will operate within (population, vocabulary,
etc.) and there must be sufficient data available
for the AI to develop the ability to fully discern
uniquely identifying characteristics.
Bias can be present within training data in two
ways: either the data is unrepresentative of
the population that the AI will operate within,
or it reflects past or existing prejudices. For
instance, the first case might occur if an AI
system trains on a set of voice samples that
has more females than males. The resulting
natural language interaction system would
be worse at recognizing male speech. An
example of the second case might be if a
recruiting tool dismisses a large proportion
of female candidates because it was trained
on historical hiring decisions, which were
dominated by males over females.
AI is fundamentally amoral as it is not
influenced by human discriminatory tendencies,
emotions, distractions or fatigue. Thus, AI
has the ability to be much more deterministic,
and less biased than human counterparts.
Furthermore, as the AI system is used and
exposed to more data in its population set, its
accuracy will continually improve.
How Motorola Solutions Supports
Fairness in our AI
Motorola Solutions thoroughly evaluates the
data employed in training our AI algorithms
to ensure that sufficient quantity, quality and
diversity exists across the dataset to properly
train the algorithm for its intended purpose and
operating environment.
We thoroughly validate the operation of the
trained algorithms with a representative and
diverse set of test cases that are applied
across a range of operational conditions.
We also test and retest our products in actual
customer environments.
In fielded operation, our systems generate
telemetry that we can continuously monitor
to identify performance issues, as well as
any inconsistent or undesirable behaviors.
For example, as we discover optimizations
and refinements that improve accuracy we
can retrain the model with an enhanced data
set (e.g., a new set of slang terminology or
speech accent that was not initially known/
anticipated) and deploy that universally across
our entire user base.
WHITE PAPER | AI AT MOTOROLA SOLUTIONS
UNDERSTANDABILITY AND
TRANSPARENCY
Understandability simply means that the
behavior and outputs of an AI system must
be readily explainable by those who provide
it. This ‘why’ component is an essential
characteristic of a system in order for users
to interpret and trust the outputs produced,
and for society to trust the tools that first
responders are applying on their behalf.
In traditional systems, explaining behavior
is generally a straightforward exercise of
reading the software programs to understand
what the programmer instructed the system to
do. Trained AI systems do not fundamentally
have programmed or codified behaviors. Their
operations are a function of the training
dataset used. Variations in output may come
from the system’s training or may be the
manifestation of traditional software bugs in
the underlying implementation of the model.
How Motorola Solutions Supports
Understandability in our AI
• Motorola Solutions maximizes our
ability to explain the operation of our
systems by adopting mature, testable
AI components that are as simple
as possible for the task at hand.
For example, by knowing the verbal
commands needed for an interaction
that the system will support, we can
greatly constrain the verbal phrases
and words (intents) that need to be
understood.
• By leveraging mature, well characterized
and understood implementations
and model frameworks with proven
reliability, we increase the likelihood
that the simplest explanation is most
likely to be correct. Simple systems
are much easier to test, understand
and explain than complex ones, and
also boost the overall reliability of the
system. Often, these components will
come from services in widely deployed
cloud service platforms operating at
scale.
• We ensure that our systems generate
operational performance data that we
can monitor and review on a regular
basis to assess efficacy of operation.
PRIVACY
In the context of AI applications, the issue of
privacy primarily involves securing and managing
the data associated with the system, including
training data (input to the system) and any
outputs that the system produces in operation.
Training data may contain Personally Identifiable
Information (PII), which has to be managed
in accordance with appropriate information
security policies. As established in the bias
and fairness discussion, it is essential to work
with representative data when training AI.
Consequently, Motorola Solutions will almost
certainly be leveraging samples of live data
(containing PII) in some instances.
How Motorola Solutions Supports
Privacy in our AI
• Where possible, Motorola Solutions
will work with anonymized data (no
assignable PII content) at the source,
synthesized data (by machine methods or
through controlled customer interactions
such as training exercises), or accumulate
our training data from publicly available
sources.
• We utilize tools and frameworks that
facilitate privacy-sensitive training by
encoding general patterns rather than
facts about specific training examples
wherever possible. Techniques such
as differential privacy and federated
learning offer strong mathematical
guarantees that models do not learn or
remember the details about any specific
user. Furthermore, no training data is
distributed with our products.
• In the USA, Motorola Solutions employs
Criminal Justice Information System
(CJIS) compliant data facilities, approved
personnel, and stringent practices to
manage the data that we house for
training and testing algorithm purposes.
Outside of the USA, where it is necessary
to apply localized data for this purpose,
we will adhere to all country specific
regulations and practices that might
entail establishment of local presence
and a secure data repository that respects
considerations such as data sovereignty.
• When securing usage or deploymentgenerated operational outputs of AI
enabled solutions, we rely upon the same
WHITE PAPER | AI AT MOTOROLA SOLUTIONS
stringent cybersecurity practices that we
apply to all of our products5
. We build
in all of the necessary security controls,
auditing, and practices necessary to
enable our users to secure and manage
sensitive data (e.g., Law Enforcement
Records) and this same fabric applies to
outputs generated by AI. Where possible
we leverage industry best practices and
scale (e.g., government cloud based
platforms and environments).
RELIABILITY AND SECURITY
As with any item that public safety users
leverage for day-to-day operation, AI based
solutions must be as reliable as possible.
Motorola Solutions has a history of delivering
mission critical solutions and a culture of
ensuring quality, performance, and integrity in
our products. This is accomplished by following
disciplined development, validation, deployment
and life-cycle management processes and
employing comprehensive product testing
practices.
When introducing changes to solutions
in the public safety space, we employ an
incremental discipline. Starting with a thorough
understanding of users and their environment,
we progress through a collaborative creation
process where we iteratively develop, test and
refine capabilities incrementally with continual
customer feedback.
How Motorola Solutions Supports
Reliability and Security in our AI
• We ensure that new capabilities don’t
eliminate, impair or alter existing
functions so that in the moment of need
nothing previously relied upon or learned
as ‘muscle memory’ is compromised.
• We introduce new capabilities in
controlled and tightly monitored
circumstances (e.g., training facilities
and environments for models, select
test users, etc.) and employ focus group
feedback. In selecting these target
environments, we consider the nature of
the AI capability being applied and ensure
that we choose diverse environments to
maximize the testing surface variation.
• We employ standard incremental rollout
methods across our user base such as
Beta testing.
• We leverage continuous integration / continuous deployment development
practices that allows us to deploy changes, fixes, and new capabilities quickly and
incrementally. This is especially important in the context of retraining AI functionality.
• We build in system monitoring and telemetry that we can threshold or monitor
regularly to ensure that system is operating as intended.
• Our products build in compliance mechanisms to apply role-based access controls
and create audits, allowing our customers to enforce compliance and accountability
across their users.
SUMMARY
Motorola Solutions is responsibly and incrementally employing AI to assist and
augment our users to help them be more efficient, effective and safe. We are
doing this by leveraging proven mission critical research and design principles
guided by the fundamental tenets of human in the loop for consequential
decisions and focused solutions that leverage mature AI.
We are developing AI aimed at customer outcomes that are familiar and
consistent with outcomes we’ve previously enabled in a more manual fashion
with other technologies. AI simply enables these outcomes in a way that is more
efficient, more effective, and safer. We can easily measure the effectiveness and
accuracy of AI-driven solutions relative to more traditional methods of achieving
the same outcomes. In this way, our AI solutions are anchored in and measured
against widely accepted, culturally and ethically appropriate methods that
support fairness, understandability, privacy, and security.
REFERENCES
1. Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms; Philips, Yates, Hu, Hahn, Noyes, Jackson, Cavazos, Jeckeln, Ranjan, Sankaranarayanan, Chen,
Castillo, Chellappa, White, O’Toole; Proceedings of the National Academy of Sciences; June 2018
2. The Future Computed – Artificial Intelligence and its Role in Society; Microsoft Corporation; 2018
3. Responsible AI Practices; Google
4. Everyday Ethics for Artificial Intelligence; IBM Corporation; September 2018
5. Motorola Solutions Products, Solutions, and Services Foundational Cybersecurity Policies; Motorola Solutions; Version 1.0; October 2017
To learn more about empowering public safety with AI, visit:
motorolasolutions.com/viqi
THE UNDERSTANDABILITY POTENTIAL OF
MACHINE LEARNING
Where possible and applicable, we apply
solutions that provide more interpretability for
our customers. There are exciting advances
being made rapidly in the arena of AI
interpretability via machine learning.
Fundamentally, traditional software uses human
readable rules to map inputs to outputs. It is
possible to use machine learning to learn these
rules directly or to decipher the implicit logic
used by the trained AI model to map inputs to
outputs. We ensure that our machine learning
models are deterministic and where possible
interpretable as rules operating on the input.
When machine learning is applied, we will
deploy trained and tested implementations
rather than field-based machine learning
wherever possible. When we do apply machine
learning it will be for low-level cases such as
allowing the solution to adapt to its environment
(e.g., using machine learning on a video camera
to estimate scene geometry and constant
conditions). This allows us to curate the training
data set to the specific objective at hand and
validate the operation of the system through
in-house testing prior to deployment.
Motorola Solutions, Inc. 500 West Monroe Street, Chicago, Il 60661 U.S.A. motorolasolutions.com
MOTOROLA, MOTO, MOTOROLA SOLUTIONS and the Stylized M Logo are trademarks or registered trademarks of Motorola Trademark Holdings, LLC and
are used under license. All other trademarks are the property of their respective owners. © 2019Motorola Solutions, Inc. All rights reserved. 10-2019",Relevant Page: https://www.motorolasolutions.com/content/dam/msi/docs/about-us/cr/the-responsible-use-of-artificial-intelligence-cr-report.pdf https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6
19,NEC,Japan,Hardware,04-01-2019,04-01-2019,https://www.nec.com/en/press/201904/images/0201-01-01.pdf,NEC Group AI and Human Rights Principles,"NEC Group AI and Human Rights Principles
Enacted: April, 2019
The NEC Group (hereinafter referred to as “NEC”), as a “Social Value Creator,” is pursuing
“for all people, an abundant society” by aiming to make lasting contributions toward the
resolution of social issues leveraging new technologies that promote the safety, security,
efficiency, and equality of society. However, we also understand that while social
implementation of Artificial Intelligence (AI) and utilization of biometrics and other data
(hereinafter referred to as “AI utilization”) enriches our lives, it may also lead to human
rights issues such as the invasion of privacy and/or discrimination depending on how it is
utilized.
NEC is enacting these principles in order to prevent and address human rights issues arising
from AI utilization. In addition to facilitating compliance with relevant laws and regulations
around the globe, these principles will guide our employees to recognize respect for human
rights as the highest priority in each and every stage of our business operations in relation
to AI utilization and enable them to take action accordingly.
(1) Fairness
NEC will ensure that individuals are not unfairly discriminated in AI utilization keeping
always aware that results of decision-making by AI are possible to be biased.
(2) Privacy
NEC will respect and protect individual's privacy in AI utilization.
(3) Transparency
NEC will strive to create a framework that enables appropriate explanations regarding the
results of decision-making in our AI utilization.
(4) Responsibility to Explain
NEC will provide all stakeholders with a comprehensive explanation of the effects, value,
and impacts of AI utilization to gain their understandings.
(5) Proper Utilization
NEC will ensure proper AI utilization in a way that respects human rights.
Furthermore, NEC will ensure that our AI products and services will be utilized by our
customers and partners in accordance with respect for human rights.
(6) AI and Talent Development
NEC will develop useful and cutting-edge technologies and talent in an effort to promote AI
utilization.
(7) Dialogue with Multiple Stakeholders
NEC will incorporate the viewpoints and opinions of third parties in addition to internal
stakeholders to prevent NEC’s AI products and services from violating human rights. To this
end, NEC will build partnerships and collaborate closely with external experts and various
other relevant stakeholders.
NEC will not ignore new social issues generated by future AI utilization, but rather face them
head on by leveraging our technologies to realize advance societies worldwide toward
deepened mutual understanding and the fulfillment of human potential.",
20,Nokia,Finland,Telecommunication,,,https://www.bell-labs.com/research-innovation/responsible-ai/#gref,6 Pillars of Responsible AI,"Six pillars of Responsible AI
Fairness
AI systems must be designed in ways that maximize fairness, non-discrimination and accessibility. All AI designs should promote inclusivity by correcting both unwanted data biases and unwanted algorithmic biases.

Reliability, Safety and Security
AI systems should cause no direct harm and always aim to minimize indirect harmful behavior. They should always perform as intended and remain resilient against threats and outside tampering.

Privacy
AI systems must respect privacy by providing individuals with agency over their data and the decisions made with it. AI systems must also respect the integrity of the data they use.

Transparency
AI systems must be explainable and understandable. They should allow for human agency and oversight by producing outputs that are comprehensible to the average person. AI decisions should be auditable and traceable, which will ultimately instill the trust AI needs for broad acceptance.

Sustainability
AI systems should attempt to be both societally sustainable, by empowering society and democracy, and environmentally sustainable, by reducing the amount of power required to train and run these systems.

Accountability
AI systems should be developed and deployed through consultation and collaboration in order to create true accountability. The long-term effects of an AI application must be understandable by all stakeholders, and those stakeholders must be empowered to act if any proposed change produces adverse effects.",Relevant Page: https://www.bell-labs.com/institute/blog/introducing-nokias-6-pillars-of-responsible-ai/#gref
21,NXP Semiconductors,Netherlands,Semiconductor,12-01-2020,12-01-2020,https://www.nxp.com/company/blog/we-are-nxp-a-framework-for-ethical-ai:BL-FRAMEWORK-FOR-ETHICAL-AI,A Framework for Ethical AI,"It’s hard to deny that artificial intelligence has come so far so fast. It’s working its way into our lives in ways that seem so natural that we find ourselves taking it for granted almost immediately. It’s helping us get around town, keeping us fit and always inventing new ways to help around the house. The ever-increasing availability of affordable sensors and computing elements will only accelerate this trend. Thinking of what’s next, which was once the domain of science fiction writers, is now our day-to-day reality. In fact, Deloitte has estimated that more than 750 million AI chips will be sold in 2020 and by 2024 they expect this will exceed 1.5 billion.
As with any useful advancement, AI will be a little bit of a mixed bag. Most AI applications will be designed for the greater good, but there will always be outlying cases. The increase in autonomous applications that carry with them the potential to put humans in danger drives home the need for a universal code of conduct for AI development. A few years ago this would have sounded preposterous, but things are changing fast.
The industry, together with governments of several of the world’s leading nations, are already developing policies that would govern AI, even going so far as discussing a “code of conduct” for AI that focuses on safety and privacy. But how does one make AI ethical? First, you have to define what is ethical and the definition isn’t as cut and dry as we may hope. Without even considering the vast cultural and societal differences that could impact any such code, in practical terms, AI devices require complicated frameworks in order to carry out their decision-making processes.
The integrity of an AI system is just as important as the ethical programming because once a set of underlying principles is decided on, we need to be sure they’re not compromised. Machine learning can be utilized to monitor data streams to detect anomalies, but it can also be used by hackers to further enhance the effectiveness of their cyberattacks. AI systems also have to process input data without compromising privacy. Encrypting all communications will maintain confidentiality of data, and Edge AI systems are starting to use some of the most advanced cryptography techniques available.
Bright minds. Bright futures. NXP team members create breakthrough technologies that advance our world. The future starts here.
Perhaps the biggest challenge is that the AI ecosystem is made up of contributions from various creators. Accountability and levels of trust between these contributors are not uniformly shared, and any breach could have far reaching implications if systematic vulnerabilities are exploited. Therefore, it’s the responsibility of the entire industry to work towards interoperable and assessable security.
Agreement on a universal code of ethics will be difficult and some basic provisions need to be resolved around safety and security. In the meantime, certification of silicon, connectivity and transactions should be a focus for stakeholders as we collaborate to build trustworthy AI systems of the future.
At NXP, we believe that upholding ethical AI principles, including non-maleficence, human autonomy, explicability, continued attention and vigilance, privacy and security by design is important. It is our responsibility to encourage our customers, partners and stakeholders to support us in this endeavor.
You can read more it in our whitepaper, The Morals of Algorithms – A contribution to the ethics of AI systems.
",
22,"OKI Electric Industry Co., Ltd.",Japan,Hardware,09-30-2019,09-30-2019,https://www.oki.com/en/press/2019/09/z19033e.pdf,OKI Group AI Principles,"OKI Group AI Principles 
In recent years, the development of AI technologies has been remarkable, and they have solved problems that were once thought insoluble and have huge potential as one of the means to make people’s lives richer and happier. The OKI Group aims to realize harmonious coexistence of human beings and AI by providing socially accepted AI products, etc. (i.e., products and services utilizing AI as well as provided AI technologies) in order to contribute to the improvement of the quality of life for people around the world. On the other hand, because of its nature, currently there are challenges and limitations for AI, and this may cause legal and ethical issues or safety related issues due to inappropriate development and/or utilization, etc. The OKI Group has established the OKI Group Charter of Corporate Conduct and the OKI Group Code of Conduct, and has been striving to fulfill our social responsibility based on our corporate philosophy. Based on the spirit of the OKI Group Charter of Corporate Conduct and the OKI Group Code of Conduct, in order to prevent or cope with the above issues when conducting research and development of AI technologies, the sale and provision of AI products, etc., and any other corporate activities related to AI (collectively, the “AI Business”), we established the OKI Group AI Principles. The OKI Group will, in addition to complying with laws and regulations, continue to develop its AI Business in compliance with these OKI Group AI Principles. (1) Respect for Human Rights AI should be developed and utilized for the realization of a human-centric society. The OKI Group respects fundamental human rights in the promotion of the AI Business, and as part of that, we will strive to prevent any unfair discrimination. Further, we will consider privacy, and will comply with applicable rules including laws and regulations concerning the handling of personal information. (2) Explanation and Transparency In the AI Business, the OKI Group will endeavor to gain the understanding of our customers and other stakeholders by explaining the envisaged purposes and methods of use of AI products, etc., as well as the effects and impacts of the utilization of AI products, etc., and their limitations, depending on the nature and utilization situation of the subject AI products, etc. In particular, we will consider the transparency of how AI reaches conclusions with respect to individual AI products, etc. and endeavor to provide information that contributes to an understanding of how AI reaches conclusions. Also, we will continue to examine better ways of providing information from various aspects based on the accumulation of individual examples and the progress of future discussions. (3) Dialogue and Collaboration In order to deepen our customers’ and other stakeholders’ understanding of AI products, etc., the OKI Group will have a necessary dialogue with them, and strive to establish relationships with them whereby they will become more convinced in using AI products, etc. Further, we recognize that there are challenges concerning collaboration between human beings and AI and AI-to-AI, and will continue to consider these challenges so as to realize a better human-centric society. (4) Safety and Handling of Data The OKI Group will strive to ensure the safety of provided AI products, etc. for our customers and other stakeholders. Further, with regard to data, while protecting personal information and respecting privacy, we will ensure the proper acquisition, use, management and security of data and will strive not to cause any undue damage to our customers and other stakeholders. (5) Development of Human Resources In order to contribute to the improvement of the quality of life for people around the world, the OKI Group thinks that, in the performance of the AI Business, it is essential to have human resources who have a proper understanding of AI technologies and the nature, issues and limitations of AI, and who may appropriately provide AI products, etc. to society. Therefore, we will actively develop diverse human resources that are needed in the era of AI utilization. The OKI Group will, in promoting the AI Business in accordance with the OKI Group AI Principles, develop the capability to share and resolve issues through the AI Business in collaboration between related group companies and departments. In addition to this, if we deem it necessary taking into account domestic and international trends and social demands, etc., we will amend these OKI Group AI Principles",
23,Salesforce,United States of America,Software,,,https://www.salesforceairesearch.com/trusted-ai,Trusted AI Principles,"Trusted AI
We deliver tools to our employees, customers and partners for developing and using AI responsibly, accurately and ethically.

Our Commitment
We believe the benefits of AI should be accessible to everyone. But it is not enough to deliver only the technological capabilities of AI – we also have an important responsibility to ensure that AI is safe and inclusive for all. We take that responsibility seriously and are committed to providing our employees, customers, and partners with the tools they need to develop and use AI safely, accurately, and ethically.


Responsible
To safeguard human rights and protect the data we are entrusted with, we work with human rights experts, and educate, empower and share our research with customers and partners.


Inclusive
AI should respect the values of all those impacted, not just those of its creators. To achieve this, we test models with diverse data sets, seek to understand their impact, and build inclusive teams.


Accountable
To create AI accountability we seek stakeholders feedback, take guidance from the Ethical Use Advisory Council, and conduct our own data science review board.


Transparent
We strive for model explainability and clear usage terms, and ensure customers control their own data & models.


Empowering
Accessible AI promotes growth and increased employment, and benefits society as a whole.",Relevant Page: https://www.salesforceairesearch.com/static/ethics/EthicalAIMaturityModel.pdf
24,Samsung Electronics,Korea; Republic (S. Korea),Hardware,,,https://www.samsungsds.com/en/digital_responsibility/ai_ethics.html,AI Ethics Principles,"AI Ethics Principles
AI is a rapidly developing area and its global social, economic impact is also growing.
While recognizing that technologies promote innovation, the same technologies raise important challenges to be addressed. Samsung SDS acknowledges the use of AI technologies should aspire to human dignity and human rights as well as to a sustainable environmental ecosystem. Therefore, Samsung SDS will establish corporate AI ethics principles based on UNESCO's recommendations for the ethics of artificial intelligence. We believe all use of AI technologies must respect the rule of law, human rights, and values of equity, privacy, and fairness. These principles set out our commitment to develop, deploy, and use technology responsibly.

1. Respect for Human Rights
Based on the Samsung Spirit, our top priority is to benefit humanity and society. We respect and comply with international human rights laws and values in general and also in relation to AI technologies. Further, we will work to limit any potentially harmful or abusive application that can negatively affect human beings and their rights as we develop and deploy AI technologies.

2. Diversity and Inclusion
We believe that everyone should be treated fairly and equitably. We understand that defining fairness is not always simple and differs across cultures and societies. We will seek to avoid biased results and unjust impact on sensitive characteristics such as race, ethnicity, gender, nationality, income, sexual orientation, ability, and political or religious belief. We also will seek to avoid exposing children to inappropriate content.

3. Data and Privacy Protection
We recognize the importance of protecting the privacy and security of people’s data. To minimize privacy risks, we will continue to monitor data processing processes and develop safe and secure practices.

4. Conservation of Environmental Ecosystem
We will comply with relevant national and international regulations, standards and practices to assure that AI development and services do not adversely affect the sustainability of the environment and ecosystem.

5.Communication
We believe in transparency and explainability. AI will be explainable for users to understand its decisions or recommendations to the extent technologically feasible and that this does not jeopardize corporate competitiveness. Samsung SDS will also devise countermeasures against the risks and negative consequences that AI technology can cause to the users.",
25,SAP,Germany,Software,01-01-2022,01-01-2022,https://www.sap.com/documents/2022/01/a8431b91-117e-0010-bca6-c68f7e60039b.html,SAP Global AI Ethics Policy,"Global Artificial Intelligence (AI) Ethics Policy
Version Number: 1.0
Effective Date: January 1, 2022
Version 1.0, January 1, 2022
The online version of this document is the officially released version.
 Any copies or print-outs are not controlled
Version 1.0, January 1, 2022
The online version of this document is the officially released version.
 Any copies or print-outs are not controlled
1 INTRODUCTION
SAP is committed to the ethical development, deployment, use, and sale of SAP developed Artificial
Intelligence (AI) systems. The policy defines a group-wide minimum standard for the development,
deployment, use, or sale of SAP’s Artificial Intelligence systems. It defines requirements for SAP’s business
processes that involve AI and assigns clear responsibilities.
The policy is based on, and has been built upon the foundation of the SAP Guiding Principles for Artificial
Intelligence, established in 2018, as laid out below:
• We are driven by our values.
• We design for people.
• We enable business beyond bias.
• We strive for transparency and integrity in all that we do.
• We uphold quality and safety standards.
• We place data protection and privacy at our core.
• We engage with the wider societal challenges of Artificial Intelligence.
2 PURPOSE AND OBJECTIVES
SAP believes that Artificial Intelligence has the potential to unlock abundant potential for businesses,
governments, and society. But, like all great technological advancements, AI also has the potential to create
economic, political, and social challenges, depending upon how it is used and implemented. In addition, the
speed at which the technology has moved into common usage has outpaced guidance from governmental
policymakers on acceptable use. For these reasons, the development, deployment, use, and sale of AI
systems at SAP needs to be governed by clear rules of ethics that are aligned with SAP’s established
guiding principles for AI and its core organizational values, supplemental to any existing or pending
legislation.
Foundational to SAP’s approach to AI Ethics is the company’s commitment (through its Global Human Rights
Commitment Statement) to uphold and support the Universal Declaration of Human Rights, and to respect,
promote, and support internationally recognized human rights and widely accepted international norms.
An essential part of this commitment is the prohibition of discrimination and harassment of humans based on
personal factors (including for example culture, race, ethnicity, religion, age, gender, sexual orientation,
gender identity, physical or mental disability etc.), in order to promote respect for human autonomy and to
ensure that the moral worth and dignity of all human beings is respected. In addition, SAP’s goals for AI
systems include protecting people from harm, looking after the well-being of others, treating all individuals
equitably and justly, ensuring the entitlement to equal freedom and dignity under the law, the protection of
civil, political and social rights, the universal recognition of personhood, and the right to free and
unencumbered participation in the life of the community.
3 SCOPE
This Policy applies to SAP and its employees worldwide involved in the development, deployment, and sale
of SAP developed AI systems. If any SAP subsidiary has its own AI Ethics Policy, upon enactment of the
Global AI Ethics Policy, it immediately must align with this Policy.
The Policy defines SAP management’s intent and expectations for the ethical development, deployment, and
sale by SAP employees of SAP-developed AI systems and the principles may be used to provide guidance
and counsel to customers and partners.
However, in ensuring the rights of customers to own and manage their own data SAP may have limited
knowledge of how they ultimately use SAP AI products and services. SAP shall endeavor to educate and
advise customers and partners based on the principles in this policy, recognizing that specific use case
responsibility lies with the customer or partner, acting in good faith in accordance with local laws and
regulations. SAP reserves the right to take appropriate action in the event of a customer violation of the
principles of this policy.
Other policies that were in place before this Policy was enacted may remain in force but must be adapted to
comply with the rules of this Policy upon the next review or modification cycle.
4 TERMS AND DEFINITIONS
SAP Group SAP SE and its subsidiaries.
SAP Subsidiary Any legal entity where SAP SE holds more than fifty percent (50%) of
the shares or voting rights or such entity is controlled by the SAP SE.
Version 1.0, January 1, 2022
The online version of this document is the officially released version.
 Any copies or print-outs are not controlled
AI Ethics A set of values, principles and techniques that employ widely
accepted standards of right and wrong to guide moral conduct in the
development, deployment, use and sale of AI technologies.
Artificial Intelligence
(AI)
Typically defined as the ability of a machine to perform cognitive
functions we associate with human minds, such as perceiving,
reasoning, learning and problem solving.
A system's ability to correctly interpret external data, to learn from
such data and to use those learnings to achieve specific goals and
tasks through flexible adaptation.
SAP differentiates between two types of AI systems:
Rule-based AI systems are characterized by the fact that the
behavior of their components is fully defined by rules created by
human experts. These systems are often described as symbolic or
expert systems.
Learning-based AI systems are differentiating themselves by the
fact that their initial configuration made by humans is only the basis
for the final form of their functions. With the help of data, they train
how to solve a problem and continuously adapt their function in this
process. For learning-based AI systems, humans define the problem
and the goal, but the behavior rules and relationships required for the
system are learnt in an automized way.
In addition, hybrid systems containing both learning-based and rulebased methods are available.
Impact Assessment shall be performed on such Learning-based AI
systems during its development phase and subsequent phases to
ensure that there are no unintended consequences.
AI Systems Any AI based algorithm, component and/or software embedded in
SAP products and services (e.g. AI platform, databases, embedded or
stand-alone applications, features and solutions, Conversational AI
(‘Chatbots’)).
Agency The capacity of individuals to act independently and to make their
own free choices.
Application Any program, or group of programs, that is designed for the end user.
Autonomy The capacity to make an informed, uncoerced decision.
Bias An inclination or prejudice for or against one person or group,
especially in a way considered to be unfair.
Black Box Algorithms Any artificial intelligence system whose inputs and operations are not
visible to the user or another interested party. A black box, in a
general sense, is an impenetrable system.
Deep learning modeling is typically conducted through black box
development: The algorithm takes millions of data points as inputs
and correlates specific data features to produce an output. That
process is largely self-directed and is generally difficult for data
scientists, programmers and users to interpret.
Data Subject Any individual person who can be identified, directly or indirectly, via
an identifier such as a name, an ID number, location data, or via
factors specific to the person's physical, physiological, genetic,
mental, economic, cultural or social identity.
Deep Neural Network A technology developed to simulate the activity of the human brain –
specifically, pattern recognition and the passage of input through
various layers of simulated neural connections.
Developed (as relates to
AI systems)
Includes Design, SAP Standard Development, Prototyping, Coinnovation, and Innovation and Cloud Service team’s development
activities.
Version 1.0, January 1, 2022
The online version of this document is the officially released version.
 Any copies or print-outs are not controlled
Explainability The ability to explain both the technical processes of an AI system
and the related human decisions (e.g. application areas of a system).
Technical explainability requires that the decisions made by an AI
system can be understood and traced by human beings.
Fairness Impartial and just treatment or behavior without unjust favoritism or
discrimination.
Feature A distinguishing characteristic of a software item (e.g. performance
or functionality).
Human-in-the-loop AI System functionality for human beings to intervene in every
decision cycle of the system
Human-on-the-loop Refers to the capability for human intervention during the design cycle
of the system and monitoring the system’s operation.
Human-in-command Refers to the capability to oversee the overall activity of the AI system
(including its broader economic, societal, legal and ethical impact)
and the ability to decide when and how to use the system in any
particular situation. This can include the decision not to use an AI
system in a particular situation, to establish levels of human discretion
during the use of the system or to ensure the ability to override a
decision made by a system.
Normative In the context of practical ethics, ‘normative’ refers to a given concept,
value, or belief that puts a moral demand on one’s practices, i.e. that
such a concept, value or belief indicates what one ‘should’ or ‘ought
to’ do in circumstances where that concept, value or belief applies.
Platform Any hardware or software used to host an application or service.
Software A set of instructions, data or programs used to operate computers and
execute specific tasks.
Solution A set of related software programs and/or services that are sold as a
package.
Subordinate Treat or regard as of lesser importance than something else
Transparency Characteristic of AI that involves the justifiability of the processes that
go into its development and the implementation and of its outcome –
i.e. the soundness of the justification of its use.
Transparency is characterized by visibility or accessibility of
information or the characteristic of being easily seen through and
explained. The principle of transparency entails that development and
implementation processes are justifiable through and through. It
demands as well that an algorithmically influenced outcome is
interpretable and made understandable to affected parties.
Version 1.0, January 1, 2022
The online version of this document is the officially released version.
 Any copies or print-outs are not controlled
5 ROLES AND RESPONSIBILITIES
AI Ethics Office Context:
• Different AI Ethics stakeholders need to be connected and
aligned.
Structure:
• Orchestration of the AI Ethics Office is the responsibility of SAP's
Chief Sustainability Officer team.
Responsibilities:
• Convenes the AI Ethics Steering Committee.
• Is the contact for a L1 unit if a L1 unit is unable to make a use
case decision regarding AI Ethics or questions or concerns
remain.
AI Ethics Steering
Committee
Context:
• As an emerging technology with significant innovation potential
without standardization so far, execution of ethical AI standards
requires a cross-company approach anchored in strong LOB
expertise and perspectives.
Structure:
• Orchestration of the AI Ethics Steering Committee is the
responsibility of SAP’s AI Ethics Office.
• AI Ethics Steering Committee is a ‘sub-committee’ of the
Sustainability Council.
• In addition to representatives with AI product, legal, governmental
and ethical expertise, members include representatives
responsible for non-standard AI development and
implementation.
Responsibilities:
• Serve to advise SAP personnel on how specific use cases are
affected by this policy and the related guiding principles
• Monitor, evolve and update SAP’s Guiding Principles for AI.
• Oversee ongoing updating of SAP’s AI Ethics policy to guide
implementation of ethical AI in all LOBs developing, deploying or
selling AI systems.
• Provide guidance/direction on AI use cases to LOBs .
• Provide recommendations to SAP Executive Board on critical
product/customer use cases with wider implications for SAP as a
company.
• Provide Sustainability Council with bi-annual update on
status/progress (written).
• Orchestrate AI Ethics use case review process.
• In instances of unresolvable AI System use case conflict between
AI Steering Committee guidance/direction and relevant Business
Unit, prepare the case and recommendation for Sustainability
Council review.
• Align with and obtain insights from External Advisory panel on AI.
• Steering committee members to offer employees, through some
of the existing channels, opportunity to provide feedback and
exchange dialog on AI Ethics on regular basis.
Sustainability Council Context:
• Sustainability Council received mandate (2018) to act as internal
‘Ethics Advisory Board’ for SAP, reviewing critical behavior and
making recommendations to the SAP Executive Board on
appropriate action.
Structure:
• Orchestration of the Sustainability Council is the responsibility
of the Chief Sustainability Officer’s office
• The council is the highest-level governance structure for
sustainability at SAP
Version 1.0, January 1, 2022
The online version of this document is the officially released version.
 Any copies or print-outs are not controlled
• Senior leaders from all board areas and specific business
units are members of the council
Responsibilities:
• Oversight of AI Ethics implementation as reported by AI Ethics
Steering Committee on bi-annual basis.
• Provide longer term strategic direction for AI Ethics related topics
at SAP.
• Review critical use cases, as elevated from AI Ethics Steering
Committee.
• Provide recommendations to SAP Executive Board on critical use
cases with wider implications for SAP as a company.
Important Stakeholders
impacted by Policy
• Developers and Data Scientists should implement and apply
the requirements to the development processes.
• Sales or Consulting/Deployment Personnel (e.g. Admin or
Data Processors) should ensure that the systems they sell and
the products and services they offer meet the requirements.
• Executive Board should support appropriate level of governance
structure to ensure the requirements are met. In addition, where
necessary, the board must take a decision on whether to allow or
prohibit a usecase as per the defined escalation process.
• End-users, customers, partners and the broader society
should be informed about these requirements and be able to
request that they are upheld.
Version 1.0, January 1, 2022
The online version of this document is the officially released version.
 Any copies or print-outs are not controlled
6 ETHICAL AI AT SAP
Whilst different groups of stakeholders have different roles to play in ensuring that the requirements are met,
end-users, customers, partners, and the broader society should be informed about these requirements and
be able to request that they are upheld.
AI systems shall only be developed, deployed, used or sold by SAP in accordance with the principles laid out
below.
Human Agency and Oversight:
Employees need to consider the following when developing AI systems as they relate to Human Agency and
Oversight:
• Before implementation, the decision-making degrees of freedom of the AI system must be defined.
• When two or more AI systems are connected to each other or embedded within each other,
additional testing and control measures should be performed for the individual AI systems as well as
for the overall system
• The target definition of the AI system must be given by a human.
• AI systems shall be subject to appropriate human oversight, and the rights and freedoms of a human
shall exceed that of AI systems.
• Human oversight shall be achieved through an appropriate governance mechanism. This could
include but not be exclusive to human-in-the-loop, human-on-the-loop, or human-in-command, and
shall be decided on a case-by-case basis. The use cases will be first reviewed based on the impact
assessment list. Selected use cases shall be brought to the attention of the steering committee for
further deliberation and decision.
• In alignment with the SAP Global DPP Policy and SAP DPP Guidance documents, in situations
where humans may be directly impacted by a decision made by SAP’s AI system, human oversight
shall be introduced to safeguard that AI system does not undermine human autonomy or introduce
unintended consequences.
• As far as is practical, a clear and simple explanation shall be provided as to how decisions were
made by an AI system used in automated decision processes.
• When human-on-the-loop models are used, appropriate extensive testing and governance shall be
conducted during development and deployment to ensure the system behaves as intended by the
developers and does not have any unintended behavior, outputs, or usage.
Addressing Bias and Discrimination:
AI systems often gain insights from the existing structures and behavior of the societies they analyze. As a
result, data-driven technologies can reproduce, reinforce, and amplify patterns of marginalization, inequality,
and discrimination that exist in society and may be encoded into data sources used for the creation of AI.
Equally, because many of the features, metrics, and analytic structures of the models that enable data
mining are chosen by their developers, AI systems can potentially replicate their developers’ preconceptions
and biases.
Finally, data samples used to train and test algorithmic systems may be insufficiently representative of the
populations or the past situations from which they are drawing inferences. This may include cases where the
type of business, industry, or enterprise from which the original datasets were obtained are inappropriate for
the AI systems being developed and deployed.
These biases can negatively impact both the development and outputs of AI systems and, in turn, customers
or end users. Particular care shall be taken when there is a risk of causing discrimination or of unjustly
impacting underrepresented groups.
Employees need to consider the following as they relate to addressing bias and discrimination in SAP
developed AI systems:
• In addition to the conditions laid down in SAP’s Global DPP Policy, AI systems shall not be
developed or deployed to de-anonymize already anonymized data which may result in the
identification of individuals or groups.
• SAP shall endeavor to achieve fairness; AI systems shall not intentionally generate unfairly biased
outputs.
• Where relevant, the data used to train AI systems shall be as inclusive as possible, representing as
diverse a cross-section of the population or past situations as possible, and as free as possible from
(or accounted and mitigated for) any historic or socially constructed biases, inaccuracies, errors, and
mistakes.
• SAP shall endeavor to detect unfairly biased outputs and shall implement technical and/or
organizational measures to prevent direct or indirect prejudice, discrimination, or marginalization of 
Version 1.0, January 1, 2022
The online version of this document is the officially released version.
 Any copies or print-outs are not controlled
groups or individuals, e.g. by reducing bias in training data.
• Wherever possible, developers shall seek to involve impacted/affected users to evaluate and check
that outputs are diverse and discrimination free.
• In addition to the conditions laid down in SAP’s Global Development Policy and Product Standards,
processes shall be put in place to test and monitor for potential biases during the development,
deployment, and use phase of AI systems.
o It shall be trained and tested on as expansive as is feasible, representative, relevant,
accurate, and generalizable datasets.
o The model architectures shall not include target variables, features, processes, or analytical
structures which are unreasonable, ethically objectionable, or unable to be validated
according to the principles laid out in this document.
o It shall be developed and deployed so that it has no intentionally harmful impacts on users
and/or direct and indirect affected of the system.
o Where feasible, a fairness function shall be applied to test AI systems for unbiased output.
• AI software shall be user-centric, addressing the widest possible range of applicable end-users, and
following relevant accessibility standards, regardless of users’ age, gender, abilities, or
characteristics.
• As it pertains to the addressing of bias and discrimination, AI systems shall comply with or be in
alignment with SAP’s Global Development Policy, Product Standards Governance, Product
Standards, CUX Design Principles, and Global DPP Policy.
• The use of data for the testing of AI systems shall comply with SAP’s Global DPP Policy.
Transparency and Explainability:
SAP’s AI systems are held to specific standards in accordance with their level of technical ability and
intended usage. Their input, capabilities, intended purpose, and limitations shall be communicated clearly to
our customers along with the necessary technical tools for training and prediction.
The problem is AI systems are not morally accountable agents and cannot be held accountable for their
actions. It is therefore essential that mechanisms be put in place so that SAP developed AI systems are
objective and viable as intended by prioritizing both the transparency of the process by which the AI system
is developed, as well as the transparency and interpretability of its decisions and behaviors.
Employees need to consider the following as they relate to the Transparency and Explainability of SAP
developed AI systems:
• The data sets and the processes that produce an AI system’s decisions, including those of data
gathering and data labelling as well as the algorithms used by the developed AI system, shall be
documented to allow for traceability and transparency.
• The capabilities and limitations shall be documented as part of the development process in a manner
appropriate to the use case at hand. This shall include information regarding the AI system's level of
accuracy (performance metric), as well as its limitations and capabilities.
• In alignment and compliance with SAP’s Global DPP Policy, products that use AI systems in the
processing of personal data must provide transparency to the extent possible as to how the AI
system was used in clear and simple language if requested by the data subject.
• In alignment and compliance with SAP’s Global DPP Policy, AI systems that engage in profiling or
automated decision-making must be able to provide explanations to the extent possible to data
subjects upon request, describing the data segment the subject was placed into and the reasons
they were placed there. In addition, the reasons as to why the decision was made shall be provided
if requested by the data subject. The explanation must be such as to provide the data subject
grounds to challenge the decision.
• The methods used for developing, testing and validating, and the outcomes of or decisions made by
the AI system shall be fully documented as part of the development process according to SAP’s
Global Development Policy and Product Development Standards.
• Where applicable, when interacting directly with humans (including via Conversational AI or
‘Chatbots’):
o AI systems shall be made identifiable as such to appropriate end users.
o AI systems shall be developed such that it does not encourage humans to develop
attachment and/or empathy of users towards the AI system.
o AI systems shall clearly signal to end users that its social interaction is simulated.
• SAP AI system developers shall endeavor to make the decisions, proposals, and outputs of the AI
system as transparent as possible, based on the use case. This can be achieved using application
logs or the user interface (UI) to allow for the best understanding and traceability of these. 
Version 1.0, January 1, 2022
The online version of this document is the officially released version.
 Any copies or print-outs are not controlled
• The user shall be made aware that AI systems typically work on confidence levels; the actual
confidence level of a particular output shall be made available if required.
• The AI system’s purpose, constraints, requirements, and decisions shall be defined and documented
in a clear and transparent manner to the non-technical general reader or user.
• ‘Black Box’ and/or Deep Neural Network software/ conditions:
o Where so-called ‘black box’ algorithms have been developed by SAP, other explicability
measures shall be provided. These should include traceability, auditability, and transparent
documentation and communication of the software’s capabilities.
o Wherever possible an explanation of the output shall be made available; where not possible,
users shall be made aware that the output may not be fully explainable.
o The requirement for this information will be dependent on the context and the severity of the
consequences. A risk matrix approach shall be followed here.
• AI system development shall take into account the context and environment in which the system will
operate such that, even with good intentions, no harm or misuse is likely to occur to humans when AI
systems are deployed.
• To the extent that a 3rd Party AI system (e.g. ‘TensorFlow’) is embedded in SAP solutions, this
policy shall apply to the overall SAP software solution.
Society:
SAP developed AI systems shall be developed to augment, complement, and empower human cognitive,
social, and cultural skills, and not act to prevent or restrict activities commensurate with a free society.
Employees need to consider the following when developing or deploying AI systems as it relates to civic
society:
• In addition to the existing provisions set out in SAP’s Global DPP Policy, AI systems shall not be
developed or deployed for human surveillance that is utilized for the targeting of individuals or
groups, either by biometrics, facial recognition, or other identifiable features, with the purpose of
disregarding or abusing the human rights of the individuals or groups.
• AI systems shall not be developed or deployed for purposes which cause individuals or groups to be
discriminated against or excluded from equal access to AI’s benefits and opportunities that may be
available to the wider population.
• AI systems shall not be developed or deployed for deception or unfair manipulation of individuals or
groups via public forums, media, or moderation of other similar uses.
• AI systems shall not be developed or deployed to undermine human debate or democratic electoral
systems.
• AI system development or deployment shall be conducted with minimum to no explicit damage to the
environment
Version 1.0, January 1, 2022
The online version of this document is the officially released version.
 Any copies or print-outs are not controlled
7 GOVERNANCE
Use Case Review
As far as SAP developed AI systems are concerned, SAP personnel shall approach ethical dilemmas and
trade-offs related to their use via reasoned, context-relevant, and evidence-based decision making rather
than intuition or random discretion.
Where a use case proposed for an SAP developed AI system may breach this policy at any stage of the
lifecycle of the AI system, or in order to determine whether the application of a specific use case should or
should not be pursued, employees should first raise the issue for evaluation by their immediate L1 unit. This
even applies if employees only have doubts or concerns.
If questions or concerns remain, or a decision is unable to be made by the appropriate L1 unit, then
employees should inform the AI Ethics Office via an e-mail to ai.ethics@sap.com describing the use case.
On receipt of the e-mail the AI Ethics Office will convene the AI Ethics Steering Committee, who will review
the use case and advise the employee on how a specific use case may be affected by this policy by
assessing each use case submitted to it in the context in which the AI system is to be applied.
Where an employee has a concern that this policy is not being followed for a particular use case, and they
are not able to obtain an appropriate response that addresses their concern(s), and wish to do so
anonymously, the employee shall use the established whistleblower process to raise this issue to the
attention of the AI Ethics Steering Committee.
In the exceptional situation that the AI Ethics Steering Committee cannot come to a decision on a use case
query referred to it, the use case may be referred to the Sustainability Council (the ‘Council’) for final
arbitration.
Where a use case decision may have wider implications to SAP as a company, then the recommendation
made by either the AI Ethics Steering Committee, or the Council may be referred to the SAP Executive
Board for review and ratification.
Where no ethically acceptable trade-offs can be identified, then the development, deployment, use or sale of
the AI System shall not proceed in that form.
The Steering Committee will continually review the appropriateness of the ethical decision made to ensure
that where necessary suitable changes can be made based on evolving circumstances.
Use case Review Process: 
www.sap.com/contactsap
© 2021 SAP SE or an SAP affiliate company. All rights reserved.
No part of this publication may be reproduced or transmitted in any form or for any purpose without the express permission of SAP SE or an SAP affiliate company.
The information contained herein may be changed without prior notice. Some software products marketed by SAP SE and its distributors contain proprietary software components of other software vendors.
National product specifications may vary.
These materials are provided by SAP SE or an SAP affiliate company for informational purposes only, without representation or warranty of any kind, and SAP or its affiliated companies shall not be liable
for errors or omissions with respect to the materials. The only warranties for SAP or SAP affiliate company products and services are those that are set forth in the express warranty statements
accompanying such products and services, if any. Nothing herein should be construed as constituting an additional warranty.
In particular, SAP SE or its affiliated companies have no obligation to pursue any course of business outlined in this document or any related presentation, or to develop or release any functionality
mentioned therein. This document, or any related presentation, and SAP SE’s or its affiliated companies’ strategy and possible future developments, products, and/or platform directions and functionality are
all subject to change and may be changed by SAP SE or its affiliated companies at any time for any reason without notice. The information in this document is not a commitment, promise, or legal obligation
to deliver any material, code, or functionality. All forward-looking statements are subject to various risks and uncertainties that could cause actual results to differ materially from expectations. Readers are
cautioned not to place undue reliance on these forward-looking statements, and they should not be relied upon in making purchasing decisions.
SAP and other SAP products and services mentioned herein as well as their respective logos are trademarks or registered trademarks of SAP SE (or an SAP affiliate company) in Germany and other
countries. All other product and service names mentioned are the trademarks of their respective companies. See www.sap.com/trademark for additional trademark information and notices.",
26,SONY,Japan,Hardware,09-25-2018,04-01-2021,https://www.sony.com/en/SonyInfo/csr_report/humanrights/AI_Engagement_within_Sony_Group.pdf,Sony Group AI Ethics Guidelines,"AI Engagement within Sony Group
Through the utilization of artificial intelligence (AI), Sony aims to contribute to the development of a
peaceful and sustainable society while delivering kando - a sense of excitement, wonder or emotion
- to the world. Starting from the electronics business, Sony has continued to expand its business area
and has become a diverse global company that offers entertainment such as music and movies, as
well as financial services. To operate these business areas based on Sony’s Purpose to ”Fill the world
with emotion, through the power of creativity and technology.”, Sony Group AI Ethics Guidelines are
hereby set forth below to ensure and promote a dialogue with various stakeholders and the proper
utilization and research and development (hereafter “R&D”) of AI within Sony Group.
Sony Group AI Ethics Guidelines
Scope of the Guidelines
The “Sony Group AI Ethics Guidelines” (Guidelines) set forth the guidelines that must be followed by
all officers and employees of Sony when utilizing AI and/or conducting AI-related R&D.
""Utilization of AI"" within Sony means the following:
1. The provision of products and services by Sony, including entertainment content and
financial services, which utilize AI; and
2. The usage of AI for various purposes by Sony in its business activities such as R&D,
product manufacturing, service provision, and other operational activities.
Definitions in the Guidelines
“AI” means any functionality or its enabling technology that performs information processing for
various purposes that people perceive as intelligent, and that is embodied by machine learning
based on data, or by rules or knowledge extracted in some methods.
“Sony” means Sony Group Corporation and any company where more than 50% of voting rights are
directly or indirectly owned by Sony Group Corporation.
Revision of the Guidelines
Sony will review and evolve the Guidelines as needed based on national and regional AI-related
guidelines, changes in people’s lifestyles and environments, accumulation of practices in the relevant
industry, and information exchanged with its various stakeholders.
2
1. Supporting Creative Life Styles and Building a Better Society
Through advancing its AI-related R&D and promoting the utilization of AI in a manner harmonized
with society, Sony aims to support the exploration of the potential for each individual to empower
their lives, and to contribute to enrichment of our culture and push our civilization forward by
providing novel and creative types of kando. Sony will engage in sustainable social development and
endeavor to utilize the power of AI for contributing to global problem-solving and for the
development of a peaceful and sustainable society.
2. Stakeholder Engagement
In order to solve the challenges arising from use of AI while striving for better AI utilization, Sony will
seriously consider the interests and concerns of various stakeholders including its customers and
creators, and proactively advance a dialogue with related industries, organizations, academic
communities and more. For this purpose, Sony will construct the appropriate channels for ensuring
that the content and results of these discussions are provided to officers and employees, including
researchers and developers, who are involved in the corresponding businesses, as well as for
ensuring further engagement with its various stakeholders.
3. Provision of Trusted Products and Services
Sony understands the need for safety when dealing with products and services utilizing AI and will
continue to respond to security risks such as unauthorized access. AI systems may utilize statistical or
probabilistic methods to achieve results. In the interest of Sony’s customers and to maintain their
trust, Sony will design whole systems with an awareness of the responsibility associated with the
characteristics of such methods.
4. Privacy Protection
Sony, in compliance with laws and regulations as well as applicable internal rules and policies, seeks
to enhance the security and protection of customers' personal data acquired via products and
services utilizing AI, and build an environment where said personal data is processed in ways that
respect the intention and trust of customers.
5. Respect for Fairness
In its utilization of AI, Sony will respect diversity and human rights of its customers and other
stakeholders without any discrimination while striving to contribute to the resolution of social
problems through its activities in its own and related industries.
6. Pursuit of Transparency
During the planning and design stages for its products and services that utilize AI, Sony will strive to
introduce methods of capturing the reasoning behind the decisions made by AI utilized in said
products and services. Additionally, it will endeavor to provide intelligible explanations and
information to customers about the possible impact of using these products and services.
7. The Evolution of AI and Ongoing Education
People’s lives have continuously changed with the advance in technology across history. Sony will be
cognizant of the effects and impact of products and services that utilize AI on society and will
proactively work to contribute to developing AI to create a better society and foster human talent
capable of shaping our collective bright future through R&D and/or utilization of AI.",Relevant Page: https://www.sony.com/en/SonyInfo/sony_ai/responsible_ai.html
27,Tieto,Finland,Software,10-17-2017,10-17-2017,https://www.tietoevry.com/contentassets/b097de43d84d4c84832f1fff2cb6a30d/tieto-s-ai-ethics-guidelines.pdf,Tieto’s AI ethics guidelines,"Tieto’s AI ethics guidelines
Human rights
Ensuring the freedom and liberty or people
to serve the social good.
Safety & security
AI systems are built to prevent misuse and
reduce the risk of being compromised.
Responsibility
Committed to harness AI for good, for the
planet and humankind.
Fairness & equality
Unbiased, fair and inclusive AI fostering
diversity and equality among people.",Press Release: https://www.tietoevry.com/en/newsroom/all-news-and-releases/press-releases/2018/10/tieto-strengthens-commitment-to-ethical-use-of-ai/
28,Workday,United States of America,Software,05-08-2019,05-08-2019,https://blog.workday.com/en-us/2019/workdays-commitments-to-ethical-ai.html,Workday’s Commitments to Ethical AI,"Workday’s Commitments to Ethical AI
Applications that use machine learning (ML) can make predictions that help inform better decisions. Here's our six key principles for responsibly integrating ML.

Today’s business environment is more complex than ever. Whether it’s new regulatory requirements or the battle for talent, customers all have a common opportunity: identifying new ways to make smarter decisions that lead to better outcomes. With this in mind, Workday is incorporating machine learning (ML) technologies—a subset of artificial intelligence (AI)—into our applications so that customers can make more informed people and business decisions, accelerate operations, and assist workers with data-driven predictions that lead to better outcomes.

At Workday, ML isn’t about supplanting human decision-makers. Rather, ML-fueled applications make predictions that, when combined with human judgment, help inform better decisions. But the success of ML, like any emerging technology, depends upon trust, and that trust will exist only if companies adhere to responsible, ethical practices.

Workday believes ML in the enterprise will fundamentally improve the way we work and live, but in the face of such a profound technological and societal change, it’s vital that we commit to an ethical compass. Ours is comprised of six key principles that guide how we develop ML for the enterprise responsibly and work to help address its broader societal impact:

We Put People First
Workday always respects fundamental human rights. We apply ML to deliver better business outcomes and help people in their decision-making. Our solutions provide customers control over how recommendations are used.

We Care about Our Society
We believe that humans will always be at the center of work. We focus on how ML can align opportunity with talent, and on contributing to the development of an ML-ready workforce.

We Act Fairly and Respect the Law
Workday acts responsibly in our design and delivery of ML products and services, and strives to identify, address, and mitigate bias in our ML technologies. We aim to ensure that ML recommendations are equitable. Our products and services are developed and designed to enable compliance and we are engaged in the policy dialogue around regulation of new technologies.

We Are Transparent and Accountable
We explain to customers how our ML technologies work, the benefits they offer, and describe the data needed to power any ML solutions we offer. We demonstrate accountability in ML solutions to customers and give them a wide range of choice in how they deploy them.

We Protect Data
Workday’s Privacy Principles apply to all of our products and services, including to our ML efforts. We minimize the data used, and embrace good data stewardship and governance processes.

We Deliver Enterprise-Ready ML Technologies
We apply our leading quality processes—with input from customers—when developing and  releasing ML technologies. We deliver meaningful ML-powered solutions that help our customers tackle real-world challenges.

But it isn’t enough just to have ethics principles: we are building them into the fabric of our product development and are ensuring we have processes that drive continued compliance with them. We have a long history of this in the privacy space, including privacy-by-design processes as well as third-party audits against our controls and standards.

We are embracing a similar set of ethics-by-design controls for ML, and already have in place robust review and approval mechanisms for release of new technologies, as well as any new uses of data. We’re committed to ongoing reviews of our processes, and evolving them to incorporate new industry best practices and regulatory guidelines.

Above all, as we look forward and as with all our product efforts, we are focused on our customers’ needs and requirements so we can provide new services and technologies that allow them, and their people, to achieve more. By partnering together, we can ensure ethical development and use of ML in the enterprise.",
29,DeepMind,United Kingdom,Software,,,https://www.deepmind.com/about/ethics-and-society,Exploring the real-world impacts of AI,"Securing safe, accountable, and socially beneficial technology cannot be an afterthought.
With the right focus on ethical standards and safety, we have better chances of finding AI’s potential benefits. By researching the ethical and social questions involving AI, we ensure these topics remain at the heart of everything we do.
We start from the belief that AI should be used for socially beneficial purposes and always remain under meaningful human control. Understanding what this means in practice is essential.  

Finding ways to involve the broader society in our work is fundamental to our mission, so partnerships with others in the field of AI ethics is a crucial element of our approach.

We embrace scientific values like transparency, freedom of thought, and the equality of access, and we deeply respect the independence and academic integrity of our researchers and partners.
Questions about AI extend far beyond its technology. Through our partnerships, we’ve created public lectures, forums, and resources to better understand the societal impacts of AI. Below are some of our recent highlights.
Launching public lectures
We partnered with the Royal Society on a free public lecture and panel series, You & AI. These lectures, featuring experts like Kate Crawford and Joseph Stiglitz, explored AI’s capabilities, future directions, and potential societal effects. Each lecture was recorded and is available to watch online.
Engaging citizens directly
Together with the RSA, we created the Forum for Ethical AI, a public engagement programme for discussing the use of automated decision-making tools. During this forum, citizen participants developed a critical framework for addressing transparency, accountability, and accessibility of AI technology.
Convening experts
In partnership with Princeton University, we organised a workshop to explore how criminal justice systems use AI technology. We brought together technologists and advocates to discuss solutions and create resources, directly informed by affected communities, which explore the harm that can be caused by predictive tools.
Themes
We want to promote research that ensures AI works for all. Our research themes are designed to reflect the key ethical challenges that exist for us and the wider AI community. We undertake research and collaborations in each of these areas, determined by the urgent challenges ahead.
Privacy, transparency, and fairness
AI systems can use large-scale and sometimes sensitive datasets, such as medical or criminal justice records. This raises important questions about protecting people’s privacy and ensuring that they understand how their data is used. Also, the data used for training automated decision-making systems can contain biases, creating systems that might discriminate against certain groups of people.

How do concepts such as consent and ownership relate to using data in AI systems?
What can AI researchers do to detect and minimise the effects of bias?
What policies and tools allow meaningful audits of AI systems and their data?
AI morality and values
AI systems could make societies fairer and more equal. But different groups of people hold different values, meaning it is difficult to agree on universal principles. Likewise, endorsing values held by a majority could lead to discrimination against minorities.

How can we ensure that the values designed for AI systems reflect society?  
How do we prevent AI systems from causing discrimination?
How do we integrate inclusive values into AI systems?
Governance and accountability
The creation and use of powerful new technologies requires effective governance and regulation, ensuring they are used safely and with accountability. In the case of AI, new standards or institutions may be needed to oversee its use by individuals, states, and the private sector - both internationally and within national borders.

What kinds of governance makes sense for rapidly developing technologies like AI?
Can existing institutions uphold the rights of everyone affected by AI?
What can we learn from other fields like biotechnology or genetics that might influence how AI is used?
AI and the world’s complex challenges
By uncovering patterns in complex datasets and suggesting promising new ideas and strategies, AI technologies may one day help solve some of humanity’s most urgent problems. But applying AI technologies to real-world problems takes careful consideration.

Which problems could AI help address?
How can AI research best contribute?
Who should we be working with to help solve problems?
Misuse and unintended consequences
While AI systems have great potential, they also come with risks. For example, they might malfunction or not operate in the ways they were intended. We might also rely on them too heavily in situations that go beyond their abilities or a technology designed to help society might be repurposed in unethical or harmful ways.

How can these risks be monitored across the world?
What structures can be put in place to minimise harm?
How do we ensure that people maintain control of AI systems?
Economic impact: inclusion and equality
Like previous waves of technology, AI could contribute to a huge increase in productivity. However, it could also lead to the widespread displacement of jobs and alter economies in ways that disproportionately affect some sections of the population. This poses important questions about the kinds of societies and economies we want to build.

How can we anticipate the social or economic impacts of AI?
What new opportunities are created?
How do we ensure AI has a net positive effect on the world?",
30,Baidu,China,Software,,,https://esg.baidu.com/en/article/Responsible_AI,Responsible AI,"Responsible AI

Artificial intelligence (AI) has witnessed the rapid development in recent years. In response to legal and ethical controversies, the National Governance Committee for the New Generation AI published the Governance Principles for the New Generation Artificial Intelligence: Developing Responsible Artificial Intelligence, proposing the theme of “Responsible AI”. These integrate ethical thinking into AI technology to avoid risks. We actively respond to this theme, deepening the interpretation of AI ethics, actively participating in the formulation of AI ethic standards, and guiding the direction of AI technology.

We join hands with stakeholders such as netizens, industry associations, scientific research institutions, and social organizations to promote the construction of a responsible AI ecosystem.

AI ethics

As AI technology develops and promotes social innovation, the ethical and moral thinking behind its technology has become a core issue of social concern.

Baidu Chairman and CEO Robin Li proposed the four principles of AI ethics. AI should be “safe and controllable”, which is the highest principle; AI’s innovative vision is to promote more equal access to technologies and abilities for humanity; the value of AI is to empower mankind to learn and grow instead of surpassing and replacing mankind; the ultimate ideal of AI is to bring more freedom and possibilities to humankind. These four principles aim to establish concepts and rules that the whole society follow in terms of all new AI products and technologies, so as to enable the co-existence between AI and humankind. Only by adhering to the highest principle of “safe and controllable”, establishing sound AI ethical norms, and accelerating the implementation of AI ethical principles can we use AI technologies to achieve multi-governance, benefit more groups, and realize sustainable social development.

We strive to avoid the negative aspects of AI and the information cocoon, promoting the well-ordered development of AI.

The AI ecosystem

Baidu adheres to the ethical principles of human-centered AI. Regarding AI ethics and standards, we followed proposals related to standardization and joined in the preparation of the Introduction of AI Ethics Risks by the National Artificial Intelligence Standardization General Working Group. Baidu also collaborated with CAICT in formulating the Research Report on Privacy and Security of Facial Recognition Technology in Applications and participated in the development of Internet of Vehicles Service —User Personal Information Protection Requirements.

In terms of international exchanges, we have actively joined in the AI for Sustainable Development Goals (AI4SDGs) research plans and international cooperation networks while funding their research projects, actively promoting the training of talent and the leveraging of technology in the AI area to build global consensus.

Believing “Everyone Can AI”, Baidu is encouraged to open-source AI-related technology tools, so every developer can access the world's most advanced AI technologies and the whole society can use the technology more easily. We embrace innovation with an open mind, empower industry with a win-win attitude, and continue to contribute to the flourishing development of the AI field.
",
31,SenseTime,China,Software,2022-10-14,2022-10-14,https://oss.sensetime.com/20221014/3ec8d5aaf00d134573f1e0d214ad9270/AI%20Governance%20for%20Balanced%20Development%20White%20Paper-eng-final.pdf,"Sustainable AI Development
Learn More

SenseTime advocates the ""AI ethics that promotes human development"" and calls on all institutions, enterprises, and individuals to follow the three principles of ""sustainability, human-centric, and controllable technology"", and to respect, understand and balance the differences between countries and regions around the world in terms of historical, cultural, social and economic development. We also call on all sides to innovate AI technologies in a responsible way so as to tackle the potential challenges facing humanity in our efforts towards SDGs and promote the common progress of human civilization.
Ethical and Responsible AI
AI Governance for Balanced Development White Paper
2022-10-14
Committee of AI Ethics and Governance
SenseTime's Committee of AI Ethics and Governance (hereinafter referred to as AI Ethics Committee) was established to support the strategic development of the company in January 2020. The Committee mainly focuses on the high-level integration of the technical and social attributes of artificial intelligence, pays attention to and forecasts various potential social issues in the development of AI technology and products, and prevents and responds to potential ethical risks in the development of the company and the industry.","To implement AI governance, we have embedded our ethics
values in our products through diffusing the principles into
“ethics by design” standards and an ethics review matrix.
SenseTime is among the first companies that have set up an
Ethics and Governance Committee in the industry and made
AI governance a strategic priority. The committee is
comprised of both internal and external experts and ensures
that our business strictly adheres to recognized ethical
principles and standards.
In response to ethics risks at the data, algorithm, and application levels, SenseTime has established risk control mechanisms covering the entire life cycle of our products and has
begun to see a close-looped AI governance system forming.
At the data level, we have established a Privacy Protection
Assessment Mechanism, and a Data Security and Personal
Information Protection Committee to conduct the assessment covering the acquisition, storage, transmission, and
processing of personal information, so as to ensure that our
products comply with the design requirements for “privacy
protection by default”.
At the algorithm level, we have established an Algorithm
Security Assessment Mechanism, and an Algorithm Security
Management Working Group. The working group is tasked
with classifying and managing algorithms according to their
data type, business scenario, ethics risk level, data quality,
storage status, application scale, data importance, intervention degree on the user’s behavior and conducting security
assessments on algorithm risks arising from technical limitations, algorithm design, software defects, data security,
framework security, and other related dimensions.
At the application level, we have established an Ethics Risk
Classification Management Mechanism and an Ethics Risk
Review Team to carry out graded and targeted ethics risk
management throughout the entire product life cycle of
design, development, deployment, and operation. We have
also set up supporting processes for self-inspection,
assessment and review of risks, and follow-up reviews. We
classify ethics risks from low to high, spanning five levels
from E0 to E4, based on the impact of final product safety,
personal rights and interests, market fairness, public safety,
and environmental health.
To promote the development of responsible and verifiable
AI, we have developed a series of internal management
tools and technical tools covering data governance,
algorithm evaluation, model examination, and ethics review.
To cultivate an organizational culture for AI governance, we
have published Ethics and Governance Policy, SenseTime AI
Ethics and Governance Committee Management Charter,
and Guidelines for Ethics risk Review, to provide a set of
guidelines and clearly-defined standards for aligning
employee’ understanding and actions on AI ethics and
governance. To better educate and communicate with
employees regarding AI governance, we have established
regular training and community engagement platforms.
So far, our AI ethics and governance system and related
technical tools have been recognized by various third-party
organizations, such as the Harvard Business Review and
Artificial Intelligence Industry Alliance (AIIA). Our first White
Paper on AI Sustainable Development was also included in
the United Nations’ Resource Guide on Artificial Intelligence
Strategies.
I.
Overview of AI Development
and Governance
05
SENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW
06
Over the past decade, driven by deep learning, big data,
and Moore’s Law, AI has made many remarkable breakthroughs, and has been commercialized in various
segments such as computer vision, natural language
processing, and speech recognition. Today, AI is being
widely deployed in city management, as well as in industries such as education, finance, medical care, retail, transportation, entertainment, and manufacturing, amongst
others, and is expanding into other fields of knowledge
exploration such as scientific research and the Arts. AI is
increasingly being recognized and adopted as a general-purpose technology, which accelerates the arrival of the
era of ubiquitous intelligence.
Throughout history, general-purpose technologies have
inevitably brought fundamental changes to the existing
structure of society, while revolutionizing social productivity. This law applies to AI too. In particular, when data-driven
AI breaks the boundaries of human cognition, its impact on
the existing social structure will be even greater. Even at
the present stage of “weak (narrow) AI”, concerns about
the robustness and fairness of automated decision-making systems and the abuse of recommender systems, deep
synthesis, and biometric information identification
technologies clearly indicate that if the industry and all
stakeholders don’t act proactively to seek broad consensus on how AI should be designed, developed, and
deployed, the journey to the era of ubiquitous intelligence
will increasingly face trust challenges.
For this reason, AI governance has garnered attention
from enterprises, government agencies, international
organizations, social groups, and other stakeholders in
the past decade and made remarkable progress.
Looking at the global AI governance process, it has so
far gone through three stages of development, and has
now entered the stage of implementation:
SENSETIME I AI FOR A BETTER TOMORROW
07
Figure 1 Global AI Governance Development History
Source: Institute for AI Industry Research, SenseTime
2016 >>> Stage 1.0
Principle Discussions
2020 >>> Stage 2.0
Policy Discussion
2022 >>> Stage 3.0
Technical Verification
After analyzing 84 principles or guidelines
regarding AI ethics worldwide, Anna Jobin et al.
also found that 88% of them had been published
after 2016
In February 2020, the European Commission
released a White Paper on Artificial Intelligence , the
first in the world to propose a risk-based regulatory
framework for AI governance. Since then, major
countries have followed suit
In May 2022, Singapore launched the world’s first
open-source testing toolbox for AI governance,
“AI.Verify”
Stage 1.0 of AI Governance began in 2016 with a focus
on principle discussion.
In their study, Jessica Fjeld et al. from Harvard University
identified September 2016 as the beginning of AI Governance 1.0 with the publication of the Principles of
Partnership on AI by a group of tech giants that included
Google, Facebook, IBM, Amazon, and Microsoft. 1After
analyzing 84 principles or guidelines regarding AI ethics
worldwide, Anna Jobin et al. also found that 88% of them
had been published after 2016, and the number of
documents released by private enterprises and government agencies accounted for 22.6% and 21.4%, respectively.2
1 https://cyber.harvard.edu/publication/2020/principled-ai
2 https://doi.org/10.1038/s42256-019-0088-2
3 https://ec.europa.eu/info/files/white-paper-artificial-intelligence-european-approach-excellence-and-trust_en
4 https://www.pymnts.com/news/regulation/2022/oecds-principles-can-guide-governments-to-design-ai-regulatory-frameworks/
5 https://www2.deloitte.com/global/en/insights/industry/technology/technology-media-and-telecom-predictions/2022/ai-regulation-trends.html
Stage 2.0 of AI Governance began in 2020 with a focus
on policy discussion.
In February 2020, the European Commission released
the White Paper on Artificial Intelligence3, the first in the
world to propose a risk-based regulatory framework for
AI governance. Since then, major countries have
followed suit and explored regulations on AI-related
technologies and applications to varying degrees. For
this reason, 2020 is often referred to as the starting point
of AI regulation. According to OECD statistics, more than
700 AI policy initiatives have been proposed by over 60
countries around the world, while Deloitte Global
predicts that 2022 will see even more discussion on the
systematic regulation of AI.45
· ·
SENSETIME I AI FOR A BETTER TOMORROW
08
AI has entered a new stage of development
since 2010 where computing power and data
became the main driving force. At this stage,
AI is no longer based on just human cognition
to some extent. In fact, its rules have been far
beyond current human cognition, sparking a
wave of discussions on its governance.
- Dr. Xu Li, Executive Chairman of the Board
 and CEO of SenseTime
The relationship between principles and action.
An IBM study found that while more than half of
surveyed organizations have published or publicly
endorsed common principles of AI ethics, less than a
quarter have actually put them into practice. 6AI ethics
and governance still face many challenges in practice:
First, the integration of AI governance into the existing
organizational structure. AI governance involves information security, data governance and other overlapping
areas with the existing organizational structure. Issues
such as overlapping responsibilities and unclear scope
of work have led to certain constraints on the promotion
of implementation at the organizational level. Second, AI
governance has not been truly integrated into the industry’s business value chain. When promoting AI ethics and
governance, the lack of clear returns on investment may
lead to organizations paying less attention to AI ethics
and governance and lagging in implementation. Third,
there is a lack of consensual standards on how to
conduct AI governance.
The relationship between policy and practice.
Policymakers and technology developers share different
perspectives, positions, and understandings of policy
implications. In the process of promoting AI governance,
policy requirements need to be transformed into practical standards that can be implemented by technical and
business personnel. The following four aspects needs to
be kept in mind when promoting AI governance: First,
policy formulation needs to take into account the
dynamics and diversity of industries and AI applications,
to foster a benign environment conducive to industry
development. Second, different institutions, public
agencies, and countries need to strive to promote the
interoperability of AI governance standards. Third, AI
governance practitioners need to be involved in policy
formulation processes, so as to make policies more
practical. Fourth, policy makers and industry practitioners need to seek consensus in defining issues related
to AI governance.
The relationship between technology and users.
At present, AI governance remains within the purview of
professional discussions and corporate governance, with
end-users yet to be included in the loop of AI governance. Consequently, there are often misunderstandings
on AI governance issues within the market and society.
For example, some users may define temporary technical
issues as long-term governance challenges. Research by
IBM found that only 40% of surveyed consumers
believed businesses were responsibly and ethically
developing and deploying new technologies, and users
lacked adequate understanding on AI governance.7Therefore, the relationship between technology
and users should be properly addressed when promoting AI governance. Technology providers need to seek to
explain technology from the users’ perspectives, while AI
governance institutions need to clarify the real challenges facing governance, deepen users’ understanding on
AI governance, and give users the opportunity to participate in AI governance. In addition, it is necessary for
enterprises to increase investment in AI governance-related technical tools, to improve the verifiability of AI
governance.
6 https://www.ibm.com/downloads/cas/VQ9ZGKAE 7 https://www.ibm.com/downloads/cas/VQ9ZGKAE
10
Figure 2: Relationship Need to Be Properly Addressed in Stage 3.0 of AI Governance
Source: Institute for AI Industry Research, SenseTime
Principle < > Action Policy < > Practice Technology < > User
An IBM study found that while more than half of surveyed
organizations have published or publicly endorsed
common principles of Al ethics, less than a quarter
have actually put them into practice.
In the process of promoting Al governance, policy
requirements need to be transformed into practical
standards that can be implemented by technical and
business personnel
An IBM study found that only 40% of surveyed
consumers believed businesses were responsibly and
ethically developing and deploying new
technologies, and users lacked adequate
understanding on Al governance.
09
Stage 3.0 of AI Governance began in 2022 with a focus
on technical verification.
In 2022, there are an increasing number of initiatives
aimed at verifying how AI governance is implemented,
as the global AI governance process continues to move
forward and concepts such as trustworthy and responsible AI gain greater traction. Within the public sectors,
the government of Singapore launched the world’s first
open-source testing toolbox for AI governance, “AI.Verify”, in May 2022. In June 2022, the Spanish government
and the European Commission introduced the first pilot
project for an AI regulatory sandbox. On the market side,
the Responsible Artificial Intelligence Institute, a
US-based AI governance research institute has released
a Responsible AI Certification Program to provide
responsible AI certification services to enterprises, organizations, and institutions.
At stage 3.0, we are of the view that the technical
verification of AI governance is twofold: one is to verify
the practicability of principles, guidelines, and policy
requirements through practice, while the other is to
verify the degree to which AI ethics standards have
been implemented by relevant parties through technical or management tools. Next, implementing AI
governance needs to properly address challenges
around the following three relationship groups:
· ·
·
·
SENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW
the process of AI development and application, which
involves the entire life cycle of data, i.e. collection,
transmission, storage, processing, and circulation.
At the algorithm level, the risk mainly involves
algorithm decision-making, black box algorithm, and
algorithm security. Of these, algorithm decision-making risk refers to the inability to predict the reasons
and effects of decisions made by AI systems, due to
the unpredictability of the results of algorithmic
reasoning and the cognitive limitations of human
beings. For example, the problem of liability fixation is
a typical one. Black box algorithm risk refers primarily
to interpretability risk resulting from opaque
decision-making and the inability to be fully explained
due to the complexity of neural network algorithms.
Algorithm security risk refers to the risk caused by the
leakage or malicious modification of model parameters and insufficient fault tolerance and elasticity.
At the application level, the risk involves algorithmic
bias, ethical conflict, and labor substitution among
others. For example, due to subjective factors, or bias
contained in training data and data input during
self-learning process, machine-learning algorithms
may introduce bias into its decision-making process.
The risk of algorithm abuse can result from ill induction to users and excessive application of algorithms.
AI can also impact employment negatively in the long
term, exacerbating unfair competition and market
dominance and causing dilemmas and risks in definII. ing responsibility.
How We Think of AI Governance
11 12
SENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW
SenseTime has long attached great importance to AI governance. Since 2019, it has established and maintained a
Global AI Ethics Risk Registry internally, which covers
hundreds of AI ethics best practices and alerts. While closely tracking the development of global AI governance, we
strive to deepen our knowledge and understanding of AI
governance as we seek more systematic thinking on AI
governance issues.
Based on our in-depth analysis of global risk cases, we
observed that the governance challenges inherent to the
AI era stem mainly from three levels: data, algorithm, and
application. Specifically:
At the data level, the risk primarily involves privacy
protection, data governance, and data quality. Of these,
privacy protection risk refers to issues of privacy violation
during AI development, testing, and operation, and it is
one of the major problems to be addressed in current AI
applications. Data quality risk refers to flaws that may
exist in training data sets and field data collected for AI,
as well as the corresponding adverse effects. This is also
a type of data risk specific to AI. Data security risk refers
to the security protection of data held by enterprises in
·
·
·
Technology and human activities today are
deeply integrated. We need ethics and
humanism to promote the healthy development of science and technology that is beneficial to mankind. Therefore, we attach great
importance to AI ethics and governance.
- Dr. Xu Li, Executive Chairman of the Board
 and CEO of SenseTime
III.
AI Ethics for “Balanced Development”
SENSETIME I AI FOR A BETTER TOMORROW
13 14
Based on our own experience and observation of the global
AI governance process, we consider AI governance as a
dynamic process driven by value, supported by technical
tools, implemented with collaboration, and achieved through
hierarchical progression. The hierarchy of AI governance
comprises four layers: Functional, Reliable, Controllable and
Trustworthy, covering the functionality, security and robustness, controllability and ethical requirements of AI governance.
Functional means that an AI system can satisfy the application requirements in terms of function and performance.
Reliable means that an AI system can satisfy the requirements of the deployment environment and sustainable
operation in terms of security and robustness.
Controllable means that an AI system can adequately
protect the independent will and rights of human beings
and guarantee a human’s control over the system on the
functional level.
Trustworthy means that the design and application of an
AI system that conforms to human values.
·
·
·
·
Figure 3: Hierarchy of AI Governance
Source: SenseTime
√ the design and application of an AI system that conforms to human values
√ an AI system can satisfy application requirements in terms of function and performance
√ an AI system can adequately protect the independent will and rights of human beings and guarantee
 a human’s control over the system on the functional level
√ an AI system can satisfy the requirements of the deployment environment and sustainable
 operation in terms of security and robustness III.
AI Ethics for “Balanced Development”
SENSETIME I AI FOR A BETTER TOMORROW
Trustworthy
Controllable
Reliable
Functional
SENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW
15 16
To embed the values in our products, we have further
diffused the principles into “ethics by design” standards.
These high-level standards are as follows:
Respect for human rights. Human freedom and dignity
must be protected along with other basic rights, as
globally recognized ethics standards are upheld, human
development and life experience can be enhanced
without harming human status.
For good. Sustainable development for humans must be
protected, just as the interests of vulnerable groups are
protected. Adhere to the basic concepts of human
ethics and morality, and the applications should be
reasonable, legal, and compliant.
Free from bias. The data used should overall be objective, neutral, and representative and balance universal
applicability with the needs of special populations.
Protection of privacy. The collection and usage of
personal information should adhere to the principle of
Figure 4: Three Core Ethical Principles of Responsible AI
Source: SenseTime
Given the state of its development and commercialization,
AI technology and its applications are still in the early
stages, and AI-related economic form, industry ecosystem,
and business model are still in the exploratory stages. Like
other general-purpose technologies that have emerged
throughout the course of history, the healthy and sustainable development of AI technology and relevant industries
not only requires an innovative space that is consistent with
the current state of its development, but also needs appropriate guidance and guardrails. Hence, we believe that AI
development and governance should go hand-in-hand,
their functions and relationships are like that of a nut and
bolt, complementing each other and are indispensable.
Based on our understanding of AI development and governance, we unveiled the “AI Ethics for Balanced Development” Report in 2021, to further crystalize SenseTime’s
three core ethical principles of responsible AI: sustainability,
human-centric approach, and controllable technology.
Specifically, the concept of “balanced development” advocates AI development should be complemented with governance and promotes the healthy and sustainable development of the AI industry, as well as the digital transformation
of the overall economy and society through AI governance.
“Human-centric” advocates respecting, accommodating, and balancing differences in historical, cultural,
AI development and governance should go
hand-in-hand. If governance is applied
prematurely, it may limit AI development.
However, if it lags behind, the consequences
could be catastrophic and the cost of reparative governance will be high.
- Dr. Xu Li, Executive Chairman of the Board
 and CEO of SenseTime
social, and economic development among different
countries and regions, and pursuing consensus among
different cultures. Meanwhile, we should also ensure the
protection of human rights and privacy and deploy
technology without prejudice.
“Controllable technology” advocates that AI is developed by and for humans and therefore, should be
controlled by humans. Correspondingly, its controllers,
i.e., humans, should be responsible for its actions.
“Sustainability” advocates the sustainable development
of society, economy, culture, and the environment, and
promotes openness and inclusive innovation.
·
·
·
·
·
·
·
·
·
·
·
·
·
·
data minimization. In particular, the processing of sensitive personal information must obtain the consent of
the information subject or specific circumstances
prescribed by law.
Reliable and controllable. Within a certain period of
time, under certain conditions, specific functions can be
implemented without failure. Even in the case of failure,
effective shutdown and human takeovers can be implemented.
Transparent and explainable. Priority should be given to
algorithmic models that can be explained. Users should
be provided with clear, understandable, and satisfactory descriptions of the product operating mechanism,
and the limitations and potential risks of the product
should be presented clearly.
Verifiable. It should be possible to verify the algorithmic
model and its results repeatedly under the same or
similar conditions.
Accountable. The rights and obligations of the subjects
in R&D, design, manufacturing, operation, and service
should be clearly defined, and relevant mechanisms
should be available to trace the models and data
behind the output results.
01
02 03
Human-centric
Human-centric
Sustainability
Sustainability Controllable
Technology
Controllable
Technology
Pursue ethical consensus between different cultures
Respect, tolerate and balance the historical, cultural, social and
economic development differences of different countries and
regions around the world
Emphasis on human rights, privacy protection and unbiased
application of Technology
·
Advance public awareness of the
benefits and potential risks of
artificial intelligence technology
Comply with applicable laws and
regulations
Ensure human control over
technology
·
·
·
·
·
Promote the sustainable development of social
economy, culture and environment
Open and inclusive cooperation
Actively explore the application of innovative and
sustainable artificial intelligence governance model
18
·
·
·
·
·
·
·
Trust must be the core of the business community and the
basic premise for the widespread acceptance of emerging
technologies. SenseTime, as an innovative scientific enterprise in the field of AI, has always regarded the trust of the
market and users as the key to its development. Since
SenseTime was founded, all our actions have been guided
by responsible AI development and deployment. At the
same time, we believe that responsible AI is not only a
principle, but also concrete and implementable. The key to
achieving this goal is to build a holistic AI governance
system.
After exploring AI governance in-depth, we have realized
that the construction of an AI governance system should
not be simply verbal or a quote on paper, but should also be
traceable and well-documented. Therefore, for the first
With the rise of new technologies, the ethics
of science and technology faces many new
topics that require joint research involving
every part of society. As an industry leader,
SenseTime has the responsibility to adhere to
high standards of AI ethics.
- Zhang Wang, Vice President of SenseTime,
 Chairman of the AI Ethics and Governance
 Committee.
The boundaries and core requirement of
managing AI ethics risks should be the development of responsible AI. In terms of governance and compliance, enterprises should
think and act ahead.
- Yang Fan, Co-founder and Vice President of
 SenseTime, Member of the AI Ethics and
 Governance Committee.
time in the industry, we propose the development of
“responsible and verifiable” AI as our vision for AI governance. Specifically, “responsible and verifiable” AI should
meet the following basic requirements:
Responsible for people. AI systems should respect and
protect the dignity and rights of people and contribute
to the health and well-being of people.
Responsible for society. AI systems should respect and
adapt to the customs and habits of different cultures
and contribute to the healthy and sustainable development of society.
Responsible for the environment. The development of AI
systems should be mindful of its environmental impact,
and its use should be beneficial to the sustainable
development of the environment.
Accountability for responsible parties. Responsible
parties should be clearly defined for the entire life cycle
and all modules of every AI system, so as to ensure
accountability.
Risks evaluated. AI systems should go through adequate
ethics risk assessments before going online.
Verifiable governance process. The entire life cycle of AI
systems and relevant governance processes should
have complete technical logs and documentation.
Verifiable governance results. The implementation of AI
governance standards over the entire life cycle of AI
systems should be supported by technical and administrative tools.
IV.
Responsible and Verifiable AI
SENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW
17
V.
How We Implement AI Governance
SENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW
19 20
(1) Organization Innovation
SenseTime is one of the first companies in the industry to
establish an AI Ethics and Governance Committee and
make AI governance a strategic priority.
To systematically respond to the ethics risks of AI at different levels, such as data, algorithms, and applications,
SenseTime officially established the AI Ethics and Governance Committee in January 2020 to develop an AI ethics
governance system. The AI Ethics and Governance Committee is comprised of two external members and four internal
members, who come from technical, engineering, legal,
ethics, and related professional backgrounds. The Committee is also supported by a secretariat, an expert advisory
group, and an executive working group to ensure the
independence, transparency, professionalism, and effectiveness of ethics governance. In addition, to ensure the
efficient operations of the AI Ethics and Governance Committee and to strengthen the compliance of ethics
standards by all employees, SenseTime has published the AI
Ethics and Governance Committee Management Charter,
Ethics and Governance Policy, Guidelines for Ethics risk
Review and other ethics-related corporate policies.
Daily Work Responsibilities
Important Work Content
Independent opinions on major issues of the company and
Al Ethics Committee (see below for details).
Formulation of
ethical strategic
development plans.
Product ethics
review and risk
control.
General ethical
knowledge
training and
promotion.
Ecological
construction of
ethical
governance.
Participation in
discussion and
formulation of
ethical standards.
Extensive joint
research.
Decision-making on major
issues of corporate ethical
strategy.
Figure 6: Responsibilities of the AI Ethics and
Governance Committee
Source: SenseTime
Figure 5：Core Requirements for Responsible
and Verifiable AI
Source: SenseTime
Setting and adjusting
development plan of AI
Ethics Committee.
Nominating, appointing,
and dismissing expert
consultants of Al Ethics
Committee.
Other matters stipulated by relevant laws,
administrative regulations, departmental rules,
normative documents, and articles of association.
Responsible
for People
Responsible
for Society
Verifiable
Governance
Process
Verifiable
Governance
Results
Responsible
for the
Environment
Responsible
and Verifiable
AI
Accountability
for
Responsible
Parties
Risk
Evaluated
21 22
SENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW
At the same time, to strengthen AI governance at all levels,
we have systematically enhanced the coordination among
internal organization structures and workflows and provided the Information Security Management Committee with
the ability to conduct privacy protection assessment and
algorithm security assessment.
(2) Mechanism Innovation
In response to ethics risks at the data, algorithm, and application levels, SenseTime has established risk control mechanisms covering the entire life cycle of our products, and
begun to see a close-looped AI governance system forming.
At the data level, we have established a Privacy Protection
Assessment Mechanism, and a Data Security and Personal
Information Protection Committee to conduct the assessment covering the acquisition, storage, transmission, and
processing of personal information, so as to ensure that our
products comply with the design requirements for “privacy
protection by default”.
At the algorithm level, we have established an Algorithm
Security Assessment Mechanism, and an Algorithm Security
Management Working Group. The task of the working
group is to classify and manage the algorithms according to
their data type, business scenario, ethics risk level, data
quality, storage status, application scale, data importance,
intervention degree on the user’s behavior, and to conduct
security assessments on algorithm risks arising from technical limitations, algorithm design, software defects, data
security, framework security, and other related dimensions.
At the application level, we have established an Ethics Risk
Classification Management Mechanism and an Ethics Risk
Review Team to carry out graded and targeted ethics risk
management throughout the entire product life cycle of
design, development, deployment, and operation. We have
also set up supporting processes for self-inspection,
assessment and review of risks, and follow-up reviews. We
classify ethics risks from low to high, spanning five levels
from E0 to E4, based on the impact of final product safety,
personal rights and interests, market fairness, public safety,
and environmental health:
E4 products: prohibited products. These refer to AI
products that deviate from SenseTime’s ethics principles and violate the requirements of laws and regulations.
E3 products: high-risk products. These refer to products
directly related to the final product’s safety, personal
rights and interests, market fairness, public safety, and
environmental health.
E2 products: medium-risk products. These refer to
products that have indirect or potentially high impact
on the final product’s safety, personal rights and interests, market fairness, public safety, and environmental
health.
E1 products: low-risk products. These refer to products
that have no obvious impact on the final product’s
safety, personal rights and interests, market fairness,
public safety, and ecological security.
E0 products: risk-free products. These refer to products
that exclude machine learning algorithms and AI
functions.
In addition, to ensure the effective implementation of AI
governance systems and to promote an ethic-respecting
culture in a corporate setting, we have also established
Ethics Risk Management Goal-setting Mechanism, Ethics
Incidents Reporting and Mitigation Mechanism, and Ethics
Governance Quality Control Mechanism.
(3) Tools Development
To promote the development of responsible and verifiable
AI, we have developed a series of internal management
tools and technical tools covering data governance,
algorithm evaluation, model examination, and ethics review.
At the data level, we have developed a unified data governance platform and standardized data collection processes
to ensure accuracy, balance, and rationality. Besides, with a
data and privacy protection platform, we realized the privacy encryption of data, thus ensuring complete data
availability, reliability, and security. At the same time, we
have designed a set of personal information protection
assessment checklists for the whole process of product
development, promoted functional product design oriented to personal information protection, ensured the design
process of AI products, and limited the collection and
processing (including usage, disclosure, retention, transmission, and disposal) to clearly defined and necessary
purposes.
In addition, in the process of data processing, by developing and deploying automatic labeling tools, we reduce the
amount of data contacted manually and the risk of introducing human bias at the source of model training. Moreover, the data labeling platform has access control and
authentication functions and can only be accessed by
certified data labeling personnel.
Figure 7: SenseTime’s Standard for Ethics Risk Classification
Source: SenseTime
·
·
·
·
·
·
Prohibited products. These refer to AI products that deviate from SenseTime's ethics principles and
violate the requirements of laws and regulations.
·
High-risk products. Products that have direct impact on the final product’s safety, personal rights
and interests, market fairness, public safety, and environmental health.
·
Medium-risk products. Products that have indirect or potentially high impacts on the final product’s
safety, personal rights and interests, market fairness, public safety, and environmental health.
·
Low-risk products. Products that have no obvious impact on the final product’s safety,
personal rights and interests, market fairness, public safety, and environmental health.
·
Risk-free products. Products that don’t have machine learning algorithms
or AI functions.
·
E4
E3
E2
E1
E0
23
Figure 8: External Recognition of SenseTime’s AI Governance Practices
Source: SenseTime
24
At present, we have received a number of internationally
recognized certifications for network and data security,
including The Information Security Management System
Certificate (ISO/IEC 27001�2013), The Privacy Information
Management System (PIMS) Certificate (ISO/IEC
27701�2019), The Code of Practice for Personally Identifiable Information Protection (ISO/IEC 29151�2017), and The
Personal Information Security Management System Certificate (BS10012). Products sold have also obtained Level 3
Certification for Important Information System Classified
Protection, and The Trusted Face Certification Special Test
Certificate among others.
At the algorithm level, the black box algorithm poses a
significant risk of discrediting the algorithm and hindering
its explanation. When designing a model, we output various
types of information in the code to enable fast traceability
in case of an algorithm decision error.
At the same time, by establishing a model inspection
platform to test the model for inference attacks and reverse
attacks, we can check and score the test factors of the
algorithm model against digital world white-box confrontation, digital world black-box query attack robust accuracy, digital world migration attack robust accuracy, physical
world sample attack success rate, and model backdoor
attack success rate among others to determine whether the
algorithm model meets the design requirements. When the
algorithm model does not meet the design requirements,
we enable the corresponding algorithm repair module to
improve security. At the same time, we build an AI firewall at
the system level to defend against attacks from adversarial
examples. When the model is released, tests are done on
the test set defined by the product, and manual testing is
conducted to ensure that the scale of the test set is large
enough and can reach the required accuracy level.
In addition, based on the algorithm verification and evaluation of data sets in real scenarios, we have developed an
algorithm evaluation tool. It is capable of fully evaluating
algorithms through full coverage of the algorithm evaluation in main scenarios and long-tail scenarios, diversified
evaluation items and rich index systems, as well as sufficient
data sets and comprehensive evaluation schemes, for
credibility and control of all commercial algorithms.
At the application level, we have designed a set of ethics
risk self-examination tools and a review platform in
conjunction with the different stages of review. At present,
all SenseTime’s AI products must undergo ethics risk
reviews through the review platform at different stages
from project approval and release to online operation.
Before the review process, self-inspection tools can be
used to prepare for the review. During the review process,
we may choose to reject new product proposals, suspend
the ongoing product development projects, or withdraw
existing products that do not meet our principles and
standards.
Due to the above-mentioned practices, our AI ethics and
governance system and related technical tools have been
recognized by various third-party organizations, such as the
Harvard Business Review and Artificial Intelligence Industry
Alliance (AIIA). Our first White Paper on AI Sustainable
Development was also included in the United Nations’
Resource Guide on Artificial Intelligence Strategies.
SENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW
VI.
“Ethics by Design” in Action
Case Study:The SenseRobot
 — An AI Chinese Chess Robot
25 26
(4) Fostering Culture
We realize that the key to developing responsible and verifiable AI is to cultivate an organizational culture for AI governance.
With the release of Ethics and Governance Policy, SenseTime AI Ethics and Governance Committee Management
Charter, and Guidelines for Ethics Risk Review, we provide
a set of guidelines and clearly-defined standards for aligning employee’ understanding and actions on AI ethics and
governance. At the same time, to better acquaint employees with AI governance topics, we have introduced regular
trainings and community engagement platforms. We send
important trends related to AI governance to all employees
weekly and regularly organize seminars, inviting internal
and external experts to provide training on AI ethics and
governance.
(5) Developing the Ecosystem
We actively participate in standard-setting bodies related
to AI ethics and governance such as the National Information Security Standardization Technical Committee and the
Institute of Electrical and Electronics Engineers (IEEE) and
serve as chair or vice chair in several working groups. At the
same time, we have established research cooperation on AI
ethics and governance with well-known universities at
home and abroad, and research institutes such as Tsinghua
University, Shanghai Jiao Tong University, and Artificial
Intelligence International Institute.
We jointly launched the Tech4SDG alliance with industry
partners to promote the steady development of responsible
and verifiable AI. Currently, the Tech4SDG alliance covers 9
countries and regions in Asia, with more than 40 members,
including many well-known universities and think tanks
from mainland China, Hong Kong, Macau, Singapore, India,
and Saudi Arabia, among others.
SENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW
together through fun and games. Through AI deep learning and self-training, SenseRobot’s capabilities are at
expert-level, and there are suitable activities for both
beginners and experienced players. In addition, it also
brings the whole family together to come up with
solutions for the chess challenges, thereby strengthening
the bonding between children, parents, and grandparents. SenseRobot’s aim to stimulate minds and bring the
family together is an example of human-centric design.
SenseRobot has a simple and sleek appearance. It presents
itself as a little “astronaut”, who teaches and plays Chinese
chess with children “face-to-face”.
Dr. Xu Li, Executive Chairman of the Board and CEO of
SenseTime, said at the product launch event, “Our goal is to
create a robot that can physically ‘think’ and ‘act’ with our
leading AI technology, bring industrial grade AI technology
into every family, and make real interactions with children
and elderly. It can not only accompany the whole development period of children, but also make high technology
intuitive, understandable and interesting for elderly. It will
bridge the digital divide and build emotional connection with
technology, while bringing overall enjoyment to the whole
family.”
SenseRobot features AI chess learning and various levels of
challenges, among others. It can introduce and explain
Chinese chess culture, rules and skills of each chess piece to
children without previous experience. While training the
children, it can also improve their cultural literacy. In addition,
it also contains more than 100 endgames and 26 levels of
chess competition, so that users not only experience playing
with actual Chinese chess pieces, but also enjoy the mental
stimulation at various difficulty levels.
With SenseTime’s leading AI technology, the SenseRobot has
remarkable coordination and can achieve millimeter-level of
operation accuracy to ensure the game runs smoothly.
Furthermore, it has been certified and authorized by the
Chess and Card Management Center of the General Administration of Sport and the Chinese Xiangqi Association, so
users can be assessed for levels 16 – 13 in the official
Chinese chess level examinations.
Based on feedback from users of the first batch of trials, they
said that SenseRobot is a product that brings the family
SENSETIME I AI FOR A BETTER TOMORROW
27 28
As mobile phones and tablets become increasingly dominant in our lives, many parents hope to see their children
spend less time on their electronic devices. On August
9th, 2022, SenseTime officially launched its first household
consumer AI product, SenseRobot, the AI Chinese chess
robot. Integrated with SenseTime’s leading AI technology
and mechanical arm technology, the SenseRobot is a
physical robot that can be placed at homes and on the
table. Children interacting with SenseRobot need not look
at its electronic screen, and are able to learn and play
Chinese chess without straining their vision.
SENSETIME I AI FOR A BETTER TOMORROW
Figure 9: SenseRobot Application Scenario",Blog: https://www.sensetime.com/en/ethics
32,Babylon,United Kingdom,Software,,,https://www.babylonhealth.com/en-gb/responsibility/clinical-ai-governance,Clinical AI Governance,"Clinical AI Governance

“Our Clinical Artificial Intelligence Governance is formed of seven pillars that merge statutory, regulatory and professional best-practice to ensure our AI helps make it easier for our users to take care of themselves and their loved ones.”

Dr Keith Grimes, Clinical AI Director

At Babylon we believe everyone, everywhere, has the right to a long, healthy and happy life. With medical professionals in short supply we are using artificial intelligence (AI) to amplify the impact of their work.

We use a robust system of governance to transparently measure and improve the AI and ensure it is used as appropriately as possible. We also adhere to the United Kingdom’s code of conduct for artificial intelligence (AI) systems used by the NHS.

Our approach to AI Governance follows the NHS guidance of:



Quality Audit

We have a cyclical process of improving the quality of our AI. We review the performance of our AI, compare it against standards and refine it as a result. These standards may be internally set, based on historical performance, developed through expert consultation and adaptation of existing clinical standards, or come from the growing range of national and international standards of best practice.



Effectiveness

We apply knowledge derived from our Scientific Research team, external research, clinical experience and user studies, to keep improving outcomes for users whilst optimising processes. We monitor practice, inform our teams and implement changes.



Education and Training

We educate and continually train not only the staff who develop and test the AI, but also the AI itself. This helps ensure the highest quality of clinical data and knowledge is used in the development of all our AI, and that every effort is made to ensure that conscious and unconscious bias is minimised.



Research and Development

Our Scientific Research team publishes peer-reviewed research and works with researchers from multiple universities to ensure best research practice. We are working with the World Health Organisation and others to establish a standardised benchmark for symptom checkers.



Risk Management

Our robust methodology assesses and manages risk to users, staff and the healthcare system, identifying and responding to incidents, complying with safety standards, and operating to the highest ethical standards. This includes management of any internal or external reports received. In assessing and managing risk, we comply with the NHS’ DCB0129 and DCB0160 Clinical Risk Management Standards in the UK, adhere to the National Institute for Health and Care Excellence (NICE) Guidance on the ‘Evidence Standards Framework for Digital Health Technologies’, utilise the NHS Clinical Risk Matrix throughout our testing and assurance processes, and globally apply EN ISO 14971:2012 risk management to our medical devices.



Information Management

We comply with applicable data protection laws (including the GDPR) and follow best practice in the ethical, secure, and legal use of patient data in the delivery of healthcare, whilst ensuring that the full value of this important resource is realised for the greatest benefit of the patients.



Openness

We engage with the global community of clinicians, our users and the wider public and welcome both feedback and compliments! We use extensive user research to guide changes, our users can feedback instantly using a real-time star rating system and we have multiple easy ways of getting in contact. All our staff can raise any concerns and we work closely with trusted bodies across the world including: World Health Organisation, NHSX, NHS Digital and leading academics in the US, UK and China.",
33,DataRobot,United States of America,Software,,,https://www.datarobot.com/platform/trusted-ai/,DataRobot’s Framework for Trusted AI,"DataRobot’s Framework for Trusted AI
DataRobot’s technology was created by some of the world’s top data scientists, incorporating best practices with recent research developments. We recognize that AI trust is multidimensional and an AI creator, operator, and consumer each have different needs. Success happens when humans and AI work together.
Performance
The DataRobot platform incorporates guardrails to ensure performance and enable the democratization of AI so that knowledgeable business users across an organization can rely on it.
Value and Risk
With DataRobot’s Humble AI feature, you get real-time analysis and protection for the predictions generated by any of your deployed models, making it easier to trust your model and get more value from it.
Ethical and Explainable
Technical and non-technical users must be able to explain AI models in a way that is transparent and reflects your company’s values. DataRobot’s platform is not a black box and will give you clear and concise explanations for the predictions it produces.",Article: https://www.datarobot.com/trusted-ai-101/
34,Argo AI,United States of America,Software,04-01-2022,04-01-2022,https://www.argo.ai/supplier-code-of-conduct/,Innovation,"Innovation
Quality, Continuous Improvement

At Argo, we hold ourselves to incredibly high quality standards and, in turn, expect Suppliers to provide the highest quality products and services. Argo also expects Suppliers to demonstrate their commitment to continuous improvement by providing feedback and input into opportunities for the products or services that they provide to Argo to ensure the highest quality is achieved.
Embracing Change and Technology

Argo recognizes that technology and business priorities change rapidly. We expect our Suppliers to be responsive and adaptive to these changes as applicable.",Report: https://www.argo.ai/wp-content/uploads/2021/04/ArgoSafetyReport.pdf
35,BMW Group,Germany,Automotive,10-12-2020,10-12-2020,https://www.bmwgroup.com/content/dam/grpw/websites/bmwgroup_com/downloads/ENG_PR_CodeOfEthicsForAI_Short.pdf,BMW Group code of ethics for artificial intelligence. ,"Corporate Communications
Press release
12 October 2020
BMW Group code of ethics for artificial intelligence.
Munich. The use of artificial intelligence (AI) is a central element of the digital
transformation process at the BMW Group. The BMW Group already uses AI throughout
the value chain to generate added value for customers, products, employees and
processes.
Building on the fundamental requirements formulated by the EU for trustworthy AI, the
BMW Group has worked out seven basic principles covering the use of AI within the
company. These will be continuously refined and adapted as required according to the
multi-layered application of AI across all areas of the company. In this way, the BMW
Group will pave the way for extending the use of AI and increase awareness among its
employees of the need for sensitivity when working with AI technologies.
Company
Bayerische
Motoren Werke
Aktiengesellschaft
Postal address
BMW AG
80788 München
Telephone
+49 89 382 60340
Internet:
www.bmwgroup.com
Press release
Date 12 October 2020
Subject BMW Group code of ethics for artificial intelligence.
Page 2
Corporate Communications
Seven principles covering the development and application of artificial
intelligence at the BMW Group:
• Human agency and oversight.
The BMW Group implements appropriate human monitoring of decisions made by
AI applications and considers possible ways that humans can overrule algorithmic
decisions.
• Technical robustness and safety.
The BMW Group aims to develop robust AI applications and observes the applicable
safety standards designed to decrease the risk of unintended consequences and
errors.
• Privacy and data governance.
The BMW Group extends its state-of-the-art data privacy and data security
measures to cover storage and processing in AI applications.
• Transparency.
The BMW Group aims for explainability of AI applications and open communication
where respective technologies are used.
• Diversity, non-discrimination and fairness.
The BMW Group respects human dignity and therefore sets out to build fair AI
applications. This includes preventing non-compliance by AI applications.
• Environmental and societal well-being.
The BMW Group is committed to developing and using AI applications that promote
the well-being of customers, employees and partners. This aligns with the BMW
Group’s goals in the areas of human rights and sustainability, which includes climate
change and environmental protection.
Press release
Date 12 October 2020
Subject BMW Group code of ethics for artificial intelligence.
Page 3
Corporate Communications
• Accountability.
The BMW Group’s AI applications should be implemented so they work responsibly.
The BMW Group will identify, assess, report and mitigate risks, in accordance with
good corporate governance.
Overall centre of competence for the company: “Project AI”.
“Project AI” was launched in 2018 to ensure that AI technologies are used ethically and
efficiently. As the BMW Group’s centre of competence for data analytics and machine
learning, it ensures rapid knowledge and technology sharing across the company.
Project AI therefore plays a key role in the ongoing process of digital transformation at
the BMW Group and supports the efficient development and scaling of smart data and
AI technologies. One of the developments to come out of Project AI is a portfolio tool
which creates transparency in the company-wide application of technologies making
data-driven decisions. This D³ (Data Driven Decisions) portfolio currently spans 400 use
cases, of which more than 50 are available for regular operation.
In the event of enquiries please contact:
Martin Tholund, Press Spokesperson Research, New Technologies, Innovations
Tel.: +49-89-382-77126, e-mail: martin.tholund@bmwgroup.com
Benjamin Titz, Head of BMW Group Design, Innovation & Motorsport Communications
Tel.: +49 (0)179 – 743 80 88, e-mail: benjamin.titz@bmw.de
The BMW Group
With its four brands BMW, MINI, Rolls-Royce and BMW Motorrad, the BMW Group is the world’s leading
premium manufacturer of automobiles and motorcycles and also provides premium financial and mobility
services. The BMW Group production network comprises 31 production and assembly facilities in 15
countries; the company has a global sales network in more than 140 countries.
In 2019, the BMW Group sold over 2.5 million passenger vehicles and more than 175,000 motorcycles
worldwide. The profit before tax in the financial year 2019 was € 7.118 billion on revenues amounting to
€ 104.210 billion. As of 31 December 2019, the BMW Group had a workforce of 126,016 employees.
The success of the BMW Group has always been based on long-term thinking and responsible action. The
company has therefore established ecological and social sustainability throughout the value chain,
comprehensive product responsibility and a clear commitment to conserving resources as an integral part
of its strategy.
Press release
Date 12 October 2020
Subject BMW Group code of ethics for artificial intelligence.",Article: https://www.press.bmwgroup.com/global/article/detail/T0318411EN/seven-principles-for-ai:-bmw-group-sets-out-code-of-ethics-for-the-use-of-artificial-intelligence?language=en
36,Adarga,United Kingdom,Software,,,https://adarga.ai/article/responsible-ai-at-adarga,Responsible AI at Adarga,"Presenting the Adarga Committee for Responsible AI (ACRAI) which guides Adarga’s approach to the responsible creation, good operation, human-centric governance and oversight of Artificial Intelligence (AI).

Artificial Intelligence (AI) is an important and powerful 21st-century technology, and as AI becomes more prevalent in our everyday lives, so too does the topic of ethics. This responsibility is central to Adarga’s work in enhancing human ingenuity and helping our users to discover the deep and critical insights that drive faster and better decisions. 

At Adarga, we address this head-on within our business: acknowledging, examining and ensuring responsibility, transparency, accountability, and ethics are ever-present in our work as a company at the cutting edge of AI technology. 

The Adarga Committee for Responsible AI (ACRAI) was specifically created to meet this need. ACRAI guides Adarga’s approach in the responsible creation, good operation, and human-centric governance and oversight of AI. 

ACRAI is composed of Adarga team members at all levels and from all departments to ensure equality and equitability in our discussions around responsible AI use. ACRAI supports Adarga employees in the creation of responsible AI software products from idea to release - advising in our use of data and informing business decisions about who we work with. The Committee creates and oversees a framework that informs the direction and purpose of our Knowledge Platform®, guides Adarga as a company in the sometimes contentious field of AI, and through which any member of the Adarga team can question or query what is right.

This starts with our Responsible AI Principles:
 
1.    We retain human oversight and accountability 
2.    We prize transparency and explainability 
3.    We operate reliably, reproducibly and resiliently 
4.    We uphold fairness and non-discrimination 
5.    We have a positive impact across our community

These Principles form the foundation of our Responsible AI Framework, governed by the Committee’s Terms of Reference and a Code of Practice. They support Adarga team members in working with best practices in Responsible AI on a day-to-day level and at the highest business level, to ensure that we have clear accountability process and purpose.

The ACRAI is also actively engaged with the ongoing legal and governance developments around AI. Where legislation and lawmakers may be less agile in responding to the demands and opportunities afforded by this fast-moving technology, this has created a space where companies need to take responsibility for their AI design, development, implementation and usage.  

The repercussions of not doing this is causing concern in the general public, creating an environment where the benefits of AI are tarnished by the discussion of unintended algorithm performance, or around the deliberate misuse of AI like fake news, bad actors or bots. Simultaneously, there are fundamental questions to be asked of AI around trust, ethics, and understandability. The ACRAI is Adarga’s mechanism by which we can ask these same questions of our AI technology and how Adarga’s work intersects with these critical issues in order to establish how best to govern this complex environment.

Through the ongoing work of ACRAI and the values of Adarga as a company, we are continuing to drive forward the responsible and transparent use of AI. Our platform supports the human user to get to the answers they need to empower their decision-making. Our AI software does not seek to replace the human user, but to work with human teams of analysts to make data manageable and information more accessible to them. 

Responsible AI, ethics and transparency are becoming ever more vital, in parallel to the increasing presence of AI in our lives, work, homes and culture. Like shifting the tiller of a boat, any changes to our machine learning product design must take into consideration the wider possible impact. We believe in foregrounding Responsible AI and supporting our data scientists and engineers in making these decisions in a conscientious way to meet the challenges that this new technology necessitates, and the new responsibilities that are now at the feet of companies like Adarga working in AI. 

""Embedding these Responsible AI principles in our ways of working, processes and business ethos is fundamental as we continue to develop and deploy state-of-the-art AI capability to our customers.""",Article: https://www.adarga.ai/article/responsible-ai-and-the-future-of-ai-regulation
37,Dataiku,United States of America,Software,,,https://www.dataiku.com/corporate-social-responsibility-policy/,Treating Customers Fairly and Fostering AI for Good,"Treating Customers Fairly and Fostering AI for Good
We seek to maintain the highest level of professional and ethical standards when conducting business. We observe an internal Anti-Bribery and Corruption Policy and handle personal information in accordance with our internal and external Privacy policies.

We strongly believe that AI has an essential role to play in creating a more sustainable economic world. We encourage the responsible use of AI by providing the appropriate tooling, training, advisory, and incentive to all customers and employees.
Creating a Socially and Environmentally Responsible Supply Chain
In our procurement operations, we ensure a spirit of fairness, impartiality, respect for laws, and conformity with social ethics. We are committed to promoting diversity in our professional dealings with suppliers. We believe establishing a relationship of trust and collaboration in Corporate Social Responsibility (CSR) strategy will accelerate both our suppliers’ and our own sustainability action’s impact.",Blog: https://blog.dataiku.com/3-steps-toward-more-ethical-ai
38,AT&T,United States of America,Telecommunication,05-15-2019,05-15-2019,https://about.att.com/innovationblog/2019/05/our_guiding_principles.html,Artificial Intelligence at AT&T: Our Guiding Principles,"1. By People, For People

We incorporate human oversight into AI. With people at the core, AI can enhance the workforce, expand capability and benefit society as a whole.

2.       Accessible and Shared

We support open-source communities whenever appropriate to further access, collaboration, standardization and participation in industry discussion.

3.       Secure and Ethical

We are grounded in ethics, safety, and values at every stage of AI, including our privacy principles and security safeguards.

­   Design: We use varied, validated datasets and diverse human input to achieve objectives.
­   Development: We use a transparent approach to algorithms that includes safeguards.
­   Deployment: We monitor outcomes to ensure accuracy and help minimize biases.
These may seem simple and common sense. But as AI leaders, we believe their importance can’t be overstated. These principles will guide our decisions from design to deployment.

Here’s a hypothetical example. AI can make our network more efficient. It reduces power needs and physical duplication by offering fast and smart routing decisions. But what if a new algorithm to skirt outages kept favoring urban customers at the expense of rural ones? Not on purpose – but as an unintended consequence.

No organization will be perfect, but that’s what humans must try to anticipate, catch and repair. The people who run AI decide on the data used, the goals set, the algorithms deployed, and the outcomes monitored.",Human Rights Policy: https://about.att.com/ecms/dam/csr/PDFs/Human_Rights_Policy.pdf
39,Deutsche Telekom,Germany,Telecommunication,05-11-2018,05-11-2018,https://www.telekom.com/resource/blob/532446/f32ea4f5726ff3ed3902e97dd945fa14/dl-180710-ki-leitlinien-en-data.pdf,AI Guidelines,"AI Guidelines
Deutsche Telekom
Preamble
Two of Deutsche Telekom’s most important goals are to keep being a trusted companion and to enhance customer experience.
We see it as our responsibility - as one of the leading ICT companies in Europe - to foster the development of “intelligent technologies”. At least either important, these technologies, such as AI,
must follow predefined ethical rules.
To define a corresponding ethical framework, firstly it needs a common understanding on what AI means. Today there are several definitions of AI, like the very first one of John McCarthy (1956)
“Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.” In line with other companies and main players
in the field of AI we at DT think of AI as the imitation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, and self-correction.
After several decades, Artificial Intelligence has become one of the most intriguing topics of today – and the future. It has become widespread available and is discussed not only among experts
but also more and more in public, politics, etc.. AI has started to influence business (new market opportunities as well as efficiency driver), society (e.g. broad discussion about autonomously
driving vehicles or AI as “job machine” vs. “job killer”) and the life of each individual (AI already found its way into the living room, e.g. with voice steered digital assistants like smart speakers).
But the use of AI and its possibilities confront us not only with fast developing technologies but as well as with the fact that our ethical roadmaps, based on human-human interactions, might not
be sufficient in this new era of technological influence. New questions arise and situations that were not imaginable in our daily lives then emerge.
We as DT also want to develop and make use of AI. This technology can bring many benefits based on improving customer experience or simplicity. We are already in the game, e.g having
several AI-related projects running. With these comes an increase of digital responsibility on our side to ensure that AI is utilized in an ethical manner. So we as DT have to give answers to our
customers, shareholders and stakeholders.
The following Digital Ethics guidelines state how we as Deutsche Telekom want to build the future with AI. For us, technology serves one main purpose: It must act supportingly. Thus AI is in any
case supposed to extend and complement human abilities rather than lessen them.
Remark: The impact of AI on DT jobs – may it as a benefit and for value creation in the sense of job enrichment and enlargement or may it in the sense of efficiency - is however not focus of
these guidelines. 
1. We are responsible.
The human always remains responsible. Our solutions come with a clear definition of who is
responsible for which AI system or feature. We are in charge of our products and services. And,
we know who is in charge for partner or third party solutions.
With AI technology being in its infancy, we are aware of our responsibility in development – from the very beginning. We make
sure that we clarify which initiative or product owner has which responsibilities. For partners or third parties, we define clear
guidelines for when a partnership can be established. And, we declare which duties are connected to the respective AI parts. 
2. We care.
We act in tune with our company values. Our systems and solutions must subordinate to humandefined rules and laws. Therefore, in addition to our technical requirements, our systems and
solutions have to obey the rules and laws that we as Deutsche Telekom, our employees – and
human beings as such – follow.
AI systems have to meet the same high technical requirements as any other IT system of ours, such as security, robustness, etc.
But since AI will be (and already is) a great part of our everyday lives, even guiding us in several areas, AI systems and their
usage also have to comply with our company values (Deutsche Telekom’s Guiding Principles and Code of Conduct), ethical
values, and societal conventions. We have to make sure of that. 
3. We put our customers first.
We enrich and simplify our customers’ lives. If an AI system or the usage of customer-related
data helps us to benefit our customers, we embrace this opportunity to meet their demands and
expectations.
The aggregation and use of customer data – especially in AI systems – shall always be clear and serve a useful purpose towards
our customers. Systems and processes that support in the background are as important as services that interact with our
customers directly.
4. We are transparent.
In no case we hide it when the customer’s counterpart is an AI. And, we are transparent about
how we use customer data. As Deutsche Telekom, we always have the customer’s trust in mind
– trust is what we stand for.
We are acting openly to our customers. It is obvious to our customers that they are interacting with an AI when they do. In
addition, we make clear, how and to which extent they can choose the way of further processing their personal data. 
5. We are secure.
Data security is a prime quality of Deutsche Telekom. In order to maintain this asset, we ensure
that our security measures are up to date while having a full overview of how customer related
data is used and who has access to which kind of data.
We never process privacy-relevant data without legal permission. This policy applies to our AI systems just as much as it does to
all of our activities. Additionally, we limit the usage to appropriate use cases and thoroughly secure our systems to obstruct
external access and ensure data privacy.
6. We set the framework.
Our AI solutions are developed and enhanced on grounds of deep analysis and evaluation. They
are transparent, auditable, fair, and fully documented. We consciously initiate the AI’s
development for the best possible outcome.
The essential paradigm for our AI systems’ impact analysis is “privacy und security by design”. This is accompanied e.g. by risks
and chances scenarios or reliable disaster scenarios. We take great care in the initial algorithm of our own AI solutions to prevent
so called “Black Boxes” and to make sure that our systems shall not unintentionally harm the users. 
7. We maintain control.
We are able to deactivate and stop AI systems at any time (kill switch). Additionally, we remove
inappropriate data to avoid bias. We have an eye on the decisions made and the information fed
to the system in order to enhance decision quality.
We take responsibility for a diverse and appropriate data input. In case of inconsistencies, we rather stop the AI system than
pursue with potentially manipulated data. We are also able to “reset” our AI systems in order to remove false or biased data. By
this, we install a lever to reduce (unintended) unsuitable decisions or actions to a minimum. 
8. We foster the cooperative model.
We believe that human and machine intelligence are complementary, with each bringing its own
strength to the table. While we believe in a people first approach of human-machine
collaboration, we recognize, that humans can benefit from the strength of AI to unfold a potential
that neither human or machine can unlock on its own.
We recognize the widespread fear, that AI enabled machines will outsmart the human intelligence. We as Deutsche Telekom
think differently. We know and believe in the human strengths like inspiration, intuition, sense making and empathy. But we also
recognize the strengths of AI like data recall, processing speed and analysis. By combining both, AI systems will help humans to
make better decisions and accomplish objectives more effective and efficient. 
9. We share and enlighten.
We acknowledge the transformative power of AI for our society. We will support people and
society in preparing for this future world. We live our digital responsibility by sharing our
knowledge, pointing out the opportunities of the new technology without neglecting its risks. We
will engage with our customers, other companies, policy makers, education institutions and all
other stakeholders to ensure we understand their concerns and needs and can setup the right
safeguards. We will engage in AI and ethics education. Hereby preparing ourselves, our
colleagues and our fellow human beings for the new tasks ahead.
Many tasks that are being executed by humans now will be automated in the future. This leads to a shift in the demand of skills. Jobs will be
reshaped, rather replaced by AI. While this seems certain, the minority knows what exactly AI technology is capable of achieving. Prejudice and
sciolism lead to either demonization of progress or to blind acknowledgment, both calling for educational work. We as Deutsche Telekom feel
responsible to enlighten people and help society to deal with the digital shift, so that new appropriate skills can be developed and new jobs can be
taken over. And we start from within – by enabling our colleagues and employees. But we are aware that this task cannot be solved by one
company alone. Therefore we will engage in partnerships with other companies, offer our know-how to policy makers and education providers to
jointly tackle the challenges ahead. ",Article: https://www.telekom.com/en/company/digital-responsibility/details/artificial-intelligence-ai-guideline-524366
40,Telefónica,Spain,Telecommunication,,,https://www.telefonica.com/es/wp-content/uploads/sites/4/2021/06/ia-responsible-governance.pdf,Telefónica´s Approach to the Responsible Use of AI,"Telefónica´s Approach to the
Responsible Use of AI
TELEFONICA, S.A.
Introduction
Artificial Intelligence (AI) is on the rise. It can be applied to many different domains such
as content recommendations, chatbots, image recognition, machine translation, fraud
detection, medical diagnosis, autonomous vehicles, legal, education, transport, and
logistics to name just a few. It can not only be used for business, but also for social
purposes such as better understanding and reducing the impact of climate change,
natural disasters, and migration. Also, in Telefonica, AI and Big Data are used
increasingly. There are four main areas of application: i) optimization of core business;
ii) innovation in the customer relationship using cognitive technologies for digital
assistants in apps, webs, call centres, shops, etc; iii) offering AI and Big Data services
to business customers through Telefónica’s B2B area; and iv) using AI and Big Data for
social good such as the fight against COVID-19 and contributing to the Sustainable
Development Goals. The last two areas are always based on anonymized and
aggregated data.
However, recently several concerns have been expressed about the use of AI, in
particular related to potential discrimination (bias, discrimination, predictive parity), lack
of interpretability of algorithmic conclusions (explainability, black box problem), and lack
of transparency of personal data used.
To deal with those potential problems, Telefonica published its AI Principles in October
2018. This document describes Telefónica´s Responsible Use of AI, which is a part of
the broader Responsibility by Design-Approach of Telefónica. Given that there is still little
industry experience, we will adapt and adjust the approach based on experience and
new external developments.
Telefónica´s Approach to the Responsible Use of AI
The approach for the responsible use of AI includes:
• a strategic model: the strategic vision on how the responsible use of AI aligns
with wider company objectives
• an organizational and relational model, which defines the functions needed,
aligned with the corporate structure, in addition to the roles & responsibilities, and
relationships
• an operating model defining the relevant processes along with roles responsible
for the tasks to be carried out. This includes the Responsible AI by Design
methodology.
The strategic model: Telefonica’s Principles of
Artificial Intelligence
Telefonica is strongly committed to respecting Human Rights, as is stated in its Business
Principles and Human Rights Policy. This includes a commitment to developing products
and services aimed at making the world a better place to live in and mitigating any
negative impacts technology may have on society or the environment. Technology
should contribute to making society more inclusive and offer better opportunities for all,
and AI can contribute to these goals.
In order to guide the organization in its uptake of AI and Big Data across the business,
Telefonica has published its “Principles of AI”. The principles include:
• Fair AI seeks to ensure that the applications of AI technology lead to fair results.
This means that they should not lead to discriminatory impacts on people in
relation to race, ethnic origin, religion, gender, sexual orientation, disability or any
other personal condition. When optimizing a machine learning algorithm, we must
take into account not only the performance in terms of error optimization, but also
the impact of the algorithm in the specific domain.
• Transparent and Explainable AI means to be explicit about the kind of personal
and/or non-personal data the AI systems uses as well as about the purpose the
data is used for. When people directly interact with an AI system, it should be
clear to the users that this is the case. When AI systems take, or support,
decisions, a certain level of understanding of how the conclusions are arrived at
needs to be ensured, by generation explanations about how they reached that
decision, like is illustrated in for the particular case of supervised machine
learning. Those explanations should always consider the user profile to adjust
them to the transparency level required. This also applies in case of using thirdparty AI technology.
• Human-centric AI means that AI should be at the service of society and generate
tangible benefits for people. AI systems should always stay under human control
and be driven by value-based considerations. AI used in products and services
should in no way lead to a negative impact on human rights or the achievement
of the UN’s Sustainable Development Goals.
• Privacy and Security by Design means that when creating AI systems, which
are fueled by data, privacy and security aspects are an inherent part of the
system’s lifecycle. This maximizes respecting people’s right to privacy and their
personal data. Notice that the data used in AI systems can be personal or
anonymous/aggregated. Notice also that this principle is broader applicable than
only to AI systems, and Telefonica already has processes in place to ensure
proper privacy and security.
• Working with partners and third parties means Telefónica is committed to
verifying the logic and data used by the providers to ensure that its principles are
respected.
Those Principles are also based on a broad consensus in expert communities, as well
as on specific aspects of the telecommunications industry.
There is, however, little published experience on how such principles can be practically
implemented in large organizations such that they have the desired effect. In this sense,
Telefonica is making an important step with this approach for the responsible use of
Artificial Intelligence.
The organizational and relationship model
We are implementing responsible AI through an organizational and relationship model
that defines what areas of the company are involved, what their roles are and how they
relate to each other for the achievement of a responsible use of AI. 
We promote a self-responsibility approach with on-demand escalation. There is a 3 step
escalation process as illustrated in the figure below.
Product managers/developers who purchase, develop and/ or use Artificial Intelligence
are to carry out a simple self-assessment of the product/service they are developing
already at the design phase through an online questionnaire. This self-assessment
explicitly covers potential human rights risks associated with the use of Artificial
Intelligence. This self-assessment will be integrated into a 3 tiered governance model,
supported by a broader Community of Experts (among them a single-point-of-contact for
questions relating to AI & Ethics, the Responsible AI Champion). If a product
manager/developer (level 1) has doubts about a potential adverse impact of a given
product/service after completing the self-assessment, and this doubt cannot be resolved
with the help of the RAI, she will be automatically directed to a group of predetermined,
multidisciplinary experts within the company (level 2), that together with the product
manager/developer try to solve the issue at hand. In case this issue turns out be a
potential risk to the company´s reputation, the matter is elevated to the Responsible
Business Office which brings together all relevant department directors at global level
(level 3).
The operating model
The operating model describes the processes of how to implement the Responsible AI
Approach in the organization on a day to day basis. Integrated within the broader
Responsibility-by-Design-Approach, it includes a methodology called “Responsible AI by
Design”, inspired by methodologies such as Privacy and Security by Design. The
operating model consists of, among others:
• Training & awareness activities
• The self-assessment on line questionnaire, where each AI principle is
operationalized through a set of questions to answer along with
recommendations.
• A set of technical tools that helps in answering the questions
The design process of the methodology required a cross-enterprise initiative involving
different departments such as Engineering, Corporate Ethics & Sustainability, Security, 
Legal, Business, Human Resources, Procurement, as well as an endorsement of top
management.
Training & awareness
While Artificial Intelligence is still a rather new technology used in large organizations,
the ethical and societal impacts are even more recent. Therefore, it is of utmost
importance to explain to employees what AI is, how it works and how it might lead to
undesired consequences.
Telefónica has therefore developed courses related to AI & Ethics that are accessible to
all employees through the standard corporate portals in three languages (Spanish,
English and Portuguese). Depending on the profile of the employee he or she can access
a light version of the course which is divided in three modules of each 10 min, or a more
profound version which takes about 1 hour to complete.
The full course is divided into 6 modules as illustrated in the figure below:
Apart from online courses, each business unit using AI is participating in dedicated
workshops explaining the governance model. Moreover, there is a mini-guide available
on the intranet where employees can get a quick overview of how to apply the AI
Principles for ensuring a responsible use of AI.
Questionnaire with recommendations
For all products and services that use AI or Big Data, the responsible manager needs to
complete the self-assessment questionnaire where for each principle, several questions
must be answered. The questionnaire is available online in Spanish and English and
integrated in the global “Responsibility by Design” initiative of Telefónica Group. All
completed questionnaires are logged for inspection, statistics and actions if so required. 
Managers who have completed the questionnaire will receive an email with their
completed questionnaire, a set of recommendations where appropriate, and an
indication of issues to resolve or revisit later.
The designing of the methodology has involved a cross-enterprise initiative have
involved different departments such as Engineering, Corporate Ethics & Sustainability,
Security, Legal, Business, Human Resources, Procurement, as well as an endorsement
of top management.
The questionnaire consists of 12 questions covering all AI Principles. An example can
be seen in the figure below:
It is important to notice that while privacy & security are part of the AI Principles of
Telefónica, the company has specific dedicated processes and areas taking care of
privacy and security. Indeed, privacy and security are relevant for any digital system, and
not only for AI and Big Data applications. Therefore, regarding the principle on Privacy
& Security, the questionnaire refers to the respective areas of the company: the DPO
and the CISO.
Technical tools
Because some of the questions of the questionnaire are impossible to answer without
specific tools, our methodology includes both in-house tools and external tools (mostly
open source). Some of the in-house tools relate to privacy-enhancing technologies such
as anonymization, a tool to detect algorithmic discrimination against protected groups,
and a personal data transparency tool. We are constantly evaluating new external tools,
especially related to bias/discrimination and explainability. This is a very active area of
research with new tools appearing at a rapid pace. It is the responsibility of the
Community of Experts to study and recommend the inclusion of new tools. 
Further Telefónica references
Towards organizational guidelines for the responsible use of AI, Richard
Benjamins, 24th European Conference on Artificial Intelligence - ECAI 2020 Santiago de Compostela, Spain, https://ecai2020.eu/papers/1347_paper.pdf
Responsible AI by Design in Practice, Richard Benjamins, Alberto Barbado, Daniel Sierra, Proceedings of the Human-Centered AI: Trustworthiness of AI Models & Data
(HAI) track at AAAI Fall Symposium, DC, November 7-9, 2019,
https://arxiv.org/html/2001.05375, https://arxiv.org/abs/1909.12838
Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and
challenges toward responsible AI, Alejandro Barreto Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio
Gil-López, Daniel Molina, Richard Benjamins, Raja Chatila, Francisco Herrera, Information Fusion, Volume 58, June 2020, Pages 82-115.
Towards a framework for understanding societal and ethical implications of
Artificial Intelligence, Richard Benjamins, Idoia Salazar, In vulnerabilidad y cultura
digital. Riesgos y oportunidades de la sociedad hiperconectada, Dykinson, pages:
2020, https://www.dykinson.com/libros/vulnerabilidad-y-cultura-digital-riesgos-yoportunidades-de-la-sociedad-hiperconectada/9788413246475/",AI Principles of Telefonica: https://www.telefonica.com/wp-content/uploads/sites/7/2021/11/principios-ai-eng-2018.pdf
41,Vodafone,United Kingdom,Telecommunication,,,https://www.vodafone.com/sites/default/files/2020-09/55678_Vodafone_AI_Framework_AW1_V3.pdf,Artificial Intelligence Framework,"Vodafone Group Plc
Artificial
Intelligence
Framework 
Vodafone Group Plc AI Framework 2019
Introduction
Vodafone Group’s Artificial Intelligence
(AI) Framework sets out our approach
to working with AI technologies and
outlines how we intend to develop
and employ it in a responsible manner
across our international business;
this also applies to the standards we
expect from third parties developing
AI systems in collaboration with and
on behalf of Vodafone. We define
AI as the application of advanced
analytical techniques (such as
Machine Learning, and Natural
Language Processing) combined with
automation to solve problems, develop
personalised products and services
and seize opportunities in new ways.
At Vodafone, we are using AI to help to improve
our products and services and to run our
business as effectively as possible. For example: • AI-powered chat bots increase the speed with
which customer enquiries can be resolved;
• AI techniques in our networks are used to
identify where capacity is needed so that our
customers can enjoy optimised data services,
 such as high quality video streaming;
• Vodafone employees use AI tools and software
to help them work more efficiently; and
• We increasingly use AI to help support good
decision-making, utilising ‘big data’ analysis
based on large, anonymised data sets.
As AI grows in usage and impact across
geographies and industries, Vodafone has
a responsibility to consider how our use of
this technology affects our customers, our
employees, and wider society. We believe it
is critical to ensure that the AI technologies
we create and employ are designed to respect
the privacy and security of the end user’s
data and their associated fundamental rights.
The customer data we use is pseudonymised
and permissioned. We will also seek to make
AI-driven decisions that are fair and free of any
harmful bias. 
Vodafone Group Plc AI Framework 2019 3
Vodafone’s AI Framework
We endeavour to develop
AI in an ethical way so
that it can be trusted.
We endeavour to clearly
inform our customers and
employees when they
communicate directly
with AI-powered systems.
State-of-the-art AI-based systems leverage large anonymised or pseudonymised data sets. It is
critical that the outputs from these data-driven systems do not inadvertently guide us to make
decisions that may affect any group or individual in an unfair way. Vodafone will strive to ensure
that there is effective oversight and a ‘human-in-control’ approach to the use of AI. Beyond that,
it is also important that Vodafone contributes to the debate about how this technology affects
the societies we live in and is made as widely available as possible.
AI-based systems, like intelligent chat bots, have the ability to seem increasingly human in
their responses. We believe that people should be informed about when they interact with an
algorithm or some form of AI/non-human system.
All our AI experts and data
scientists are subject to our
Code of Conduct, which
includes explicit provisions
for nondiscrimination and fair
treatment.
A non-human system that
acts as the primary interface
for Vodafone customers or
employees (for example,
customer care chat bots) will
identify itself as a non-human
operator.
Transparency and Accountability Ethics and Fairness
Vodafone will proactively
participate in the scientific
community, industry coalitions
and self-regulatory bodies
working on research, laws,
regulations and ethical
guidelines for AI, such as the EU
High Level Expert Group on AI.
We strive to clearly inform our
customers and employees
about what data we collect on
our users and how our systems
utilise that data.
Vodafone AI systems should
empower human beings,
enabling them to make
informed decisions.
Vodafone will seek to reduce
any digital divide that occurs
in our markets because of
differential access to AI-based
technologies. That will include
building capacity for greater
AI usage through awarenessraising and educational
partnerships. 
Vodafone Group Plc AI Framework 2019 4
We will ensure that we
respect international
human rights standards
and best practice
around ensuring AI
systems foster diversity,
accessibility and
inclusivity.
We endeavour to respect
the privacy and protect
the security of all
individuals served
by the AI we develop.
Vodafone upholds international human rights across its business footprint. Many of the issues
that arise in the context of AI and human rights are not novel, but are exacerbated by the
scale, proliferation, and real-life impact that AI facilitates. Because of this, the potential of
AI to both strengthen and diminish human rights is much greater than in previous waves of
technological development.
We will act responsibly and in
accordance with technology
industry best practice to
minimise the risks of our
systems being unlawfully used
to the detriment of people’s
human rights.
Preservation of Privacy and Security Human Rights, Diversity and Inclusivity
Vodafone will proactively
engage with industry peers
and other relevant experts (e.g.
academics and civil society)
in consultation exercises to
ensure that AI-based systems
are human-centric and foster
diversity and inclusivity.
We will establish and maintain
a regular dialogue with our
customers as to how we
should use data and AI tools
to serve them.
Customer trust is our number one priority. Respecting the privacy of our customers is essential
to maintaining their trust in our business. Managing privacy risks effectively, including securing
our network and putting customers in control of their data, is core to our approach.
We will ensure that customer
data is carefully managed, in
line with our strong privacy
commitments and prevailing
legislation and only used with
AI systems when we have
established a clear legal basis
to do so.
Our customer data will always
be securely stored under
strict access control and in
compliance with our Group data
security policies and applicable
local laws.
We will make sure that our
security systems for AI are
continually updated as the
threat landscape evolves. 
Vodafone Group Plc AI Framework 2019 5
Vodafone is a responsible
employer and is
determined to become
a leading, human-centric,
digital business.
Over time, we expect AI to automate an increasing number of routine tasks, enabling our
employees to spend more time on higher value and rewarding activities, including the
innovation needed to underpin sustainable business growth into the future.
We will support our existing
employees to gain new skills
so that they can apply for
appropriate roles that are
created by our improved
digital capability.
Maximising the Benefits of AI While Managing
the Disruption of its Implementation
We will deploy AI-based
systems to provide more
effective work environments,
simplifying the work of our
employees and improving their
experience. We will provide
training and education to help
our employees use AI-based
systems to support their work.
We will strive to ensure that our
AI teams – in line with all other
teams in Vodafone – are diverse.
“ AI is not an end in itself, but rather a promising
means to increase human flourishing, thereby
enhancing individual and societal well-being and
the common good, as well as bringing progress
and innovation.”
Ethics Guidelines for Trustworthy AI,
European Commission
Vodafone Group Plc AI Framework 2019
6
Right of redress
Anyone who feels they have been unfairly treated
as a result of a decision made by an AI system
deployed by Vodafone will have the opportunity
to escalate their concerns under the published
process for Vodafone complaints in their country
of operation.
Updates to this document
Vodafone will regularly review and update
this Framework, in light of new products and
technological developments. As the Framework
evolves over time, we pledge to report on
key learnings and observations regarding the
development of ethical AI and the dissemination
of these Framework commitments across
the business. 
© 2019 Vodafone Group Plc
Registered Office:
Vodafone House
The Connection
Newbury
Berkshire
RG14 2FN
Registered in England No. 1833679",Article: https://www.vodafone.com/about-vodafone/how-we-operate/public-policy/policy-positions/artificial-intelligence-framework
42,"Nippon Telegraph and Telephone
",Japan,Telecommunication,05-29-2019,05-29-2019,https://www.nttdata.com/global/en/about-us/ai-guidelines,NTT DATA Group’s AI Guidelines,"1. Realizing Well-being and Sustainability of Society
NTT DATA will focus on sustainability as well as well-being of human society. NTT DATA will promote solving social issues by using AI in recognition of diversity and in consideration with basic human rights. AI should not harm humans; instead, we will use AI to empower humans by supporting their lives and extending their capabilities.

2. Co-Creating New Values by AI
NTT DATA strives to harness the potential benefits of AI and promote innovation by interacting and co-creating with AI stakeholders, including those who engage in research, development, operation, and utilization of AI and by establishing long-term relationships with clients.

3. Fair, Reliable, and Explainable AI
For users of AI to realize its fairness and trustworthiness, NTT DATA invests reasonable efforts in presenting the decisions based on AI in a form that can be understood by humans and doesn’t cause unfair discrimination. We will strive to minimize concerns about the AI's decision-making process to apply constant provision of appropriate AI services.

4. Data Protection
Privacy and security is critical to NTT DATA in providing AI services. Our AI leverages the principles of security and design to protect against unauthorized access, strengthen data traceability, and develop the proper means and policy for collecting, storing, using, and providing personal information.

5. Contribution to Dissemination of Sound AI
NTT DATA will work to further the understanding of AI in order to accelerate its acceptance. NTT DATA will also contribute to the development of society with healthy and harmonious dissemination of AI by improving literacy of AI users and by service design that is universal design compliant.",Press Release: https://www.nttdata.com/global/en/news/press-release/2019/may/ntt-data-introduces-ai-guidelines
43,Softbank Group,Japan,Telecommunication,07-12-2022,07-12-2022,https://www.softbank.jp/en/corp/aboutus/governance/ai-ethics/,SoftBank AI Ethics Policy,"1. Principle of Human-Centeredness
We believe that AI exists to make people happy and should be used only as a decision-making aid for people who use it to solve social problems. We aim to create AI that makes people happier through its use.
2. Respect for Fairness
Fair AI cannot be realized unless we humans have fair values. We respect all people regardless of race, nationality, age, religion, gender, etc. We will develop, design, provide, and use AI based on the same principle, paying attention to the existence of social minorities, such as women, racial minorities, and sexual minorities, which are not readily apparent in commonly distributed data. We aim to realize an inclusive society through technology.
3. Pursuit of Transparency and Accountability
In order to coexist and develop together with AI, it is important to create an environment in which people can understand the results of AI-based judgments and verify the basis of those judgments when necessary. We will strive for transparency in explaining how we use the results of AI judgments. We aim to develop and use AI with verifiability regarding the reliability of the AI and a high level of accountability regarding the basis for the judgments.
4. Ensuring Safety
We believe that AI that contributes to people's happiness can only be realized with a high level of safety. We shall strive to design and develop AI that does not threaten the life, liberty, dignity, or property of individuals, and to create a world in which all people can use AI safely.
5. Privacy Protection and Security
Data is indispensable for the design, development, and use of AI. Without a large amount of accurate data to educate AI, its development and contribution to society cannot be realized. That is why we will appropriately manage and operate various types of data, including personal data, in accordance with internal rules, relevant laws and regulations, and social ethics to prevent unauthorized access, etc., and protect privacy so that people can use AI with peace of mind.
6. Development of AI Human resources and Literacy
We have been focusing on the development of advanced technology. While technology is advancing by leaps and bounds, the people who utilize it must also be constantly evolving and capable of handling technology responsibly. In order to realize a society in which everyone can enjoy the benefits of AI to the fullest, we will actively engage in education and training to enhance the skills and literacy of both the developers and users of AI.",
44,Cohere,Canada,Software,,,https://cohere.ai/responsibility,Responsibility principles,"Responsibility principles
Model the world as we hope it will become.

Anticipate risks and listen to those affected.

Build in mitigation efforts commensurate with expected and actual impacts.

Continually assess the societal impacts of our work.

Our process
We believe that no technology can be made absolutely safe; machine learning is no exception. This requires anticipating and accounting for risks during our development process. We run adversarial attacks, filter our training data for harmful text, and measure our models against safety research benchmarks. We also evaluate evolving risks with monitoring tools designed to identify harmful model outputs.

We recognize that misuse of powerful language models will disproportionately impact the most vulnerable, so we aim to balance safety considerations and equity of access. This is an ongoing process. As we release early versions of our technology, we’ll work closely with our partners and users to ensure its safe and responsible use.

Accountability
Responsibility means more than getting the technology right. It’s about who makes the decisions, communicating risks upfront, and the way we’re held accountable. To help API users anticipate risks, we have published data statements (public documentation about the data used to train our models) and model cards (benchmarks and information about where our technology is and is not working well) for our datasets and models in our documentation here.

We believe that good decisions don’t happen in a vacuum. Responsibility must be baked into the culture and norms of the machine learning ecosystem, and we’re committed to sharing knowledge and best practices. We’ll be supporting workshops around responsible use—if you’re developing language models or working to mitigate their risks, please reach out.

Our advisory Responsibility Council is made up of external experts who will advise our business and research practices. They have visibility into our customer development pipeline and engineering processes, and agency to ask difficult questions around the permissible uses of our technologies.

Usage Guidelines
We require our users to abide by Cohere’s Usage Guidelines. Access will be revoked if these terms are not followed. If you spot the Cohere Platform being used in a harmful or otherwise unproductive way, please report it to us.",
45,Moveworks,United States of America,Software,,,https://www.moveworks.com/hubfs/docs/Datasheets/Moveworks-Datasheet-ResponsibleAI.pdf,Our Approach to Responsible AI,"The gold standard of security
Building responsibility into our AI means implementing robust data privacy
protections across our entire AI platform, so that our customers’ information is
always secure.
Moveworks meets the highest security standards for an enterprise SaaS company.
We’re ISO 27001 certified, we’re compliant with SOC 2 Type 1 and Type 2, and we
achieve “Gold” status for CSA STAR Level 2, which is given only to organizations
with the most mature cloud security postures. Indeed, security is ingrained into
every aspect of our business:
• Our people are trained to keep pace with the latest security best practices
• Our processes are continuously tested to meet stringent privacy requirements
• Our product is designed with a DevSecOps approach across the board
To learn more, please review our dedicated Security webpage >
© Moveworks, Inc www.moveworks.com
“Trust, security, and responsibility
are at the core of our business
at DocuSign. We only evaluate
vendors who share those same
values, and that’s what gave us
the confidence to partner with
Moveworks.”
—Saran Mandair,
 VP of Global IT, DocuSign
Our Approach to
Responsible AI
Moveworks is deeply committed to the practice of responsible
AI. This datasheet summarizes how we protect our customers’
data and mitigate the risk of bias. 
Responsible AI also requires ensuring our machine learning
(ML) process is as unbiased as possible. That’s why Moveworks
actively minimizes or eliminates potential sources of bias in our ML
models with respect to all protected classes, including race, age,
disability, color, national origin, religion, sexual orientation, gender
identification, and genetic information.
At its core, machine learning is about extracting the signal from the
noise—taking the relevant features from an input, such as a piece
of text, and making a prediction using patterns that an ML model
has learned from similar data. The bias and equity challenge for ML
developers is to ensure that the signals extracted from the noise
are appropriate inferences to make based on the input, rather than
replications of implicit or explicit biases that exist in the training
data.
Moveworks is well aware of these challenges; we believe we have
taken appropriate measures to minimize or eliminate sources of
bias. Given the problem that our platform solves, Moveworks has a
lower risk of prejudicial bias than other AI companies. For instance,
resetting a password or provisioning a Zoom license doesn’t
require our ML models to know personal information about the
user.
However, we still take precautions to reduce the risk of bias:
1. Moveworks’ ML models are trained primarily on data
sampled from production usage. Using a technique known
as Collective Learning, many of our models are trained on
anonymized data drawn from multiple customers, allowing
them to learn the universal structure of requests from
employees with different backgrounds and characteristics.
2. We annotate this data without exposing any user
characteristics to the annotator: no names, photos, or other
category-identifying features are included in the annotation
interface. When annotating the intention of a request, for
example, the annotator only sees the text of the message and
the name of the organization.
3. During training, we do not include protected attributes, such
as gender or race, in the inputs from which the models learn
to derive signals.
4. For the vast majority of requests sent to our bot, it would be
extremely difficult, if not impossible, for a human annotator
to guess any protected categories about the knowledge
worker from the text of the request—and harder still for our ML
models. This means the models are much less likely to learn
an intermediate representation associated with a protected
class.
All of these measures reinforce our commitment to responsible
AI at Moveworks, from securing our platform to ensuring data
privacy to eliminating sources of bias.
Eliminating sources of bias
© Moveworks, Inc www.moveworks.com
moveworks.com/request-demo
Request a demo",
46,Boston Consulting Group (Gamma),United States of America,Software,,,https://media-publications.bcg.com/AI-Code-of-Conduct.pdf,AI Code of Conduct,"AI Code of Conduct
CONTENTS
BCG Purpose + Responsible AI
Living Our Principles
Our Promise to Act
Future Proofing Responsible AI
Boston Consulting Group partners with leaders in business
and society to tackle their most important challenges and
capture their greatest opportunities. BCG was the pioneer
in business strategy when it was founded in 1963. Today,
we work closely with clients to embrace a transformational
approach aimed at benefiting all stakeholders—empow
-
ering organizations to grow, build sustainable competitive
advantage, and drive positive societal impact.
Our diverse, global teams bring deep industry and function
-
al expertise and a range of perspectives that question the
status quo and spark change. BCG delivers solutions through
leading-edge management consulting, technology and
design, and corporate and digital ventures. We work in a
uniquely collaborative model across the firm and through
-
out all levels of the client organization, fueled by the goal of
helping our clients thrive and enabling them to make the
world a better place.
A Message from Our CEO
Dear colleagues and friends of BCG,
As you know, AI and data are playing an increasingly vital role in our clients’ businesses. Clients
trust us to help them transform their organizations and industries through AI. And though
AI presents another exciting avenue for us to
live our Purpose—to unlock the potential of
those who advance the world—it comes with
risks. We have an obligation to ensure that the
AI solutions we create deliver transformative
impact without inadvertently harming people
or communities or compromising BCG’s values.
This is a time for BCG to lead with integrity. We
have a duty to ourselves, our clients, and society to proactively ensure that the responsible
use of AI is core to our approach. This Code of
Conduct outlines what we’re already doing, as
well as our promise for the future: to continue to
responsibly design, develop, and deploy AI systems around the world.
Thank you for your continued support and engagement. I look forward to advancing our clients and the world through AI – responsibly.
Christoph Schweizer
We have an obligation to ensure that the AI
solutions we create deliver transformative
impact without inadvertently harming
people or communities or compromising
BCG’s values.
– Christoph Schweizer, CEO
5 AI CODE OF CONDUCT BOSTON CONSULTING GROUP 6
BCG Purpose + Responsible AI
In the constantly changing field of AI, with its potential benefits and mounting risks, it’s vital that we anchor ourselves to our Purpose: to unlock the potential
of those who advance the world.
Our Purpose is underpinned by five pillars that capture the core facets of who we are as an organization.
They guide how we interact with each other, our clients, and the world, including our approach to designing, developing, deploying, and using AI. Our
Responsible AI approach is anchored to our timeless Purpose Principles, which capture our distinctive strengths: Bring Insight to Light, Drive Inspired
Impact, Conquer Complexity, Lead with Integrity,
and Grow by Growing Others.
BRING INSIGHT TO LIGHT
We make the unknowns of AI known, illuminating all possible outcomes of a system before and
during the design process. We drive transparency by explaining how an AI algorithm operates,
what data it uses, how it decides, and what its intentions are.
Example:
BCG developed FACET, an open-source library
for explainable AI to support exploration and
understanding of supervised machine-learning
models. FACET breaks down feature interactions
into three key components: redundancy, synergy, and independence. By helping developers
and business users understand how algorithms
analyze the data sets on which AI predictions are
based, FACET reestablishes human control over
and trust in AI.
DRIVE INSPIRED IMPACT
We employ AI boldly and proactively to create
transformative business impact, but with protections to ensure that it doesn’t harm society.
By creating safeguards to protect data and reduce unintended behaviors and outcomes, we
can be confident that the systems we devise are
secure, equitable, and fair.
Example:
BCG worked with a brick-and-mortar retailer to optimize the footprint of its locations.
We helped the client think beyond just cost
and revenue forecasts to consider the socioeconomic diversity of the neighborhoods
where its stores would be opened or closed.
The approach helped our client live up to the
DEI commitments it had made to shareholders
and customers.
CONQUER COMPLEXITY
We bring to light the possibilities and risks
that machine learning presents. Our task is to
design elegant AI systems to solve problems,
create value, and minimize the environmental impact of their operation and the outcomes
they produce.
Example:
BCG worked with a global fashion retailer to retool its demand forecasting system in
a way that enabled human control while leveraging AI. We created a system which AI
made forecasts that humans could then modify based on emerging fashion trends. The
company was rewarded with increased sales,
reduced costs, less waste, and a smaller
carbon footprint.
7 AI CODE OF CONDUCT BOSTON CONSULTING GROUP 8
LEAD WITH INTEGRITY
We seek to set an ethical standard in our indus
-
try. As a champion of accountability, we openly
discuss risks with our clients. We decline projects
that conflict with our values and principles, we
protect data and personal privacy, and we take
responsibility for what we make and what our
systems make happen.
Example:
BCG worked with a payroll and HR services pro
-
vider to deploy and test methods for bias de
-
tection in its sales lead generation process. We
helped develop a B2B decision support system
to identify sources of bias related to the demo
-
graphic makeup of potential customers. We
worked with the client to surface biases encod
-
ed in the historical data, designed the system to
minimize those biases, and developed an internal white paper for company leadership to educate business stakeholders and data scientists.
GROW BY GROWING OTHERS
We grow our clients’ businesses by empowering
leaders to make the right economic and ethi
-
cal decisions. This means finding solutions that
augment or advance both machine learning and
human progress.
Example:
BCG worked with Microsoft to develop a set of in
-
dustry-leading guidelines for product leaders to
help implement AI responsibly. The resource pro
-
vides a clear, actionable framework for leaders to
guide product teams in assessing, designing, and
validating Responsible AI (RAI) systems within
their organizations.
9 AI CODE OF CONDUCT 10
Applying our Responsible AI Principles requires investing in the
people, processes, and technology that touch every aspect of our
organization.
OUR PRODUCTS
We conduct early risk assessments to ensure that projects align with our values. We also offer BCG’s RATE.ai
Responsible AI algorithmic impact assessment to help product managers guide their teams toward proactively identifying and addressing risks.
Our Delivery Excellence program provides added layers of independent oversight to help us evaluate risk, develop mitigations, review execution for quality, and, when needed, escalate for senior leader alignment.
OUR PEOPLE
To empower our teams to act on our Responsible AI Principles,
raise concerns, and take responsibility for all that we build,
we offer a range of RAI training.
We’ve developed a comprehensive set of interactive technical tutorials and open-source software to support our technical teams and ensure implementation of best practices.
We’ve also expanded the role of our ombudspersons to allow
staff to easily raise RAI concerns.
OUR LEADERS
A multidisciplinary Responsible AI Council composed of global senior leaders monitors all RAI implementation. Officers
leading product teams evaluate AI as a distinct category of
risk, and teams have clear escalation paths to firm leadership. To complement these measures, we give leaders dedicated training to build and maintain their RAI awareness.
OUR CLIENTS
To aid clients on their RAI journey, we enable their product
teams with open-source tools like FACET and Code Carbon
and frameworks such as Ten Guidelines for Product Leaders
to Implement AI Responsibly. In addition to publishing our
insights on RAI, we share our expertise in a variety of industry forums, including the Business Roundtable and the World
Economic Forum. Our goal is to encourage and champion
broad RAI adoption.
Living Our
Principles
12
Responsible AI is about more than building superior
technical solutions; it’s about earning the trust of our
employees, leaders, clients, and society. We’re com
-
mitted to providing transparency and holding ourselves accountable to our promise of Responsible AI
through the following actions.
INTENDED USE
We’ll partner with clients to clearly define the in
-
tended use for every AI product.
TRANSPARENCY
We’ll make available to our clients the output
from our algorithmic impact assessments.
DOCUMENTATION
We’ll share AI product documentation (e.g.,
model cards, data provenance) with our clients,
including RAI risks and mitigations.
REGULAR REPORTING
We’ll ensure that our Responsible AI Council
generates an annual report on our RAI program.
ENABLEMENT
We’ll empower our clients by continuing to build
tools, frameworks, and other artifacts that they
can use to implement Responsible AI practices.
COMMUNITY ENGAGEMENT
We’ll continue to proactively engage the busi
-
ness and AI ecosystem by sharing our knowl
-
edge, encouraging others to act, and advancing
Responsible AI before challenges arise.
Our Promise to
Act
Responsible AI obligations do not end when an AI
product is deployed. Issues can emerge when input
data evolves, source code is modified, or underlying
software packages or models are updated. Core to
our approach is enabling our clients’ monitoring re
-
gimes to ensure that emergent issues are rapidly iden
-
tified and addressed.
In addition, Responsible AI approaches can
-
not be static. They must support ongoing gov
-
ernance and monitoring, and they must evolve
with AI techniques, data availability, real-world
outcomes, and regulations. We’ve created a re
-
sponsive, adaptive governance system that
scans the environment for cutting-edge re
-
search, breakthrough best practices, and emerg
-
ing risks and opportunities. We then weave
those findings into how we do things. We’re com
-
mitted to helping others do the same.
To fulfill this commitment, we’ll continue to
evolve our practices and ground them in our
Purpose: to unlock the potential of those
who advance the world.
Future Proofing
Responsible AI
13 14
To find the latest BCG content and register to receive e-alerts on this topic or
others, please visit bcg.com.
Follow Boston Consulting Group on Facebook and Twitter.
© Boston Consulting Group, Inc. 2022. All rights reserved.",Blog: https://www.bcg.com/publications/2022/acting-responsibly-in-tight-ai-regulation-era
47,Deloitte,United Kingdom,Software,08-26-2020,08-26-2020,https://www2.deloitte.com/us/en/pages/deloitte-analytics/solutions/ethics-of-ai-framework.html,Deloitte’s Trustworthy AI™ framework,"Deloitte’s Trustworthy AI™ framework
We put trust at the center of everything we do. We use a multidimensional AI framework to help organizations develop ethical safeguards across six key dimensions—a crucial step in managing the risks and capitalizing on the returns associated with artificial intelligence.

Trustworthy AI™ requires governance and regulatory compliance throughout the AI lifecycle from ideation to design, development, deployment and machine learning operations (MLOps) anchored on the six dimensions in Deloitte's Trustworthy AI™ framework—transparent and explainable, fair and impartial, robust and reliable, respectful of privacy, safe and secure, and responsible and accountable. At its foundation, AI governance encompasses all the above stages, and is embedded across technology, processes and employee trainings. This includes adhering to applicable regulations, as it prompts risk evaluation, control mechanisms, and overall compliance. Together, governance and compliance are the means by which an organization and its stakeholders ensure AI deployments are ethical and can be trusted.
Fair and impartial
Assess whether AI systems include internal and external checks to help enable equitable application across all participants.
Transparent and explainable
Help participants understand how their data can be used and how AI systems make decisions. Algorithms, attributes, and correlations are open to inspection.
Responsible and accountable
Put an organizational structure and policies in place that can help clearly determine who is responsible for the output of AI system decisions.
Robust and Reliable
Confirm that AI systems have the ability to learn from humans and other systems and produce consistent and reliable outputs.
Respectful of privacy
Respect data privacy and avoid using AI to leverage customer data beyond its intended and stated use. Allow customers to opt in and out of sharing their data.
Safe and secure
Protect AI systems from potential risks (including cyber risks) that may cause physical and digital harm.",Services: https://www2.deloitte.com/us/en/pages/deloitte-analytics/solutions/ethics-of-ai-framework.html
48,Price waterhouse Coopers (PwC),United Kingdom,Software,,,https://www.pwc.com/gx/en/issues/data-and-analytics/artificial-intelligence/what-is-responsible-ai/pwc-responsible-ai.pdf,PwC’s Responsible AI ,"PwC’s Responsible AI
AI you can trust
AI Risks
AI algorithms that
ingest real-world
data and
preferences as
inputs, run a risk of
learning and
imitating our biases
and prejudices.
For as long as
automated systems
have existed,
humans have tried
to circumvent them.
This is no different
with AI.
Similar to any other
technology, AI
should have
organisation-wide
oversight with
clearly-identified
risks and controls.
The widespread
adoption of
automation across
all areas of the
economy may
impact jobs and
shift demand to
different skills.
The widespread
adoption of complex
and autonomous AI
systems could
result in “echochambers”
developing between
machines and
humans.
AI solutions are
designed with
specific objectives in
mind which may
compete with
overarching
organisational and
societal values within
which they operate.
1. Performance 2. Security 3. Control 4. Economic 5. Societal 6. Ethical
AI is here to stay—bringing limitless potential to push us
forward as a society. Used wisely, it can create huge
benefits for businesses, governments, and individuals
worldwide.
How big is the opportunity? Our research estimates that AI could
contribute $15.7 trillion to the global economy by 2030, as a result of
productivity gains and increased consumer demand driven by AIenhanced products and services. AI solutions are diffusing across
industries and impacting everything from customer service and sales to
back office automation. AI’s transformative potential continues to be top
of mind for business leaders: Our CEO survey finds that 72% of CEOs
believe that AI will significantly change the way they do business in the
next five years.
With great potential comes great risk. Are your algorithms making decisions that align with your values? Do customers trust you
with their data? How is your brand affected if you can’t explain how AI systems work? It’s critical to anticipate problems and futureproof your systems so that you can fully realize AI’s potential. It’s a responsibility that falls to all of us — board members, CEOs,
business unit heads, and AI specialists alike.
Performance risks
include:
•Risk of errors
•Risk of bias
•Risk of opaqueness
•Risk of instability of
performance
•Lack of feedback
process
Security risks
include:
•Cyber intrusion
risks
•Privacy risks
•Open source
software risks
•Adversarial attacks
Control risks
include:
•Risk of AI going
rogue
•Control malevolent
AI
Economic risks
include:
•Risk of job
displacement
•Risk of
concentration of
power within 1 or a
few companes
•Liability risk
Societal risks
include:
•Risk of autonomous
weapons
proliferation
•Risk of an
Intelligence divide
Ethical risks
include:
•Lack of values risk
•Values
misalignment risk
Our holistic approach helps you address the five dimensions of Responsible AI
Innovate Responsibly
Whether you're just getting started or are getting ready to scale, put your trust in Responsible AI. Drawing on our
proven capability in AI innovation and deep global business expertise, we'll assess your end-to-end needs, and design
a solution to address your unique risks and challenges.
© 2019 PwC. All rights reserved.
PwC refers to the PwC network and/or one or more of its member firms, each of which is a separate legal entity. Please see www.pwc.com/structure for further
details.
This content is for general information purposes only, and should not be used as a substitute for consultation with professional advisors.
Liability limited by a scheme approved under Professional Standards Legislation.
At PwC, our purpose is to build trust in society and solve important problems. We’re a network of firms in 158 countries with more than 236,000 people who are
committed to delivering quality in assurance, advisory and tax services. Find out more and tell us what matters to you by visiting us at www.pwc.com.
WL 127068859
Our Ethical AI
Framework provides
guidance and a
practical approach to
help your organisation
with the development,
and governance of AI
solutions that are
ethical and moral.
Ethics &
Regulation
Fairness is a social
construct with many
different and, at
times, conflicting
definitions.
Responsible AI helps
your organisation to
become more aware
of bias, and take
corrective action to
help systems improve
fairness in their
decision-making.
Robustness &
Security
The foundation for
Responsible AI is an
end-to-end enterprise
governance
framework. This
focuses on the risks
and controls at every
step of your
organisation’s AI
journey, and at every
level, from the
enterprise to the
individual AI model.
Governance
An AI system that
human users are
unable to understand
can lead to a “black
box” effect. We
provide services to
help you explain both
overall decisionmaking and also
individual choices and
predictions, tailored to
the perspectives of
different stakeholders.
Interpretability &
Explainability
To help make your
systems more
resilient, Responsible
AI includes services to
identify weaknesses
in models, assess
system safety and
monitor long-term
performance.
Bias & Fairness
Contact us
Contact us today. Learn more about how to become an industry leader in the responsible use of AI.
Anand Rao
Global & US Artificial Intelligence and US Data
& Analytics Leader, PwC US
Email: anand.s.rao@pwc.com
Flavio Palaci
Global Data Analytics & Artificial Intelligence
Leader, PwC Australia
Email: flavio.j.palaci@pwc.com
PwC’s Responsible AI Toolkit
Your stakeholders, including board members, customers, and regulators, will have many questions about your
organisation's use of AI and data, from how it’s developed to how it’s governed. You not only need to be ready to
provide the answers, you must also demonstrate ongoing governance and regulatory compliance.
Our Responsible AI Toolkit is a suite of customizable frameworks, tools and processes designed to help you harness
the power of AI in an ethical and responsible manner - from strategy through execution. With the Responsible AI
toolkit, we’ll tailor our solutions to address your organisation’s unique business requirements and AI maturity.",Report: https://www.pwc.com/gx/en/issues/data-and-analytics/artificial-intelligence/what-is-responsible-ai/responsible-ai-practical-guide.pdf
49,Deeper Insights,Portugal,Software,,,"https://deeperinsights.com/our-principles/#:~:text=Ethical%20AI,lives%20for%20the%20greater%20good.",Ethical AI,"ETHICAL AI 
As AI becomes more advanced, we pledge that our technology innovation remains fair and free from bias, that data is safe and secure and used appropriately to deliver intelligent solutions that improve lives for the greater good. We may choose not to work with companies who are deemed to operate unethically or do harm to the environment.",Article: https://deeperinsights.com/making-data-ethics-your-competitive-edge/
