{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kensuzuki95/Corporate_AI_Ethics_Guideline_Analysis/blob/main/Document_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** **\n",
        "# Step 1: Load Package\n",
        "** **"
      ],
      "metadata": {
        "id": "8i4pbaLeO0uI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "id": "sgS4cm5QRPr0"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import requests\n",
        "import io"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** **\n",
        "# Step 2: Load Data\n",
        "** **"
      ],
      "metadata": {
        "id": "iLTCvQ8rO8gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the csv file from your GitHub account\n",
        "\n",
        "url_1 = (\"https://raw.githubusercontent.com/Kensuzuki95/Corporate_AI_Ethics_Guideline_Analysis/main/Dataset/Dataset_Filtered.csv\")\n",
        "download = requests.get(url_1).content\n",
        "\n",
        "dataset = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "\n",
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "fJk-IOizSh78",
        "outputId": "89cae663-274a-4887-d39e-32a8af151703"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   No. Company Name                   Country    Industry Published Year  \\\n",
              "0    1    Accenture                   Ireland  Consulting     03-30-2021   \n",
              "1    2        Adobe  United States of America    Software            NaN   \n",
              "2    3     Alphabet  United States of America    Software            NaN   \n",
              "3    4       Amazon  United States of America    Software            NaN   \n",
              "4    5         Atos                    France  Consulting            NaN   \n",
              "\n",
              "  Last Revised                                               Link  \\\n",
              "0   03-30-2021  https://www.accenture.com/content/dam/accentur...   \n",
              "1          NaN  https://www.adobe.com/content/dam/cc/en/ai-eth...   \n",
              "2          NaN  https://ai.google/responsibilities/responsible...   \n",
              "3          NaN  https://d1.awsstatic.com/responsible-machine-l...   \n",
              "4          NaN  https://atos.net/en/lp/cybersecurity-magazine-...   \n",
              "\n",
              "                                Document Name  \\\n",
              "0  Responsible AI From principles to practice   \n",
              "1             Adobe’s Commitment to AI Ethics   \n",
              "2                    Responsible AI practices   \n",
              "3         Responsible Use of Machine Learning   \n",
              "4       The Atos Blueprint for Responsible AI   \n",
              "\n",
              "                                           Main Text  \\\n",
              "0  Responsible AI\\r\\nFrom principles to practice\\...   \n",
              "1  Adobe’s Commitment to AI Ethics\\r\\nAt Adobe, o...   \n",
              "2  Responsible AI practices\\r\\nThe development of...   \n",
              "3  Responsible Use of Machine Learning\\r\\nAt AWS,...   \n",
              "4  AI is a broad topic encompassing many differen...   \n",
              "\n",
              "                                             Comment  \n",
              "0  Addtional Details: https://www.accenture.com/u...  \n",
              "1  Addtional Details: https://www.adobe.com/conte...  \n",
              "2  Addtional Information: https://ai.google/princ...  \n",
              "3                                                NaN  \n",
              "4                                                NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-56d2e59d-ff3b-4f0c-bf2a-1c76a226a24d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No.</th>\n",
              "      <th>Company Name</th>\n",
              "      <th>Country</th>\n",
              "      <th>Industry</th>\n",
              "      <th>Published Year</th>\n",
              "      <th>Last Revised</th>\n",
              "      <th>Link</th>\n",
              "      <th>Document Name</th>\n",
              "      <th>Main Text</th>\n",
              "      <th>Comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Accenture</td>\n",
              "      <td>Ireland</td>\n",
              "      <td>Consulting</td>\n",
              "      <td>03-30-2021</td>\n",
              "      <td>03-30-2021</td>\n",
              "      <td>https://www.accenture.com/content/dam/accentur...</td>\n",
              "      <td>Responsible AI From principles to practice</td>\n",
              "      <td>Responsible AI\\r\\nFrom principles to practice\\...</td>\n",
              "      <td>Addtional Details: https://www.accenture.com/u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Adobe</td>\n",
              "      <td>United States of America</td>\n",
              "      <td>Software</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://www.adobe.com/content/dam/cc/en/ai-eth...</td>\n",
              "      <td>Adobe’s Commitment to AI Ethics</td>\n",
              "      <td>Adobe’s Commitment to AI Ethics\\r\\nAt Adobe, o...</td>\n",
              "      <td>Addtional Details: https://www.adobe.com/conte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Alphabet</td>\n",
              "      <td>United States of America</td>\n",
              "      <td>Software</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://ai.google/responsibilities/responsible...</td>\n",
              "      <td>Responsible AI practices</td>\n",
              "      <td>Responsible AI practices\\r\\nThe development of...</td>\n",
              "      <td>Addtional Information: https://ai.google/princ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Amazon</td>\n",
              "      <td>United States of America</td>\n",
              "      <td>Software</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://d1.awsstatic.com/responsible-machine-l...</td>\n",
              "      <td>Responsible Use of Machine Learning</td>\n",
              "      <td>Responsible Use of Machine Learning\\r\\nAt AWS,...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Atos</td>\n",
              "      <td>France</td>\n",
              "      <td>Consulting</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>https://atos.net/en/lp/cybersecurity-magazine-...</td>\n",
              "      <td>The Atos Blueprint for Responsible AI</td>\n",
              "      <td>AI is a broad topic encompassing many differen...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56d2e59d-ff3b-4f0c-bf2a-1c76a226a24d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-56d2e59d-ff3b-4f0c-bf2a-1c76a226a24d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-56d2e59d-ff3b-4f0c-bf2a-1c76a226a24d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean the Dataset Format"
      ],
      "metadata": {
        "id": "Zon0wpKfUVvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for unecesarry columns\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "y7dr4ByrXSHd",
        "outputId": "2948c840-eb53-40aa-fb3a-a14a76d5370e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['No.', 'Company Name', 'Country', 'Industry', 'Published Year',\n",
              "       'Last Revised', 'Link', 'Document Name', 'Main Text', 'Comment'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = dataset.drop(columns=['No.','Country', 'Industry', 'Published Year', 'Last Revised', 'Link', 'Comment'], axis=1)\n",
        "text_data.info()\n",
        "text_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "Tl4D22fNOwT1",
        "outputId": "6eb8d2dc-101f-40ec-82fe-2a24561cfba4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 49 entries, 0 to 48\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Company Name   49 non-null     object\n",
            " 1   Document Name  49 non-null     object\n",
            " 2   Main Text      49 non-null     object\n",
            "dtypes: object(3)\n",
            "memory usage: 1.3+ KB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Company Name                               Document Name  \\\n",
              "0    Accenture  Responsible AI From principles to practice   \n",
              "1        Adobe             Adobe’s Commitment to AI Ethics   \n",
              "2     Alphabet                    Responsible AI practices   \n",
              "3       Amazon         Responsible Use of Machine Learning   \n",
              "4         Atos       The Atos Blueprint for Responsible AI   \n",
              "\n",
              "                                           Main Text  \n",
              "0  Responsible AI\\r\\nFrom principles to practice\\...  \n",
              "1  Adobe’s Commitment to AI Ethics\\r\\nAt Adobe, o...  \n",
              "2  Responsible AI practices\\r\\nThe development of...  \n",
              "3  Responsible Use of Machine Learning\\r\\nAt AWS,...  \n",
              "4  AI is a broad topic encompassing many differen...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5aa86e7-fa01-4f5f-9915-806325059220\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Company Name</th>\n",
              "      <th>Document Name</th>\n",
              "      <th>Main Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Accenture</td>\n",
              "      <td>Responsible AI From principles to practice</td>\n",
              "      <td>Responsible AI\\r\\nFrom principles to practice\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adobe</td>\n",
              "      <td>Adobe’s Commitment to AI Ethics</td>\n",
              "      <td>Adobe’s Commitment to AI Ethics\\r\\nAt Adobe, o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Alphabet</td>\n",
              "      <td>Responsible AI practices</td>\n",
              "      <td>Responsible AI practices\\r\\nThe development of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Amazon</td>\n",
              "      <td>Responsible Use of Machine Learning</td>\n",
              "      <td>Responsible Use of Machine Learning\\r\\nAt AWS,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Atos</td>\n",
              "      <td>The Atos Blueprint for Responsible AI</td>\n",
              "      <td>AI is a broad topic encompassing many differen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5aa86e7-fa01-4f5f-9915-806325059220')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f5aa86e7-fa01-4f5f-9915-806325059220 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f5aa86e7-fa01-4f5f-9915-806325059220');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3bNa1W-LibG"
      },
      "source": [
        "** **\n",
        "#Step 3: Data Cleaning\n",
        "** **\n",
        "\n",
        "Since the goal of this analysis is to perform topic modeling, we will solely focus on the text data from each paper, and drop other metadata columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQkJtSCMLibH"
      },
      "source": [
        "## Remove punctuation/lower casing\n",
        "\n",
        "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "\n",
        "# Remove punctuation\n",
        "text_data['Main_Text_Processed'] = text_data['Main Text'].map(lambda x: re.sub('[,\\.!?()]', '', x))\n",
        "\n",
        "# Convert the text to lowercase\n",
        "text_data['Main_Text_Processed'] = text_data['Main_Text_Processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Applying Tokenization\n",
        "nltk.download('punkt')\n",
        "text_data['Main_Text_Tokenized'] = text_data.apply(lambda row: nltk.word_tokenize(row['Main_Text_Processed']), axis=1)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "#training_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NfpVyTk1MTZ",
        "outputId": "0f9bc953-d7f6-4631-a99a-e09b16626c37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecu91GdKLibI"
      },
      "source": [
        "## Tokenize words and further clean-up text\n",
        "\n",
        "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CvXWqoBMagLR",
        "outputId": "bbf6801a-eed6-419c-9610-7e14d2a6b5c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Company Name                               Document Name  \\\n",
              "0    Accenture  Responsible AI From principles to practice   \n",
              "1        Adobe             Adobe’s Commitment to AI Ethics   \n",
              "2     Alphabet                    Responsible AI practices   \n",
              "3       Amazon         Responsible Use of Machine Learning   \n",
              "4         Atos       The Atos Blueprint for Responsible AI   \n",
              "\n",
              "                                           Main Text  \\\n",
              "0  Responsible AI\\r\\nFrom principles to practice\\...   \n",
              "1  Adobe’s Commitment to AI Ethics\\r\\nAt Adobe, o...   \n",
              "2  Responsible AI practices\\r\\nThe development of...   \n",
              "3  Responsible Use of Machine Learning\\r\\nAt AWS,...   \n",
              "4  AI is a broad topic encompassing many differen...   \n",
              "\n",
              "                                 Main_Text_Processed  \\\n",
              "0  responsible ai\\r\\nfrom principles to practice\\...   \n",
              "1  adobe’s commitment to ai ethics\\r\\nat adobe ou...   \n",
              "2  responsible ai practices\\r\\nthe development of...   \n",
              "3  responsible use of machine learning\\r\\nat aws ...   \n",
              "4  ai is a broad topic encompassing many differen...   \n",
              "\n",
              "                                 Main_Text_Tokenized  \n",
              "0  [responsible, ai, from, principles, to, practi...  \n",
              "1  [adobe, ’, s, commitment, to, ai, ethics, at, ...  \n",
              "2  [responsible, ai, practices, the, development,...  \n",
              "3  [responsible, use, of, machine, learning, at, ...  \n",
              "4  [ai, is, a, broad, topic, encompassing, many, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62766428-ac64-402a-9ecf-4b0faaa9923d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Company Name</th>\n",
              "      <th>Document Name</th>\n",
              "      <th>Main Text</th>\n",
              "      <th>Main_Text_Processed</th>\n",
              "      <th>Main_Text_Tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Accenture</td>\n",
              "      <td>Responsible AI From principles to practice</td>\n",
              "      <td>Responsible AI\\r\\nFrom principles to practice\\...</td>\n",
              "      <td>responsible ai\\r\\nfrom principles to practice\\...</td>\n",
              "      <td>[responsible, ai, from, principles, to, practi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adobe</td>\n",
              "      <td>Adobe’s Commitment to AI Ethics</td>\n",
              "      <td>Adobe’s Commitment to AI Ethics\\r\\nAt Adobe, o...</td>\n",
              "      <td>adobe’s commitment to ai ethics\\r\\nat adobe ou...</td>\n",
              "      <td>[adobe, ’, s, commitment, to, ai, ethics, at, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Alphabet</td>\n",
              "      <td>Responsible AI practices</td>\n",
              "      <td>Responsible AI practices\\r\\nThe development of...</td>\n",
              "      <td>responsible ai practices\\r\\nthe development of...</td>\n",
              "      <td>[responsible, ai, practices, the, development,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Amazon</td>\n",
              "      <td>Responsible Use of Machine Learning</td>\n",
              "      <td>Responsible Use of Machine Learning\\r\\nAt AWS,...</td>\n",
              "      <td>responsible use of machine learning\\r\\nat aws ...</td>\n",
              "      <td>[responsible, use, of, machine, learning, at, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Atos</td>\n",
              "      <td>The Atos Blueprint for Responsible AI</td>\n",
              "      <td>AI is a broad topic encompassing many differen...</td>\n",
              "      <td>ai is a broad topic encompassing many differen...</td>\n",
              "      <td>[ai, is, a, broad, topic, encompassing, many, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62766428-ac64-402a-9ecf-4b0faaa9923d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-62766428-ac64-402a-9ecf-4b0faaa9923d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-62766428-ac64-402a-9ecf-4b0faaa9923d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the function to remove stopwords from tokenized text\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stopwords]\n",
        "    return output\n",
        "\n",
        "#applying the function\n",
        "text_data['Main_text_without_stopwords'] = text_data['Main_Text_Tokenized'].apply(lambda x:remove_stopwords(x))\n",
        "#training_data.head()"
      ],
      "metadata": {
        "id": "YPZpzYNm8AZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74af38d8-dc48-408b-c04e-51c131550c2d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the Stemming function from nltk library\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "#defining the object for stemming\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "#defining a function for stemming\n",
        "def stemming(text):\n",
        "  stem_text = [porter_stemmer.stem(word) for word in text]\n",
        "  return stem_text\n",
        "\n",
        "text_data['Main_text_stemmed'] = text_data['Main_text_without_stopwords'].apply(lambda x: stemming(x))\n",
        "text_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8EW10zRD8ld0",
        "outputId": "063a6f23-5781-457b-d4ec-c489d122d9e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Company Name                               Document Name  \\\n",
              "0    Accenture  Responsible AI From principles to practice   \n",
              "1        Adobe             Adobe’s Commitment to AI Ethics   \n",
              "2     Alphabet                    Responsible AI practices   \n",
              "3       Amazon         Responsible Use of Machine Learning   \n",
              "4         Atos       The Atos Blueprint for Responsible AI   \n",
              "\n",
              "                                           Main Text  \\\n",
              "0  Responsible AI\\r\\nFrom principles to practice\\...   \n",
              "1  Adobe’s Commitment to AI Ethics\\r\\nAt Adobe, o...   \n",
              "2  Responsible AI practices\\r\\nThe development of...   \n",
              "3  Responsible Use of Machine Learning\\r\\nAt AWS,...   \n",
              "4  AI is a broad topic encompassing many differen...   \n",
              "\n",
              "                                 Main_Text_Processed  \\\n",
              "0  responsible ai\\r\\nfrom principles to practice\\...   \n",
              "1  adobe’s commitment to ai ethics\\r\\nat adobe ou...   \n",
              "2  responsible ai practices\\r\\nthe development of...   \n",
              "3  responsible use of machine learning\\r\\nat aws ...   \n",
              "4  ai is a broad topic encompassing many differen...   \n",
              "\n",
              "                                 Main_Text_Tokenized  \\\n",
              "0  [responsible, ai, from, principles, to, practi...   \n",
              "1  [adobe, ’, s, commitment, to, ai, ethics, at, ...   \n",
              "2  [responsible, ai, practices, the, development,...   \n",
              "3  [responsible, use, of, machine, learning, at, ...   \n",
              "4  [ai, is, a, broad, topic, encompassing, many, ...   \n",
              "\n",
              "                         Main_text_without_stopwords  \\\n",
              "0  [responsible, ai, principles, practice, conten...   \n",
              "1  [adobe, ’, commitment, ai, ethics, adobe, purp...   \n",
              "2  [responsible, ai, practices, development, ai, ...   \n",
              "3  [responsible, use, machine, learning, aws, pro...   \n",
              "4  [ai, broad, topic, encompassing, many, differe...   \n",
              "\n",
              "                                   Main_text_stemmed  \n",
              "0  [respons, ai, principl, practic, content, resp...  \n",
              "1  [adob, ’, commit, ai, ethic, adob, purpos, ser...  \n",
              "2  [respons, ai, practic, develop, ai, creat, new...  \n",
              "3  [respons, use, machin, learn, aw, proud, suppo...  \n",
              "4  [ai, broad, topic, encompass, mani, differ, fa...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d495e6b-9b22-4fd0-8f88-97429b9d3137\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Company Name</th>\n",
              "      <th>Document Name</th>\n",
              "      <th>Main Text</th>\n",
              "      <th>Main_Text_Processed</th>\n",
              "      <th>Main_Text_Tokenized</th>\n",
              "      <th>Main_text_without_stopwords</th>\n",
              "      <th>Main_text_stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Accenture</td>\n",
              "      <td>Responsible AI From principles to practice</td>\n",
              "      <td>Responsible AI\\r\\nFrom principles to practice\\...</td>\n",
              "      <td>responsible ai\\r\\nfrom principles to practice\\...</td>\n",
              "      <td>[responsible, ai, from, principles, to, practi...</td>\n",
              "      <td>[responsible, ai, principles, practice, conten...</td>\n",
              "      <td>[respons, ai, principl, practic, content, resp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adobe</td>\n",
              "      <td>Adobe’s Commitment to AI Ethics</td>\n",
              "      <td>Adobe’s Commitment to AI Ethics\\r\\nAt Adobe, o...</td>\n",
              "      <td>adobe’s commitment to ai ethics\\r\\nat adobe ou...</td>\n",
              "      <td>[adobe, ’, s, commitment, to, ai, ethics, at, ...</td>\n",
              "      <td>[adobe, ’, commitment, ai, ethics, adobe, purp...</td>\n",
              "      <td>[adob, ’, commit, ai, ethic, adob, purpos, ser...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Alphabet</td>\n",
              "      <td>Responsible AI practices</td>\n",
              "      <td>Responsible AI practices\\r\\nThe development of...</td>\n",
              "      <td>responsible ai practices\\r\\nthe development of...</td>\n",
              "      <td>[responsible, ai, practices, the, development,...</td>\n",
              "      <td>[responsible, ai, practices, development, ai, ...</td>\n",
              "      <td>[respons, ai, practic, develop, ai, creat, new...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Amazon</td>\n",
              "      <td>Responsible Use of Machine Learning</td>\n",
              "      <td>Responsible Use of Machine Learning\\r\\nAt AWS,...</td>\n",
              "      <td>responsible use of machine learning\\r\\nat aws ...</td>\n",
              "      <td>[responsible, use, of, machine, learning, at, ...</td>\n",
              "      <td>[responsible, use, machine, learning, aws, pro...</td>\n",
              "      <td>[respons, use, machin, learn, aw, proud, suppo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Atos</td>\n",
              "      <td>The Atos Blueprint for Responsible AI</td>\n",
              "      <td>AI is a broad topic encompassing many differen...</td>\n",
              "      <td>ai is a broad topic encompassing many differen...</td>\n",
              "      <td>[ai, is, a, broad, topic, encompassing, many, ...</td>\n",
              "      <td>[ai, broad, topic, encompassing, many, differe...</td>\n",
              "      <td>[ai, broad, topic, encompass, mani, differ, fa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d495e6b-9b22-4fd0-8f88-97429b9d3137')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0d495e6b-9b22-4fd0-8f88-97429b9d3137 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0d495e6b-9b22-4fd0-8f88-97429b9d3137');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMidbq6b3tRS"
      },
      "source": [
        "** **\n",
        "#Step 4: Measure Text Similarity\n",
        "** **\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create BERT-based Text Similaarity Scoring Model"
      ],
      "metadata": {
        "id": "EEpILcO9LnGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db43e48-3736-48d3-f6da-709bd6b8c127",
        "id": "2VTA9HUD__OE"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.8/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.1.97)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (4.26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dataset with only Company Name & Main Text of the AI Ethics Principle Document\n",
        "similarity_data = text_data[['Company Name','Main Text']]\n",
        "sentences = similarity_data['Main Text'].values.tolist()"
      ],
      "metadata": {
        "id": "pY2z_qE4Jq_g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load UNESCO's AI Ethics Principles Dataset\n",
        "url_2 = (\"https://raw.githubusercontent.com/Kensuzuki95/Corporate_AI_Ethics_Guideline_Analysis/main/Dataset/UNESCO_AI_Ethics_Principles.csv\")\n",
        "download = requests.get(url_2).content\n",
        "\n",
        "principles = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
        "principles.head()"
      ],
      "metadata": {
        "id": "xr8ZpwfSM37P",
        "outputId": "50980bcd-eae3-4583-af6b-0ab1acb469d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   No.                         Principle Name  \\\n",
              "0    1         Proportionality and Do No Harm   \n",
              "1    2                    Safety and security   \n",
              "2    3        Fairness and non-discrimination   \n",
              "3    4                         Sustainability   \n",
              "4    5  Right to Privacy, and Data Protection   \n",
              "\n",
              "                                             Content  \n",
              "0  It should be recognized that AI technologies d...  \n",
              "1  Unwanted harms (safety risks), as well as vuln...  \n",
              "2  AI actors should promote social justice and sa...  \n",
              "3  The development of sustainable societies relie...  \n",
              "4  Privacy, a right essential to the protection o...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30669879-9d38-4236-9690-75523931e52b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No.</th>\n",
              "      <th>Principle Name</th>\n",
              "      <th>Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Proportionality and Do No Harm</td>\n",
              "      <td>It should be recognized that AI technologies d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Safety and security</td>\n",
              "      <td>Unwanted harms (safety risks), as well as vuln...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Fairness and non-discrimination</td>\n",
              "      <td>AI actors should promote social justice and sa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Sustainability</td>\n",
              "      <td>The development of sustainable societies relie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Right to Privacy, and Data Protection</td>\n",
              "      <td>Privacy, a right essential to the protection o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30669879-9d38-4236-9690-75523931e52b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-30669879-9d38-4236-9690-75523931e52b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-30669879-9d38-4236-9690-75523931e52b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principle 1"
      ],
      "metadata": {
        "id": "wg7VVABTHo3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract UNESCO's AI Ethics Princilple #1\n",
        "principle_1 = principles.iloc[0]['Content']\n",
        "principle_1"
      ],
      "metadata": {
        "id": "Ug8jUGUVNIhK",
        "outputId": "b0567f7b-6aff-492d-e2a9-fc5591e7ea69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It should be recognized that AI technologies do not necessarily, per se, ensure human and environmental and ecosystem flourishing. Furthermore, none of the processes related to the AI system life cycle shall exceed what is necessary to achieve legitimate aims or objectives and should be appropriate to the context. In the event of possible occurrence of any harm to human beings, human rights and fundamental freedoms, communities and society at large or the environment and ecosystems, the implementation of procedures for risk assessment and the adoption of measures in order to preclude the occurrence of such harm should be ensured.\\nThe choice to use AI systems and which AI method to use should be justified in the following ways: (a) the AI method chosen should be appropriate and proportional to achieve a given legitimate aim; (b) the AI method chosen should not infringe upon the foundational values captured in this document, in particular, its use must not violate or abuse human rights; and (c) the AI method should be appropriate to the context and should be based on rigorous scientific foundations. In scenarios where decisions are understood to have an impact that is irreversible or difficult to reverse or may involve life and death decisions, final human determination should apply. In particular, AI systems should not be used for social scoring or mass surveillance purposes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert UNESCO's AI Ethics Princilple #1 to list of AI Ethics Guideline texts\n",
        "\n",
        "sentences_1 = sentences\n",
        "sentences_1.insert(0, principle_1) # Do not run this line more than once to avoid errors \n",
        "#sentences_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW2VzxAW4V_z",
        "outputId": "d35682f1-4439-48b7-9865-824a4c3ded69"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It should be recognized that AI technologies do not necessarily, per se, ensure human and environmental and ecosystem flourishing. Furthermore, none of the processes related to the AI system life cycle shall exceed what is necessary to achieve legitimate aims or objectives and should be appropriate to the context. In the event of possible occurrence of any harm to human beings, human rights and fundamental freedoms, communities and society at large or the environment and ecosystems, the implementation of procedures for risk assessment and the adoption of measures in order to preclude the occurrence of such harm should be ensured.\\nThe choice to use AI systems and which AI method to use should be justified in the following ways: (a) the AI method chosen should be appropriate and proportional to achieve a given legitimate aim; (b) the AI method chosen should not infringe upon the foundational values captured in this document, in particular, its use must not violate or abuse human rights; and (c) the AI method should be appropriate to the context and should be based on rigorous scientific foundations. In scenarios where decisions are understood to have an impact that is irreversible or difficult to reverse or may involve life and death decisions, final human determination should apply. In particular, AI systems should not be used for social scoring or mass surveillance purposes.',\n",
              " \"Responsible AI\\r\\nFrom principles to practice\\r\\nContents\\r\\nResponsible AI in practice—\\r\\nessential but not easy\\r\\n03\\r\\n• A growing imperative\\r\\n• Practitioners’ insights—the realities of Responsible AI\\r\\n05 Moving from principles to practice\\r\\n• The four pillars of Responsible AI\\r\\n• Pain points, recommendations and case studies\\r\\n• Organizational\\r\\n• Operational\\r\\n• Technical\\r\\n• Reputational\\r\\n14 The next step—from practice to proof\\r\\nArtificial intelligence (AI) has the potential to add trillions to the\\r\\nglobal economy. And the benefits it can deliver to the enterprise\\r\\nare well documented. However, Accenture research found that to\\r\\ndeliver on the promise of this technology, AI must be scaled across\\r\\nthe organization.\\r\\nResponsible AI 03\\r\\nSuccessfully scaling AI for business value requires a number of key factors. These include a focus on\\r\\nvalue, new-skilling and scaling to production. But often, a stumbling block that organizations struggle\\r\\nto overcome is the significant uncertainty and risks associated with AI. Trust inside and outside of the\\r\\norganization is a key component to getting to value from AI. And to get to trust, organizations must\\r\\nmove beyond defining Responsible AI principles and put those principles into practice.\\r\\nAccenture has worked with organizations worldwide to build this trust. How? By defining and\\r\\nimplementing solutions across four Responsible AI pillars—moving from principles to practice.\\r\\nIn this report we share what we have learned—from practitioners’ pain points and how to address\\r\\nthem, to case studies of what good looks like in the real world.\\r\\nFrom principles to practice\\r\\nThe potential value organizations can achieve through AI is clear. So is the danger of being left behind\\r\\nby competitors if the technology is not leveraged. But many businesses feel overwhelmed trying to\\r\\ndetermine how to address the risks associated with it. In a global survey of risk managers, 58% identify\\r\\nAI as the biggest potential cause of unintended consequences over the next two years. Only 11%\\r\\ndescribe themselves as fully capable of assessing the risks associated with adopting AI\\r\\norganization-wide.\\r\\nOne of the main reasons for this crisis of confidence? The way AI shifts the risk landscape in new and\\r\\nunexpected ways for organizations. As AI decisions increasingly influence and impact people’s lives at\\r\\nscale, so responsibility on enterprises increases to manage the potential ethical and socio-technical\\r\\nimplications of AI adoption.\\r\\nIn particular, bias, discrimination and fairness have emerged as areas of paramount concern, alongside\\r\\nexplainability. Recent academic advances in algorithmic fairness focus on a variety of very specific\\r\\ndefinitions of these problem areas, which can be highly contextual, mutually exclusive and\\r\\ncontradictory. Translating these into action is fraught with tough decisions, trade-offs and\\r\\napplication-specific constraints, requiring multiple cross-domain perspectives to reach consensus.\\r\\nResponsible AI in practice—essential but\\r\\nnot easy\\r\\nA growing imperative\\r\\n\\r\\nResponsible AI From principles to practice 04\\r\\nShareholders, regulators, media, employees and the public are increasingly aware of the\\r\\npositive impact that scaling AI can have. But it is also clear that the potential for significant\\r\\ndamage exists if Responsible AI isn’t included in an organization’s approach.\\r\\nIn response, many enterprises have started to act\\r\\n(or in other words, to Professionalize their approach\\r\\nto AI and data). Those that have put in place the right\\r\\nstructures from the start, including considering\\r\\nResponsible AI, are able to scale with confidence,\\r\\nachieving nearly three times the return on their AI\\r\\ninvestments when compared to those that have not.\\r\\nHowever, there are also multiple examples of\\r\\norganizations which have taken initial steps to\\r\\nestablish AI ethics principles and well-meaning\\r\\nproofs of concept but struggle to scale these\\r\\nconcepts into their live processes. Which begs\\r\\nthe questions: why is it so hard to go from\\r\\nprinciples to practice? What are the common\\r\\npitfalls and how can we address them?\\r\\nResponsible AI\\r\\nWe define Responsible\\r\\nAI as the practice of\\r\\ndesigning, building\\r\\nand deploying\\r\\nAI in a manner that\\r\\nempowers employees\\r\\nand businesses\\r\\nand fairly impacts\\r\\ncustomers and society.\\r\\nTo better understand why organizations struggle to move from principles to practice, Accenture\\r\\nconducted a global Responsible AI Practitioners' Survey—26 in-depth interviews with Responsible AI\\r\\npractitioners from 19 organizations, across four continents. We spoke with technological practitioners\\r\\n(data scientists and AI engineers), lawyers, industrial/organizational psychologists, and project\\r\\nmanagers, all of whom are charged with implementing Responsible AI practices in their organizations.\\r\\nOur analysis indicates that given the embryonic nature of Responsible AI, some organizations\\r\\nhave struggled to develop a systematic internal approach to convert their principles into practice.\\r\\nOur experience shows that this is because they underestimate the technical complexity and\\r\\nscale of people and process change required.\\r\\nApproximately 75% of the Responsible AI projects referred to in these interviews were more than a year\\r\\nold. Yet, in almost all cases, efforts had stagnated or remained incomplete due to a range of problems.\\r\\nIn the report, we will analyze the problems we uncovered from the survey results, and provide\\r\\nrecommendations for how to move forward.\\r\\nPractitioners’ insights—the realities of Responsible AI\\r\\nResponsible AI From principles to practice 05\\r\\nTo successfully move from Responsible AI principles to practice, organizations need to tackle a central\\r\\nchallenge: translating ethical principles and academic theories like algorithmic fairness into practical,\\r\\nmeasurable metrics and thresholds that are right for them. To embed these responsible approaches\\r\\nand metrics into everyday practices, organizations will also need to put in place the necessary\\r\\norganizational, technical, operational, and reputational scaffolding.\\r\\nWe’ve defined four pillars of successful Responsible AI implementations, based on what we’ve learned\\r\\ndelivering Responsible AI solutions to organizations across the globe.\\r\\nMoving from principles to practice\\r\\n\\x1fe four pillars of Responsible AI\\r\\nPain points, recommendations and case studies\\r\\nFor each pillar of Responsible AI, we have summarized the key practitioner pain points from our survey,\\r\\nprovided recommendations to address these pain points based on our implementation experience,\\r\\nand included a case study to demonstrate what good looks like.\\r\\nIn our experience, many organizations begin their journey by focusing on one issue, such as\\r\\nalgorithmic fairness or compliance. However, those that are most successful understand the\\r\\nimportance of investing in all four pillars that underpin Responsible AI from the very start.\\r\\nDemocratize the new way\\r\\nof working and facilitate\\r\\nhuman + machine\\r\\ncollaboration.\\r\\nOrganizational\\r\\n01\\r\\nSet up governance and\\r\\nsystems that will enable\\r\\nAI to flourish.\\r\\nOperational\\r\\n02\\r\\nHelp ensure systems\\r\\nand platforms are\\r\\ntrustworthy and\\r\\nexplainable by design.\\r\\nTechnical\\r\\nArticulate the Responsible\\r\\nAI mission and ensure\\r\\nit’s anchored to your\\r\\ncompany’s values,\\r\\nethical guardrails, and\\r\\naccountability structure.\\r\\nReputational\\r\\n03 04\\r\\nThe four pillars of Responsible AI\\r\\n01 Organizational\\r\\nPractitioner pain points\\r\\nPractitioner interviewees regularly highlighted the need for\\r\\nappropriate performance metrics and recognition of their\\r\\nResponsible AI work. They indicated that little value was being\\r\\nplaced today on risk mitigation, including prevention of reputational\\r\\nharm, while time pressures meant short-term product success\\r\\nwas prioritized ahead of the long-term benefits of Responsible AI.\\r\\nThis often came from the top, with interviewees highlighting an\\r\\nunwillingness by leadership to recognize Responsible AI failures\\r\\nand deeply engage with issues like lack of fairness, transparency,\\r\\nand the potential for discrimination.\\r\\nInterviewees felt that as organizations create new Responsible\\r\\nAI duties and roles, a culture of confidence must be built across\\r\\nthe enterprise, empowering employees to raise concerns and act\\r\\nethically through the right combination of responsible product\\r\\nand individual success metrics and incentive structures.\\r\\nWhat we recommend\\r\\nStrong leadership is pivotal in empowering employees, elevating\\r\\nResponsible AI as a key business imperative. To democratize this\\r\\nnew way of working, successful organizations recognize the need\\r\\nfor new and changing roles, and actively upskill, re-skill, or hire.\\r\\nIn our experience, organizations should actively create and\\r\\nencourage an organizational culture that empowers individuals to\\r\\nraise doubts or concerns with AI systems, without stifling innovation.\\r\\nEstablishing clear success criteria, incentives and training helps to\\r\\nnurture these new roles and skills, diffusing a Responsible AI culture\\r\\nacross the organization and cultivating trust in AI systems.\\r\\nResponsible AI From principles to practice 06\\r\\nResponsible AI From principles to practice 07\\r\\nLeadership plays an essential role in the successful\\r\\nadoption of Responsible AI. A leading European\\r\\nfinancial services organization wanted to explore the\\r\\nuse of algorithmic fairness across their model\\r\\ninventory. Strong buy-in and support at the executive\\r\\nlevel enabled a culture of openness and\\r\\ncollaboration, where an internal, multidisciplinary\\r\\nteam were free to work with Accenture to learn and\\r\\nexplore algorithmic fairness, raising any concerns in\\r\\na safe environment.\\r\\nA strong focus was initially placed on education\\r\\nand upskilling, ensuring all team members fully\\r\\nunderstood algorithmic fairness, the choices\\r\\nand trade-offs that need to be made, and how to\\r\\nutilize Accenture’s Algorithmic Assessment toolkit.\\r\\nThis foundation of support and knowledge removed\\r\\nmany of the fears and orthodoxies that often\\r\\nexist around bias and fairness, creating an open,\\r\\ncollaborative culture. Individuals were empowered\\r\\nto ask the hard questions, with potential sources\\r\\nof bias thoroughly investigated. In undertaking this\\r\\nprocess, the organization now has the training and\\r\\nskills needed to implement the latest advances in\\r\\nthe algorithmic fairness space.\\r\\nPutting organizational\\r\\nprinciples into practice\\r\\nCase study\\r\\nPractitioner pain points\\r\\nPractitioners interviewed as part of our research indicated that\\r\\ncompanies consistently struggled with stakeholder misalignment,\\r\\nfrustrating bureaucracy, conflicting agendas and a lack of clarity\\r\\non processes or ownership. For example, individuals operated in an\\r\\nad hoc nature based on their own values and personal assessment\\r\\nof relative importance. A lack of clarity on governance and\\r\\naccountability structures, undue conflict and competing\\r\\nincentives across groups ultimately led to Responsible AI\\r\\ninertia and a reactive mindset.\\r\\nWhat we recommend\\r\\nEffective organizations establish transparent, cross-domain,\\r\\ngovernance structures. These structures identify roles,\\r\\nexpectations and accountability to build internal confidence and\\r\\ntrust in AI technologies. Throughout the project lifecycle, these\\r\\norganizations transform ethical principles into clear processes,\\r\\nprocedures and chains of command that respect the\\r\\ncontext-specific needs of each application.\\r\\nAccenture has found that the creation of a cross-domain ethics\\r\\ncommittee at an early stage has been invaluable for some\\r\\norganizations. In clearly defining roles and expertise, ways of\\r\\nworking and authority to govern, procedures can be maintained on\\r\\nan ongoing basis, while also enabling on-demand responses as\\r\\nissues arise.\\r\\n02 Operational\\r\\nResponsible AI From principles to practice 08\\r\\nResponsible AI From principles to practice 09\\r\\nPutting operational principles into practice\\r\\nFollowing the release of EU AI Trustworthy Guidelines, one global communications vendor asked\\r\\nAccenture to help develop their own internal ethical principles and translate them into operational\\r\\nactions, activities, and structures. This would give them a practical Responsible AI risk management\\r\\nfoundation on which to build their future AI pipeline with confidence.\\r\\nOver the course of seven weeks, Accenture helped to define a governance framework,\\r\\nwith four key deliverables:\\r\\nIn particular, the playbook enabled our client to navigate the difficult path of principles to practice.\\r\\nFor each key activity along a project lifecycle, the playbook makes clear the activities to be completed,\\r\\nthe team members required to take action and the stakeholders responsible for successful completion.\\r\\nEach of these activities are aligned with one or more of the client’s ethical principles, ensuring that the\\r\\nright principles are being considered and acted on accordingly, at each stage. The committee\\r\\nsupports day-to-day activity, resolving any alerts or escalations, while also running check-ins at each\\r\\nphase of the AI lifecycle.\\r\\nResponsible AI\\r\\nprinciples defined\\r\\nand detailed in an\\r\\nactionable playbook.\\r\\nPrinciples and\\r\\nplaybook\\r\\nNew committee\\r\\ncreated with clearly\\r\\ndefined roles, ways of\\r\\nworking and authority\\r\\nto govern.\\r\\nResponsible AI\\r\\ncommi\\x1eee\\r\\nPersonalized training\\r\\nplans built for all team\\r\\nmembers based on their\\r\\nroles and Responsible AI\\r\\nresponsibilities.\\r\\nTraining\\r\\nStructures and channels\\r\\nestablished to safely\\r\\nescalate risks and ways\\r\\nto actively foster dissent.\\r\\nMonitoring\\r\\nCase study\\r\\nPractitioner pain points\\r\\nOne of the biggest barriers that practitioner’s organizations faced\\r\\nwas a lack of expertise in defining and measuring ethical use and\\r\\nalgorithmic impact of data, models and model outcomes on an\\r\\nongoing basis. Without established technical methods to identify,\\r\\nmitigate and monitor these risks, organizations cannot be confident\\r\\nthat the system is fair and safe.\\r\\nInterviewees also indicated that their organizations struggled to\\r\\nintegrate academic metrics like algorithmic fairness, finding them\\r\\nvery different from traditional benchmarks and KPIs. Responsible AI\\r\\ncannot be measured in revenue generation or click-through rates,\\r\\nyet organizations still relied on performance metrics like these to\\r\\ndefine the success or failure of Responsible AI practitioners.\\r\\nWhat we recommend\\r\\nSuccessful organizations architect and deploy AI models, systems\\r\\nand platforms that are trustworthy, fair and explainable by design.\\r\\nUsing proven qualitative and quantitative techniques to assess\\r\\npotential risks, they are better placed to reach cross-domain\\r\\nconsensus on mitigation strategies. They also clearly define\\r\\nmeasurable performance metrics and establish techniques for\\r\\nongoing monitoring, control and re-assessment.\\r\\nIt is important for organizations to invest time in fully understanding\\r\\nthe sources of bias in their various systems. In our experience, this\\r\\nprocess leads to better informed resolution strategies that match\\r\\nthe organization and the application. Having the right set of tools\\r\\nto thoroughly investigate sources of bias and understand the\\r\\ntrade-offs and impacts of fairness decisions is invaluable in\\r\\nreaching common cross-domain consensus.\\r\\nResponsible AI From principles to practice 10\\r\\n03 Technical\\r\\nResponsible AI From principles to practice 11\\r\\nPutting technical\\r\\nprinciples into practice\\r\\nAllied Irish Bank (AIB), wanted to ensure they were\\r\\nahead of the industry and bring their data-science\\r\\nteams rapidly up to speed on the latest developments\\r\\nin algorithmic fairness, while further enhancing the\\r\\nintegration of algorithmic fairness assessment in\\r\\nthe models used to aid their decision-making.\\r\\nWorking with the bank’s data science team, we leveraged\\r\\nAccenture’s Algorithmic Assessment toolkit to assess\\r\\nfairness and actions needed to mitigate bias in two\\r\\nnew models that were in development.\\r\\nAt relevant points in the model development workflow,\\r\\nanalyses were surfaced to a multidisciplinary group. By\\r\\nbreaking this complex problem down into manageable,\\r\\nunderstandable “chunks”, the tools gave data scientists\\r\\nand business executives a deeper understanding of their\\r\\ndata and model outcomes from a fairness perspective.\\r\\nThey then directed further investigation into areas of\\r\\npotential bias. This methodology informed and improved\\r\\ndecision-making during the model-build process.\\r\\nWe enabled AIB to integrate a data-driven assessment\\r\\nof the complex problem of algorithmic fairness in the\\r\\nend-to-end model lifecycle. The bank’s data science\\r\\nteams are now self-sufficient on the tool, and are able\\r\\nto independently use it in their ongoing work to affirm\\r\\nconfidence and a deeper understanding of their models.\\r\\nCase study\\r\\nPractitioner pain points\\r\\nWithout the necessary organizational, operational, and technical\\r\\nfoundations in place, interviewees reported that organizations are\\r\\nforced into a reactive approach to Responsible AI, leaving them\\r\\nexposed to significant reputational damage. Interviewees indicated\\r\\nthat in the absence of clear legal requirements, they had used\\r\\nreputational risk (in the form of catastrophic media attention)\\r\\nas a way of incentivizing change.\\r\\nWhat we recommend\\r\\nLeaders in this field clearly articulate their Responsible Business\\r\\nmission, anchored in their principles and informed by brand,\\r\\npublic risk assessments and guidance. Accurate, ongoing\\r\\nmeasurement and monitoring of key Responsible AI metrics—\\r\\nsuch as fairness—ensures they are managing risk and able to\\r\\ncommunicate with confidence and transparency.\\r\\nIn some cases, internal stakeholders can be skeptical of the value of\\r\\nethical principles or risk averse in communicating these externally.\\r\\nSuccessful organizations embrace these internal skeptics and the\\r\\nfresh perspective they bring, encouraging the core team to\\r\\npressure-test the principles they’re defining. The result is often\\r\\na more considered, precise set of principles in which everyone\\r\\nhas confidence. Responsible AI is a cross-domain challenge\\r\\nand consensus is vital to build a culture of confidence and\\r\\nenable trust in the technology.\\r\\nResponsible AI From principles to practice 12\\r\\n04 Reputational\\r\\nResponsible AI From principles to practice 13\\r\\nPutting reputational principles into practice\\r\\nEstablishing a Responsible AI approach that is robust, fair and maintained on an ongoing\\r\\nbasis can also enable organizations to communicate and collaborate with confidence.\\r\\nA major government agency wanted to apply an “ethics-by-design” approach to the algorithms\\r\\nit utilizes to serve citizens. The agency wanted to have a clear view of potential bias present in\\r\\nthe data, model, and output to identify improvement points and make sure the algorithms it\\r\\nused were treating all people in a fair way.\\r\\nWorking together, we established an end-to-end approach to Responsible AI, combining\\r\\nour Algorithmic Assessment toolkit with strong governance structures, ensuring continued\\r\\nmonitoring and evaluation of fairness over time. A multidisciplinary team was also established\\r\\nto ensure adherence to ethical principles, as well as serving as a mechanism for escalation of\\r\\nethical issues, should they ever arise. In taking a Responsible AI approach, the agency was also\\r\\nable to establish new ethics roles to create internal and external awareness, foster trust and\\r\\nencourage communication and collaboration.\\r\\nCase study\\r\\nResponsible AI From principles to practice 14\\r\\nThe value of AI is clear. But it can bring with it new, dynamic, ethical and social issues. A failure to\\r\\nmanage these issues can have a significant impact at a human and societal level, leaving organizations\\r\\nexposed to financial, legal, and reputational repercussions.\\r\\nWhile many organizations have taken the first step and defined AI principles, translating these into\\r\\npractice is far from easy, especially with few standards or regulations to guide them. Our global\\r\\nResponsible AI Practitioner's Survey identified a range of organizational, operational, technical and\\r\\nreputational challenges that hold well-intentioned organizations back. While the initial focus is often on\\r\\nethical and legal requirements, success is also a function of an organization’s ability to modify its\\r\\ntraditional ways of working to support Responsible AI—and AI more broadly.\\r\\nIn our experience, successful organizations understand the importance of taking a systematic\\r\\napproach from the start, addressing these challenges in parallel, while others underestimate the scale\\r\\nand complexity of change required. A systematic approach requires proven tools, frameworks and\\r\\nmethodologies, enabling the organizations to move from principles to practice with confidence and\\r\\nsupporting the professionalization of AI.\\r\\nIn undertaking this process organizations also establish the structures needed to demonstrate the\\r\\nlong-term value of Responsible AI by scaling it across the organization, enabling the essential move\\r\\nfrom “practice to proof.”\\r\\nWe use a set of 25 questions to help our clients to benchmark their motivators and challenges,\\r\\ntogether with their maturity in terms of people, process and technology against their peers. Where are\\r\\nyou on your Responsible AI journey?\\r\\nThe next step? From practice to proof\\r\\nAbout Accenture\\r\\nAccenture is a global professional services company with leading\\r\\ncapabilities in digital, cloud and security. Combining unmatched\\r\\nexperience and specialized skills across more than 40 industries,\\r\\nwe offer Strategy and Consulting, Interactive, Technology and\\r\\nOperations services—all powered by the world’s largest network of\\r\\nAdvanced Technology and Intelligent Operations centers. Our\\r\\n514,000 people deliver on the promise of technology and human\\r\\ningenuity every day, serving clients in more than 120 countries.\\r\\nWe embrace the power of change to create value and shared\\r\\nsuccess for our clients, people, shareholders, partners and\\r\\ncommunities. Visit us at www.accenture.com.\\r\\nFollow @AccentureAI\\r\\nAuthors\\r\\nThis document is produced by consultants at Accenture as general guidance.\\r\\nIt is not intended to provide specific advice on your circumstances. If you\\r\\nrequire advice or further details on any matters referred to, please contact\\r\\nyour Accenture representative.\\r\\nThis document makes descriptive reference to trademarks that may be\\r\\nowned by others. The use of such trademarks herein is not an assertion of\\r\\nownership of such trademarks by Accenture and is not intended to represent\\r\\nor imply the existence of an association between Accenture and the lawful\\r\\nowners of such trademarks.\\r\\nCopyright © 2021, Accenture. All rights reserved.\\r\\nAccenture and its logo are trademarks of Accenture.\\r\\nRay Eitel-Porter\\r\\nManaging Director, Applied Intelligence\\r\\nGlobal Lead for Responsible AI\\r\\nray.eitel-porter@accenture.com\\r\\nMedb Corcoran\\r\\nManaging Director, Accenture Labs\\r\\nGlobal Responsible AI Lead for Tech Innovation\\r\\nmedb.corcoran@accenture.com\\r\\nPatrick Connolly\\r\\nResearch Manager\\r\\nResponsible AI\\r\\npatrick.connolly@accenture.com\\r\\nContributors\\r\\nBogdana Rakova\\r\\nArtificial Intelligence Consultant, Responsible AI\\r\\nAnika Mahajan\\r\\nResponsible AI – Technical & ASEAN Lead\\r\\nAbout Applied Intelligence\\r\\nApplied Intelligence is Accenture’s approach to scaling AI for our\\r\\nclients. We embed AI-powered data, analytics and automation\\r\\ncapabilities into business workflows to accelerate time to value. Our\\r\\nexpertise in defining end to-end strategy, combined with deep data\\r\\ninfrastructure capabilities, cognitive services and industrialized\\r\\naccelerators help smooth clients’ path to AI adoption, extending\\r\\nhuman capabilities and supporting clients in scaling AI responsibly.\\r\\nRecognized as a leader by industry analysts, we collaborate with a\\r\\npowerful global alliance, innovation and delivery network to help\\r\\nclients deploy and scale AI within any market and industry.\\r\\nVisit us at www.accenture.com/appliedintelligence.\\r\\nAbout Accenture Labs\\r\\nAccenture Labs incubates and prototypes new concepts through\\r\\napplied R&D projects that are expected to have a significant impact\\r\\non business and society. Our dedicated team of technologists and\\r\\nresearchers work with leaders across the company and external\\r\\npartners to imagine and invent the future. Accenture Labs is located\\r\\nin seven key research hubs around the world: San Francisco, CA;\\r\\nSophia Antipolis, France; Washington, D.C.; Shenzhen, China;\\r\\nBangalore, India; Herzliya, Israel and Dublin, Ireland; and 25 Nano Labs\\r\\nThe Labs collaborates extensively with Accenture’s network of nearly\\r\\n400 innovation centers, studios and centers of excellence located in\\r\\n92 cities and 35 countries globally to deliver cutting-edge research,\\r\\ninsights and solutions to clients where they operate and live. For\\r\\nmore information, please visit www.accenture.com/labs.\\r\\nAbout Accenture Research\\r\\nAccenture Research shapes trends and creates data driven\\r\\ninsights about the most pressing issues global organizations face.\\r\\nCombining the power of innovative research techniques with a deep\\r\\nunderstanding of our clients’ industries, our team of 300 researchers\\r\\nand analysts spans 20 countries and publishes hundreds of reports,\\r\\narticles and points of view every year. Our thought-provoking\\r\\nresearch—supported by proprietary data and partnerships with\\r\\nleading organizations, such as MIT and Harvard—guides our\\r\\ninnovations and allows us to transform theories and fresh ideas\\r\\ninto real-world solutions for our clients. For more information,\\r\\nvisit www.accenture.com/research.\",\n",
              " 'Adobe’s Commitment to AI Ethics\\r\\nAt Adobe, our purpose is to serve the creator and respect the consumer, and our heritage is built on providing\\r\\ntrustworthy and innovative solutions to our customers. As our technology becomes more sophisticated, our products\\r\\nand features have the potential to impact our customers in profound and exciting ways. However, we believe we have a\\r\\nrole that goes beyond creating the world’s best technology. We are committed to ensuring that our technology and the\\r\\nuse of our technology benefits society. At Adobe, as we innovate and harness the power of AI in our tools, we are\\r\\ndedicated to addressing the harms posed by biased data in the training of our AI. AI Ethics is one of the core pillars of\\r\\nour commitment to Digital Citizenship, a pledge from Adobe to address the consequences of innovation as part of our\\r\\nrole in society.\\r\\nHow AI is used in Adobe’s products\\r\\nWe believe AI will enhance human creativity and drive value from the complex global digital ecosystem. For the\\r\\nCreative Cloud, we are focused on making it easier for everyone to tell their story with simpler and more intuitive tools.\\r\\nAs part of our Digital Experience offerings, Adobe’s enterprise customers use our AI to deliver relevant and meaningful\\r\\ninsights and personalized digital experiences to their end customers. And with Document Cloud, AI-enabled features\\r\\nhelp understand the structure of PDFs to assist the user in viewing, searching, and editing documents on any platform.\\r\\nHowever, we recognize the potential challenges inherent in this powerful technology. AI systems are based on data,\\r\\nand that data can be biased. AI systems trained on biased data can unintentionally discriminate or disparage, or\\r\\notherwise cause our customers to feel less valued. Therefore, we are committed to maintaining a principled and\\r\\nethically sound approach to ensure our work stays aligned with our intended outcomes and consistent with our values.\\r\\nAnd we are actively participating in government discussions around the world to shape AI Ethics regulation for the\\r\\ngood of the consumer and effectiveness in the industry.\\r\\nAI Ethics Principles\\r\\nAt Adobe, we believe responsible AI development is based on the following three principles:\\r\\n• Responsibility: We will approach designing and maintaining our AI technology with thoughtful evaluation and\\r\\ncareful consideration of the impact and consequences of its deployment. We will ensure that we design for\\r\\ninclusiveness and assess the impact of potentially unfair, discriminatory, or inaccurate results, which might\\r\\nperpetuate harmful biases and stereotypes. We understand that special care must be taken to address bias if a\\r\\nproduct or service will have a significant impact on an individual’s life, such as with employment, housing,\\r\\ncredit, and health.\\r\\n• Accountability: We take ownership over the outcomes of our AI-assisted tools. We will have processes and\\r\\nresources dedicated to receiving and responding to concerns about our AI and taking corrective action as\\r\\nappropriate. Accountability also entails testing for and anticipating potential harms, taking preemptive steps to\\r\\nmitigate such harms, and maintaining systems to respond to unanticipated harmful outcomes.\\r\\nPage 2 of 4\\r\\nAdobe’s Commitment to AI Ethics\\r\\n• Transparency: We will be open about, and explain, our use of AI to our customers so they have a clear\\r\\nunderstanding of our AI systems and their application. We want our customers to understand how Adobe uses\\r\\nAI, the value AI-assisted tools bring to them, and what controls and preferences they have available when they\\r\\nengage with and utilize Adobe’s AI-enhanced tools and services.\\r\\nAI development and AI ethical review is still in its infancy. With any such complex topic, errors may occur, but with the\\r\\ncommitment of our engineers and with help from our employees, our products and features will be best-in-class while\\r\\ncontinuing to reflect Adobe’s values.\\r\\nAdobe’s AI Ethics Principles\\r\\nResponsibility\\r\\nWe at Adobe place a high value on taking responsibility for the impact of our company and the innovation we deliver\\r\\nto the world. It stems from taking pride in our work and our dedication to the best possible outcomes. Therefore, we’ve\\r\\ndetermined that responsibility is the critical foundational principle to underpin Adobe’s commitment and efforts toward\\r\\ndeveloping AI. We must understand and address the impact of introducing new technologies such as AI.\\r\\nResponsible development of AI encompasses the following: designing an AI system thoughtfully, evaluating how it\\r\\ninteracts with end users, exercising due diligence to avoid unwanted kinds of bias, and vetting the AI system to\\r\\ndetermine when the behavior of the system is unacceptable. This entails anticipating potential harms, taking\\r\\npreemptive steps to mitigate such harms, measuring and documenting system performance throughout the technology\\r\\nlifecycle, and establishing systems to monitor and respond to unanticipated harmful outcomes.\\r\\nResponsibility and Bias\\r\\nThe behavior of AI features is strongly dependent on the data used to train them. We understand that AI\\r\\nmodels can result in unintended biases in training data, whether produced by the selection of data to include\\r\\nor by the customer actions that produce the data, can result in correspondingly biased behavior in the system.\\r\\nTherefore, we are committed to building and curating AI training sets in order to avoid harmful bias for the\\r\\ninstances where bias perpetuates societal stereotypes which in turn negatively impact people’s lives. However,\\r\\nwe understand that all data has bias; therefore, we are committing to ensuring that the output of our AI\\r\\nsystems are remediated for bias, regardless of the input.\\r\\nAs part of developing and deploying its AI systems, Adobe will seek to mitigate unintended bias related to\\r\\nhuman attributes (e.g. race, gender, color, ethnic or social origin, genetic or identity preservation features,\\r\\nreligion or belief, political belief, geography, income, disability, age, sexual orientation or vocation), and will\\r\\napply a special focus, and strict standards of fairness and inclusiveness, to situations where the outcome would\\r\\nhave an outsized impact on an individual’s life, such as access to information about employment, housing,\\r\\ncredit, and health.\\r\\nOur ultimate goal is to design for inclusiveness rather than exclusion or discrimination. We will also determine\\r\\nwhether the advantages of using AI outweigh the risk of harm of using AI at all.\\r\\nThis notion of fairness, however, does not imply a rigid uniformity of experience across customers, as some of\\r\\nthe most typical AI use cases distinguish between individuals in ordinary and acceptable ways, as in\\r\\ndemographic marketing or personalized product recommendations. Responsible development of AI means\\r\\nusing AI in reasonable ways that accommodate the norms and values of our society.\\r\\nPage 3 of 4\\r\\nAdobe’s Commitment to AI Ethics\\r\\nResponsibility and Adobe’s Digital Media Tools\\r\\nIt is possible, despite reasonable prevention efforts, that an outside party (customer or otherwise) might make\\r\\nuse of Adobe’s AI technology, such as with our video and photo tools, in a way that results in questioning the\\r\\nauthenticity of content. Adobe feels a responsibility to support the creative community, and society at large,\\r\\nand is committing to contributing to solutions that address the issues of manipulated media.\\r\\nHowever, we believe it is important to be clear about actions to which Adobe is not committing. We do not believe\\r\\nthat we will develop AI that:\\r\\n• Ensures egalitarian fairness, in the sense of providing identical experiences to all users. The value of AI is in its\\r\\nability to differentiate, and as long as the AI is free of unfair bias, enabling customization and personalization of\\r\\ntools, recommendations, and technologies is valuable, important, and welcomed by our users.\\r\\n• Certification that inbound training data is \"unbiased\". All training data encodes bias in one direction or another,\\r\\nas humans create the data point, and humans have inherent bias. We do not believe it is reasonable or useful\\r\\nto commit to a zero-bias starting point; rather, it is more important and effective to commit to having an\\r\\noutcome that mitigates against bias.\\r\\nAccountability\\r\\nAccountability means the commitment to take ownership for the outcomes of our actions. At Adobe, while anyone\\r\\ninvolved with AI has an obligation to help ensure it’s being managed responsibly, business leaders are held accountable\\r\\nfor the ethical operation of Adobe’s AI technologies. We are ensuring processes are in place and resources are\\r\\ndedicated to meet Adobe’s AI Ethics commitments, including to develop and implement the necessary engineering\\r\\npractices to achieve our responsibility goals, receive and respond to internal and external concerns, and to take\\r\\ncorrective action as required.\\r\\nHow We Ensure Accountability:\\r\\n• Establishing governance processes to evaluate and track the performance of AI algorithms, data and designs,\\r\\nincluding labeling datasets and models for any identified bias to ensure remediation can occur at product\\r\\ndesign stage;\\r\\n• Requiring an AI Impact Assessment (as part of our services development process) to ensure an AI ethics review\\r\\nhappens before deployment of new AI technologies;\\r\\n• Creating an Ethics Advisory Board to oversee the promulgation of AI development requirements and be a\\r\\nplace where any AI ethics concerns can be heard, while safeguarding ethical whistleblowers;\\r\\n• Processes to ensure remediation of any negative AI impacts that are discovered after deployment;\\r\\n• Education of engineers and product managers via mandatory training courses on AI ethics issues.\\r\\nPage 4 of 4\\r\\nAdobe’s Commitment to AI Ethics\\r\\nTransparency\\r\\nTransparency is the reasonable public disclosure, in clear and simple language, of how we responsibly develop and\\r\\ndeploy AI within our tools. Adobe values our trusted relationship with our customers and feels that transparency is\\r\\nintegral to that relationship. This includes sharing information on how or whether Adobe collects and uses customer\\r\\nassets and usage data to improve our products and services.\\r\\nTransparency includes the disclosure of the following:\\r\\n• Our data collection practices:\\r\\no When and if individual’s data will be collected for AI training, and what controls a user will have over\\r\\nthe collection\\r\\no Providing notice prior to and if human-review of customer data for AI training will be implemented\\r\\n• Model development:\\r\\no How datasets are used in building AI models\\r\\n• Accountability processes:\\r\\no How Adobe is testing for and resolving issues related to unfair bias.\\r\\n• General disclosure of how data and AI are used in Adobe’s tools and services;\\r\\n• Provide external and internal feedback mechanisms to report concerns on our AI practices',\n",
              " 'Responsible AI practices\\r\\nThe development of AI is creating new opportunities to improve the lives of people around the world, from business to healthcare to education. It is also raising new questions about the best way to build fairness, interpretability, privacy, and security into these systems.\\r\\n\\r\\nThese questions are far from solved, and in fact are active areas of research and development. Google is committed to making progress in the responsible development of AI and to sharing knowledge, research, tools, datasets, and other resources with the larger community. Below we share some of our current work and recommended practices. As with all of our research, we will take our latest findings into account, work to incorporate them as appropriate, and adapt as we learn more over time.\\r\\n\\r\\nExplore our responsible practices:\\r\\nGeneral recommended practices for AI\\r\\nFairness\\r\\nInterpretability\\r\\nPrivacy\\r\\nSecurity\\r\\nBack to top ↑\\r\\nGeneral recommended practices for AI\\r\\nReliable, effective user-centered AI systems should be designed following general best practices for software systems, together with practices that address considerations unique to machine learning. Our top recommendations are outlined below, with additional resources for further reading.\\r\\n\\r\\nRecommended practices\\r\\nUse a human-centered design approach\\r\\nThe way actual users experience your system is essential to assessing the true impact of its predictions, recommendations, and decisions.\\r\\n\\r\\nDesign features with appropriate disclosures built-in: clarity and control is crucial to a good user experience.\\r\\nConsider augmentation and assistance: producing a single answer can be appropriate where there is a high probability that the answer satisfies a diversity of users and use cases. In other cases, it may be optimal for your system to suggest a few options to the user. Technically, it is much more difficult to achieve good precision at one answer (P@1) versus precision at a few answers (e.g., P@3).\\r\\nModel potential adverse feedback early in the design process, followed by specific live testing and iteration for a small fraction of traffic before full deployment.\\r\\nEngage with a diverse set of users and use-case scenarios, and incorporate feedback before and throughout project development. This will build a rich variety of user perspectives into the project and increase the number of people who benefit from the technology.\\r\\nIdentify multiple metrics to assess training and monitoring\\r\\nThe use of several metrics rather than a single one will help you to understand tradeoffs between different kinds of errors and experiences.\\r\\n\\r\\nConsider metrics including feedback from user surveys, quantities that track overall system performance and short- and long-term product heath (e.g., click-through rate and customer lifetime value, respectively), and false positive and false negative rates sliced across different subgroups.\\r\\nEnsure that your metrics are appropriate for the context and goals of your system, e.g., a fire alarm system should have high recall, even if that means the occasional false alarm.\\r\\nWhen possible, directly examine your raw data\\r\\nML models will reflect the data they are trained on, so analyze your raw data carefully to ensure you understand it. In cases where this is not possible, e.g., with sensitive raw data, understand your input data as much as possible while respecting privacy; for example by computing aggregate, anonymized summaries.\\r\\n\\r\\nDoes your data contain any mistakes (e.g., missing values, incorrect labels)?\\r\\nIs your data sampled in a way that represents your users (e.g., will be used for all ages, but you only have training data from senior citizens) and the real-world setting (e.g., will be used year-round, but you only have training data from the summer)? Is the data accurate?\\r\\nTraining-serving skew—the difference between performance during training and performance during serving—is a persistent challenge. During training, try to identify potential skews and work to address them, including by adjusting your training data or objective function. During evaluation, continue to try to get evaluation data that is as representative as possible of the deployed setting.\\r\\nAre any features in your model redundant or unnecessary? Use the simplest model that meets your performance goals.\\r\\nFor supervised systems, consider the relationship between the data labels you have, and the items you are trying to predict. If you are using a data label X as a proxy to predict a label Y, in which cases is the gap between X and Y problematic?\\r\\nData bias is another important consideration; learn more in practices on AI and fairness.\\r\\nUnderstand the limitations of your dataset and model\\r\\nA model trained to detect correlations should not be used to make causal inferences, or imply that it can. E.g., your model may learn that people who buy basketball shoes are taller on average, but this does not mean that a user who buys basketball shoes will become taller as a result.\\r\\nMachine learning models today are largely a reflection of the patterns of their training data. It is therefore important to communicate the scope and coverage of the training, hence clarifying the capability and limitations of the models. E.g., a shoe detector trained with stock photos can work best with stock photos but has limited capability when tested with user-generated cellphone photos.\\r\\nCommunicate limitations to users where possible. For example, an app that uses ML to recognize specific bird species might communicate that the model was trained on a small set of images from a specific region of the world. By better educating the user, you may also improve the feedback provided from users about your feature or application.\\r\\nTest, Test, Test\\r\\nLearn from software engineering best test practices and quality engineering to make sure the AI system is working as intended and can be trusted.\\r\\n\\r\\nConduct rigorous unit tests to test each component of the system in isolation.\\r\\nConduct integration tests to understand how individual ML components interact with other parts of the overall system.\\r\\nProactively detect input drift by testing the statistics of the inputs to the AI system to make sure they are not changing in unexpected ways.\\r\\nUse a gold standard dataset to test the system and ensure that it continues to behave as expected. Update this test set regularly in line with changing users and use cases, and to reduce the likelihood of training on the test set.\\r\\nConduct iterative user testing to incorporate a diverse set of users’ needs in the development cycles.\\r\\nApply the quality engineering principle of poka-yoke: build quality checks into a system, so that unintended failures either cannot happen or trigger an immediate response (e.g., if an important feature is unexpectedly missing, the AI system won’t output a prediction).\\r\\nContinue to monitor and update the system after deployment\\r\\nContinued monitoring will ensure your model takes real-world performance and user feedback (e.g., happiness tracking surveys, HEART framework) into account.\\r\\n\\r\\nIssues will occur: any model of the world is imperfect almost by definition. Build time into your product roadmap to allow you to address issues.\\r\\nConsider both short- and long-term solutions to issues. A simple fix (e.g., blocklisting) may help to solve a problem quickly, but may not be the optimal solution in the long run. Balance short-term simple fixes with longer-term learned solutions.\\r\\nBefore updating a deployed model, analyze how the candidate and deployed models differ, and how the update will affect the overall system quality and user experience.',\n",
              " 'Responsible Use of Machine Learning\\r\\nAt AWS, we are proud to support our customers as they invent, build, and use machine learning (ML) systems\\r\\nto solve real-world problems. We see the transformational nature of ML technology across industries every\\r\\nday. ML techniques can make tasks easier, safer, and more efficient. For example, ML has been used to develop\\r\\ntranscription and translation services, fraud detection software, search and recommendation engines, and tools\\r\\nthat monitor and help protect our environment.\\r\\nGiven the breadth and depth of ML, many customers are asking for perspectives on how to responsibly develop\\r\\nand use ML systems. This document shares some recommendations, examples, and tools that can be used\\r\\nacross three major phases of ML life cycles: (1) design and development; (2) deployment; and (3) ongoing use.\\r\\nAn important preliminary note is that we believe all use of ML must respect the rule of law, human rights,\\r\\nand values of equity, privacy, and fairness. The field of responsible ML is a rapidly developing area, so these\\r\\nrecommendations should be viewed as a starting point and not the final answer. We encourage readers to\\r\\nconsider the spirit and intent behind the recommendations. They should be considered along with third party\\r\\nand AWS tools and resources on responsible development and use of ML systems, such as the ones listed below.\\r\\nWe are also eager to receive feedback, and appreciate the opportunity to contribute to this important topic\\r\\nwhile continuing to learn from the broader community.\\r\\nPhase 1: Design and Development\\r\\nThis phase includes establishing requirements of the ML system, defining performance criteria, exploring the\\r\\npotential impact of the system on users and other parties, collecting and curating training data, and building and\\r\\ntesting models and other system components.\\r\\nEvaluating Use Cases: There are a wide variety of use cases that may incorporate ML, with different goals,\\r\\ncharacteristics, user bases, and potential impacts. Developers should consider the benefits and potential risks of\\r\\ntheir specific use case. Given the broad nature and applicability of ML, many applications may pose limited or no\\r\\nrisk (e.g., movie recommendation systems), while others could involve significant risk, especially if used in a way\\r\\nthat impacts human rights or safety. Examples of risks worth carefully evaluating include: technical limitations\\r\\nof an ML system, over-reliance on limited data or inaccurate output, the potential for bias in training data or\\r\\nthe model itself, and intentional or unintentional misuse, as well as the likelihood and impact of those risks\\r\\nand possible solutions. Potential mitigation options may include detailed documentation, explicit warnings or\\r\\ncontractual restrictions, technical restrictions, or mechanisms to receive and act on feedback.\\r\\nML Capabilities and Limitations: Developers and users should understand the nature, capabilities, and limitations\\r\\nof ML systems, including important concepts like the probabilistic nature of ML, confidence levels, and human\\r\\nreview. Many ML systems predict a possible or likely answer, not the answer itself. The probabilistic nature of\\r\\nML means that use cases that require definitive answers (as opposed to possible or likely answers) may benefit\\r\\nfrom additional guardrails. Consider providing a numeric or other indicator of the confidence level associated\\r\\nwith system output to help users evaluate the output for their use case. Also consider whether human review\\r\\nor oversight of the system may be appropriate, and when it should be required. As an example, if an ML system\\r\\nhelps predict the risk of fraud in online transactions, it may not be appropriate to take output from the system as\\r\\nthe sole indicator of fraud, but as one factor to be analyzed in connection with the overall transaction. In certain\\r\\ncases it may be appropriate for a trained person to review the ML prediction and transaction before any action is\\r\\ntaken. In cases where human review is needed, consider how to provide reviewers the necessary training, context,\\r\\nand interface to take action.\\r\\nBuilding and Training Diverse Teams: It is important to have diverse backgrounds, perspectives, skills, and\\r\\nexperiences on teams that are developing ML systems. Assess whether teams include a wide array of genders,\\r\\nraces, ethnicities, abilities, ages, religions, sexual orientations, military status, backgrounds, and political views.\\r\\nFurther assess whether teams may have gaps and consider adding underrepresented perspectives to fill those\\r\\ngaps to enhance performance. Successful teams will likely have cross-functional expertise (e.g., technologists,\\r\\nacademics, industry experts, lawyers, and other stakeholders) and diverse characteristics to help ensure important\\r\\nperspectives are taken into account. Consider resources such as user testing, focus groups, and/or third party\\r\\nadvocacy groups to obtain additional perspectives from outside parties. There are many public resources, such as\\r\\nthe EU Assessment List for Trustworthy Artificial Intelligence, that can enable deeper analyses on these\\r\\nsubjects.\\r\\nBe Mindful of Overall Impact: Consider the potential impact of an ML system on parties that are not customers or\\r\\ndirect users of the system, but may still be affected. For example, if an autonomous vehicle is not operating as\\r\\nexpected, it could have an impact on passengers, other drivers, pedestrians, or property. Similarly, consider\\r\\nguidelines or restrictions for use of ML systems that determine whether users are eligible for certain services or\\r\\nbenefits, if those users may lose eligibility based on how the ML system’s output is used. \\r\\nData Collection: Consider how you will acquire data to develop and test ML models. For example, data may\\r\\nbe available through open source repositories, through licenses from third party data providers, or already in\\r\\nyour possession. Involve your legal and procurement teams as appropriate to assess the impact of any privacy\\r\\nconsiderations or other relevant laws, license, or contractual requirements that may impact your collection or use\\r\\nof the data. Consider any necessary processes for handling data securely and safely, and ways to mitigate risk.\\r\\nFor example, if certain portions of a data set are sensitive, but are not necessary for development of the model,\\r\\nconsider whether you can discard that content.\\r\\nTraining and Testing Data: When collecting and evaluating data to develop and test models, consider its\\r\\ncompleteness, representativeness, and breadth. Diversity of data is often important for use cases that involve\\r\\npersonal characteristics like race and gender, but can also apply in non-obvious contexts. Develop mechanisms to\\r\\nevaluate whether the data appropriately represents real world use, and collect and test additional data to address\\r\\nunderrepresented attributes. For example, an audio transcription system may need data with different accents,\\r\\nspeech speeds, vernacular, and background environments, and autonomous transport systems may need data\\r\\nfrom different terrains and obstacles (e.g., cobblestones, dirt, and cracked sidewalks). Review data for freshness\\r\\n(data may be outdated and in need of replacement), potential sources of error (inherent to the data itself, in its\\r\\nstructure and organization, or introduced during annotation), and bias (discussed below). It is important to have\\r\\nseparate sets of data for training and testing, but both sets should be complete and representative.\\r\\nBias: Consider ways to maximize accuracy and reduce bias in data, algorithms, and system design. Some\\r\\nsuggestions include:\\r\\n• Staffing development and annotation teams with a diverse set of backgrounds, perspectives, skills,\\r\\nexperiences, and demographics as appropriate for your system’s use case and performance.\\r\\n• Understanding perspectives and potential biases of data annotators and developers, and having processes to\\r\\nmitigate human error (for example, by using annotators familiar with the subject matter, being thoughtful\\r\\nwhen using labels that require subjective versus objective judgment, and checking annotation samples for\\r\\naccuracy). Consider using multiple data annotators and developers to help identify discrepancies.\\r\\n• Creating fairness goals and metrics (including potential minimum acceptable thresholds) to measure\\r\\nperformance across different subgroups, communities, and demographics applicable to the use case, and\\r\\ntesting and measuring progress against those metrics. For example, a speech recognition system may be\\r\\nevaluated for accuracy across different speaker groups by running statistical studies to determine the\\r\\ncorrelation between speaker demographic variables (such as regional accents) and error rates. Consider\\r\\nwhether and how bias is measured in existing processes that do not use ML, and how to use that information to\\r\\nevaluate the effectiveness of the ML system.\\r\\n• Having independent teams help test the system for bias, and considering whether it may be appropriate or\\r\\nfeasible to have external parties perform such evaluations.\\r\\n• Developing plans to remediate potential inaccuracies and bias, which may include evaluating root causes,\\r\\ndeveloping new requirements, acquiring more data, and re-training models.\\r\\n• Considering mechanisms to allow users to evaluate system performance and bias/accuracy using their own\\r\\ndata for their specific use case.\\r\\nExplainability of ML systems: Consider the need to explain the methodology and important factors that influence\\r\\nthe ML system’s output. Mechanisms to help explain complex ML models are still being researched and there is\\r\\ncurrently no “silver bullet” for explainability, but some areas (like explainability of models that use structured\\r\\ntabular data) have progressed further than others and can be used to help explain certain predictions today. The\\r\\nimportance of explainability will vary depending on the use case: many systems that have low or no risk may not\\r\\nrequire explainability, while ML systems whose output may be used in a manner that could impact human rights\\r\\nor safety will likely need a method for determining how the system performed its analysis. If explainability is not\\r\\ntechnically feasible, consider whether other mechanisms, such as human review, auditability (next section), and\\r\\nre-focusing or limiting the scope of the use case might serve as an appropriate alternative.\\r\\nAuditability: Consider the need for implementing mechanisms to track and review steps taken during\\r\\ndevelopment and operation of the ML system, e.g., to trace root causes for problems or meet governance\\r\\nrequirements. Evaluate the need to document relevant design decisions and inputs to assist in such reviews.\\r\\nEstablishing a traceable record can help internal or external teams evaluate the development and functioning of\\r\\nthe ML system.\\r\\nLegal Compliance: Engage with legal advisors to assess requirements for and implications of building your ML\\r\\nsystem. This may include vetting legal rights to use data and models, and determining applicability of laws\\r\\naround privacy, biometrics, anti-discrimination, and other use-case specific regulations. Be mindful of differing\\r\\nlegal requirements across states, provinces, and countries, as well as new AI/ML regulation being considered\\r\\nand proposed around the world. Re-visit legal requirements and considerations through future deployment and\\r\\noperations phases. \\r\\nPhase 2: Deployment\\r\\nThis phase includes preparing and deploying ML systems for use, including understanding and accounting for\\r\\ncapabilities, limitations, and risks associated with deployment.\\r\\nEducation, Documentation and Training: Consider whether users and other stakeholders should be educated\\r\\non topics like the predictive nature of ML, confidence indicators and thresholds, capabilities and limitations of\\r\\nthe system, recommended or prohibited uses, and best practices. As an example, if deploying a conversational\\r\\nchatbot, users should be informed that they are interacting with a computer system and not a real person,\\r\\nto avoid misunderstandings about the nature of the interaction. If using a facial recognition system to assist\\r\\npersonnel in making decisions that could impact a person’s civil liberties or human rights, include appropriate\\r\\ntraining for human reviewers on the nature and proper use of such systems. Consider appropriate mechanisms\\r\\nand processes for carrying out trainings or communications, and the need to provide more educational resources\\r\\naround issues like privacy, safety, transparency, accessibility, inclusiveness, and bias.\\r\\nConfidence Levels and Human Review: As noted earlier, it’s important to understand that many ML systems\\r\\ngenerate predictions of a possible or likely answer, not the answer itself. If confidence indicators are available,\\r\\ntake them into account (or instruct your users to take them into account) when reviewing and taking action using\\r\\noutput provided by the system. For higher risk use cases, be mindful of situations where confidence indicators\\r\\nmay not be appropriately considered and may instead be used as a shortcut to make decisions, and whether\\r\\nsuch behavior can be mitigated. Regardless of confidence levels, consider whether human review or oversight\\r\\nover the operation of the system may be appropriate or necessary (e.g., in situations where ML systems may be\\r\\nused in a manner that impact human rights or safety), and if so, how to best incorporate such human input into\\r\\nthe overall operation of the system. Human reviewers should be appropriately trained on real world scenarios,\\r\\nincluding examples where the system fails to properly process inputs or cannot handle edge cases, and have ways\\r\\nto exercise meaningful oversight.\\r\\nUse Case Evaluation and Testing: Consider whether a particular ML model is appropriate for the use case,\\r\\nincluding any benefits, limitations, and risks. This should be reassessed if the model is used for new use cases, or\\r\\nbeyond the system for which it was designed. It is important to test ML systems in the operational environments\\r\\nand on the data on which they will be deployed before live deployment. Develop metrics and a test plan to\\r\\nmeasure performance of the system against production uses, and consider ongoing tests against a frequently\\r\\nupdated “gold standard” dataset. Testing should include not just the ML system itself but also the overall process\\r\\nit is a part of, including decisions or actions that might be taken based on system output. In some situations, it\\r\\nmay not be appropriate to use the system if testing does not reach a specified accuracy level. In other cases, such\\r\\nas where the system is used for entertainment purposes or to “narrow the field” for additional review or human\\r\\njudgment, accuracy is one variable that should be balanced with other factors, such as the need to generate a\\r\\nlarge number of results. Deployers should also factor in localization requirements when deploying an ML system\\r\\ninto a new use case, region, or geography different from the one for which it was designed and tested -- for\\r\\nexample, real estate pricing models in different geographic areas, or voice recognition systems deployed in areas\\r\\nwith different dialects or accents. \\r\\nNotice and Accessibility: Consider whether to notify end users about the use of ML in the system they are\\r\\ninteracting with, such as the earlier example about notifying users that they are interacting with a chatbot\\r\\nand not a live human. Consider whether it is appropriate or feasible to allow end users to bypass interacting\\r\\nwith the system and offer an alternate method to accomplish the use case -- for example, some users may\\r\\nprefer not to use a facial recognition authentication system and request a different method of authentication.\\r\\nConsult accessibility resources to ensure that the system is actually usable by the target audience and provides\\r\\nappropriate access options to all intended users.\\r\\nOperational Data: Consider the sources of any data used with the ML model once it is deployed. As with data\\r\\nused for training and testing, involve your legal and procurement teams as appropriate to assess the impact of\\r\\nany relevant laws or contractual requirements on operational data. Consider any necessary processes for handling\\r\\ndata securely and safely, and ways to mitigate risk.\\r\\nSafety, Security, and Robustness: Use of the ML system must be safe for both users and third parties. As with\\r\\nany technology, deployers should implement appropriate mechanisms to protect the ML system and associated\\r\\ndata (both inputs and outputs) from loss, attack, vulnerabilities, or unexpected or malicious user behavior. These\\r\\nmechanisms may include limiting potential access to the system, putting in place legal or technical restrictions\\r\\non use, and/or implementing warnings, notices, education and trainings that educate users about risks and\\r\\nconsequences of improper use. Consider how potential inaccuracies in results produced by the ML system may\\r\\nimpact users and relevant stakeholders, and prepare a plan for addressing these inaccuracies, which may involve\\r\\nnarrowing the scope of use, relying on human review or oversight, or altering dependencies on the system.\\r\\nLegal Compliance: As noted in the development phase, it is important to engage your legal advisors to assess\\r\\nlegal requirements arising from your deployment and use of the system. \\r\\nPhase 3: Operation\\r\\nThis phase deals with ongoing operation of the system after it is developed and deployed. Note that many\\r\\nconsiderations and questions from Phases 1 and 2 continue to be relevant.\\r\\nProvide and Use Feedback Mechanisms: Since ML systems can continue to “learn” and improve throughout their\\r\\nlifecycle, an important aspect of improvement involves receiving and incorporating feedback from users and\\r\\nstakeholders. Consider soliciting feedback through programmatic and manual methods, including in-system\\r\\nmechanisms or third party outreach through surveys and focus groups. Keep in mind that not all feedback will be\\r\\nrelevant or actionable, and it may be appropriate to develop and communicate expectations for acknowledging\\r\\nand addressing feedback. If appropriate for the use case (such as if an ML system might be used to help make\\r\\ndecisions on eligibility for important services), consider mechanisms for users or stakeholders to request more\\r\\ninformation about, or obtain remediation for, negative impact arising from how system output is used.\\r\\nContinuous Improvement and Validation: ML is an iterative science. Consider the issues raised in previous phases\\r\\nabout monitoring and testing of your system. ML models can be subject to “concept drift,” where model behavior\\r\\nchanges as a result of changes in users, environments, or data over time. There are multiple ways that models\\r\\nin ML systems can drift, including changes to the use case, operating environment, or types and quality of data.\\r\\nDevelop and run ongoing performance tests, and use these test results and feedback to identify areas where\\r\\nadditional data or development may improve your system’s performance. Be thoughtful about the data being\\r\\nused as inputs and for any further training or tuning of the ML system. Continue to monitor for potential bias\\r\\nand accuracy, including that your models perform as expected across different segments. Consider appropriate\\r\\nadjustments to both the system and overall processes that involve the system, such as updated training, new\\r\\nnotices or restrictions, or optimizing the ways system output is evaluated and used.\\r\\nOngoing Education: ML is a constantly evolving landscape, and new techniques, technologies, laws, and social\\r\\nnorms will continue to be developed and refined over time. It is critical that all parties involved with building and\\r\\nusing ML systems stay educated on these issues and account for them in the design, deployment, and operation\\r\\nof their systems. We encourage all stakeholders in the field, and other interested parties, to contribute knowledge\\r\\nand relay their experiences and learnings to the broader community. \\r\\nTools and Resources\\r\\nAWS offers a large number of tools and resources to help you responsibly develop and use ML systems, including\\r\\nmethods to help address some of the issues above.\\r\\nAmazon SageMaker Clarify provides ML developers with greater visibility into their training data and models so\\r\\nthey can identify and limit bias and explain predictions. Amazon SageMaker Clarify detects potential bias during\\r\\ndata preparation, after model training, and in a deployed model by examining attributes you specify. For instance,\\r\\nyou can check for bias related to age in your initial dataset or in your trained model and receive a detailed report\\r\\nthat quantifies different types of possible bias. SageMaker Clarify also helps you to look at the importance of\\r\\nmodel inputs to explain why models make the predictions they do. SageMaker Clarify includes feature importance\\r\\ngraphs that help you explain model predictions and produces reports which can be used to support internal\\r\\npresentations or to identify issues with your model that you can take steps to correct.\\r\\nAmazon Augmented AI makes it easy to build the workflows required for human review of ML systems. Amazon\\r\\nA2I brings human review to all developers, removing the difficult tasks of building custom human review systems\\r\\nor managing large numbers of human reviewers. As mentioned above, in some situations it may be appropriate\\r\\nto have human oversight over ML systems to help ensure accuracy, provide continuous improvements, or retrain\\r\\nmodels with updated predictions. Amazon A2I streamlines building and managing human reviews for ML\\r\\napplications. Amazon A2I provides built-in human review workflows for common ML use cases, such as content\\r\\nmoderation and text extraction from documents. You can also create your own workflows for ML models built\\r\\non SageMaker or any other tools. Using Amazon A2I, you can allow human reviewers to step in when a model is\\r\\nunable to make a high-confidence prediction or to audit its predictions on an ongoing basis.\\r\\nAmazon SageMaker Model Monitor helps maintain high quality ML models by detecting model and concept drift\\r\\nin real-time, and sending alerts to enable immediate action. Model and concept drift are detected by monitoring\\r\\nthe quality of the model based on independent and dependent variables. Independent variables (also known as\\r\\nfeatures) are the inputs to an ML model, and dependent variables are the outputs of the model. For example, with\\r\\nan ML model predicting a bank loan approval, independent variables could be age, income, and credit history of\\r\\nthe applicant, and the dependent variable would be the actual result of the loan application. SageMaker Model\\r\\nMonitor constantly monitors model performance characteristics such as accuracy, which measures the number of\\r\\ncorrect predictions compared to the total number of predictions, so you can take action to address anomalies.\\r\\nAmazon SageMaker Data Wrangler gives you better control of your training and testing data by simplifying\\r\\nthe process of data preparation and feature engineering. You can complete each step of the data preparation\\r\\nworkflow, including data selection, cleansing, exploration, and visualization from a single visual interface. It\\r\\nalso helps you identify potential errors, extreme values, and inconsistencies in your data preparation workflow\\r\\nthrough visualization templates. Using SageMaker Data Wrangler’s data selection tool, you can choose the data\\r\\nyou want from various data sources and import it with a single click. SageMaker Data Wrangler contains over 300\\r\\nbuilt-in data transformations so you can quickly normalize, transform, and combine features without having to\\r\\nwrite any code.\\r\\nTraining and Professional Services. AWS offers the latest in ML education through the Machine Learning\\r\\nUniversity, Training and Certification program, and AWS ML Embark. You can also work with experts in responsible\\r\\nML within our AWS Professional Services organization to create an operational approach encompassing people,\\r\\nprocesses, and technology to develop and operationalize responsible ML principles based on a proven framework.\\r\\nThis can helps you look around corners, uncover potential unintended impacts, and mitigate risks related to the\\r\\ndevelopment, deployment, and operationalization of ML systems.\\r\\nResearch, Innovation, and External Collaboration. AWS collaborates with academia and other stakeholders\\r\\nthrough strategic partnerships with universities including University of California, Berkeley, MIT, California\\r\\nInstitute of Technology, the University of Washington, and others. We are also active members of multistakeholder organizations relating to AI, including OECD AI working groups and The Partnership on AI. We also\\r\\nprovide research grants through Amazon Research Awards and the joint Amazon and National Science Foundation\\r\\nFairness in AI Grants program. See some of our recent videos and publications related to Responsible AI:\\r\\nAmazon scientist Dr. Nashlie Sephus focuses on ensuring accuracy in machine learning\\r\\nAmazon Scholars Michael Kearns and Aaron Roth discuss the ethics of machine learning\\r\\nHow a paper by three Oxford academics influenced AWS bias and explainability software\\r\\nNine videos about explainable AI in industry\\r\\nRelated Publications From Amazon Science:\\r\\nCorrecting exposure bias for link recommendation\\r\\nFair Bayesian optimization\\r\\nGeneral Fair Empirical Risk Minimization\\r\\nLearning Deep Fair Graph Neural Networks\\r\\nBias preservation in machine learning: The legality of fairness metrics under EU non-discrimination law\\r\\nLearning fair and transferable representations with theoretical guarantees\\r\\nExploiting MMD and Sinkhorn divergences for fair and transferable representation learning\\r\\nTowards unbiased and accurate deferral to multiple experts\\r\\nAmazon SageMaker Clarify: Machine learning bias detection and explainability in the cloud\\r\\nLearning to rank in the position based model with bandit feedback\\r\\nDecoding and diversity in machine translation\\r\\nFairness measures for machine learning in finance\\r\\nFair Bayesian optimization\\r\\nMixed-privacy forgetting in deep networks\\r\\nContinuous compliance\\r\\n© 2022, Amazon Web Services, Inc. or its affiliates. All rights reserved.',\n",
              " 'AI is a broad topic encompassing many different families of algorithms and techniques. However, we currently live in a narrow AI era where AI is used for very specific tasks. Some AI techniques such as machine learning have proven to be more efficient than humans in areas such as computer vision, automated translation, predictions and anomaly detection.\\r\\n\\r\\nDespite the progress, a number of technical challenges remain. There is a huge need for diverse and qualitative data and — in some cases — large volumes of historical data. Furthermore, we need more efficient computing, algorithms and DevOps scalability (e.g. parallelization for algorithms and MLOps tooling for DevOps).\\r\\nBringing the best out of AI\\r\\nAtos defines responsible AI with four key dimensions: fair and ethical, robust and secure, industrialized and eco-sustainable. Atos enables you to boost your business by keeping these foundational elements of responsible AI in mind.\\r\\nThe proposed way in which Atos can boost your business are:\\r\\nEnable AI – design and deliver cost-efficient and secured infrastructure for your AI needs\\r\\nAugment with AI – leverage AI to optimize existing business processes and operations\\r\\nGrow the business with AI – leverage AI to create new business models\\r\\nThe four horsemen of the apocalyptic AI\\r\\nA failure to follow the four dimensions of responsible AI can have a serious business impact. Below are some of the problems and consequences of (non)responsible AI:\\r\\n(Un)fair and (un)ethical — Compliance fines, reputation damage, reduced talent attraction, negative corporate social responsibility impacts\\r\\n(Non)robust and (un)secure — Reputation risk, user acceptability, revenue impacts, compliance fines\\r\\n(Non)industrialized — Impact on gross margin, lower revenue, productization and scalability challenges\\r\\n(Non)sustainable — Ambiguous decarbonization goals, reputation damage, higher cost for customers, reduced talent attraction, lower revenue, compliance fines\\r\\nEthical considerations are important in AI solutions. There has been a shift from algorithms written by humans to algorithms that learn their behaviour from data. This implies that humans need to be in the loop to control outputs of the AI algorithms, which can be biased by input data or potentially compromised. Full delegation of human-controlled tasks to AI solutions implies greater responsibility in the way AI solutions are designed and maintained in a secure and explainable way.\\r\\nPutting customer’s purposes at the heart\\r\\nFinally, it’s important to position AI ethics within a customer relationship. Ethics is relative to every purpose and company. Our goal is to support our customers in building responsible AI. We do this by first defining Atos ethics, which explain how we want to drive our AI projects from an ethical point of view. In addition, they must be aligned with the Atos’s enterprise purpose. Then, we define the AI ethics specificity driven by a particular project that requires a custom view on the situation (e.g. a self-driving car that needs to adapt to the principles and cultures of a particular country).\\r\\nWe establish processes and provide rules and tools to make sure that AI solutions take ethical concerns into account up-front, from their creation to their retirement. In a nutshell, we advise our clients to include AI concerns in their ethics.\\r\\nOnly by taking this broad array of factors into account and designing solutions that align with our priorities, client priorities and the specific business, cultural and legal parameters can we develop AI solutions that are a true win-win for all parties.',\n",
              " 'Our Code of Ethics for AI\\r\\nAI is a general-purpose technology that can affect entire\\r\\neconomies, and which is spreading very visibly beyond\\r\\nthe business area to areas of daily life. A challenge\\r\\nfacing both business and society today is how to\\r\\noptimize the opportunities offered by AI technology,\\r\\nwhilst addressing the risks and fears that AI may generate.\\r\\nSince its foundation, Capgemini places ethics at the center of\\r\\nits activity. As a leader in digital transformation, we are committed\\r\\nto the adoption of AI in a way that delivers clear benefits from AI\\r\\ntechnologies within a trusted framework, by building a Code of Ethics\\r\\nfor AI.\\r\\nOur ethical culture drives our vision of AI, guided notably by 5 of\\r\\nour core Values: Honesty, Trust, Boldness, Freedom, and Modesty.\\r\\nThese Values work together to inform our approach. Boldness drives\\r\\nus to act as entrepreneurs, identifying and pursuing the opportunities\\r\\npresented by innovation in this field. We aspire to increase Freedom\\r\\nby empowering, complementing and augmenting human cognitive,\\r\\nsocial and cultural skills, giving people more say over how they\\r\\nlive their lives. Modesty keeps us mindful of the need to mitigate\\r\\nrisks, building solutions that are robust, safe, and human-centric.\\r\\nHonesty underpins our commitment to transparency, and to creating\\r\\nsolutions that are accountable and controllable. We consider\\r\\nTrust to be an essential basis for long-standing interdependent\\r\\nrelationships with clients, users, and all members of our ecosystem;\\r\\nthe value we place on Trust drives our efforts to create AI that\\r\\nprotects privacy and ensures equal access rights and fair treatment.\\r\\nOur Code of Ethics for AI guides our organization on how to\\r\\nembed ethical thinking in our business. It is illustrated by concrete\\r\\nexamples from projects or solutions that we deliver. Reference\\r\\nto its principles stimulates ethical reasoning and is intended\\r\\nto launch an open-ended process of discussion within the company,\\r\\nwith our clients, and with all stakeholders.\\r\\nOur Code of Ethics for AI concerns both the intended purpose\\r\\nof the AI solution, and the way we embed ethical principles in\\r\\nthe design and delivery of AI solutions and services to our clients.\\r\\nIt should be read in combination with applicable legislations\\r\\nwith which Capgemini is, of course, committed to comply.\\r\\nOur Code of Ethics for AI\\r\\nAI definition\\r\\nArtificial Intelligence (AI) is a\\r\\ncollective term for the capabilities\\r\\nshown by learning systems\\r\\nthat are perceived by humans\\r\\nas representing intelligence.\\r\\nThese intelligent capabilities\\r\\ntypically can be categorized\\r\\ninto Machine Vision & Sensing,\\r\\nNatural language processing,\\r\\nPredicting & Decision-making,\\r\\nand Acting & Automating.\\r\\nVarious applications of AI include\\r\\nspeech, image, audio and video\\r\\nrecognition, autonomous vehicles,\\r\\nnatural language understanding and\\r\\ngeneration, conversational agents,\\r\\nprescriptive modelling, augmented\\r\\ncreativity, smart automation,\\r\\nadvanced simulation, as well as\\r\\ncomplex analytics and predictions.\\r\\nTechnologies that enable these\\r\\napplications include automation,\\r\\nbig data systems, deep learning,\\r\\nreinforcement learning\\r\\nand AI acceleration hardware.\\r\\n22 Capgemini Capgemini Our Code of Ethics for AI Our Code of Ethics for AI\\r\\nAt Capgemini, we believe that human ethical values should\\r\\nnever be undermined by the uses made of AI by business. We want\\r\\nAI solutions to be human-centric, which we define as follows:\\r\\n1. AI with carefully delimited impact – designed for human benefit,\\r\\nwith a clearly defined purpose setting out what the solution will deliver,\\r\\nto whom.\\r\\n2. Sustainable AI – developed mindful of each stakeholder, to benefit\\r\\nthe environment and all present and future members of our ecosystem,\\r\\nhuman and non-human alike, and to address pressing challenges such\\r\\nas climate change, CO₂ reduction, health improvement, and sustainable\\r\\nfood production.\\r\\n3 Fair AI – produced by diverse teams using sound data for unbiased\\r\\noutcomes and the inclusion of all individuals and population groups.\\r\\n4. Transparent and explainable AI – with outcomes that can be\\r\\nunderstood, traced and audited, as appropriate.\\r\\n5. Controllable AI with clear accountability – enabling humans\\r\\nto make more informed choices and keep the last say.\\r\\n6. Robust and safe AI – including fallback plans where needed.\\r\\n7. AI respectful of privacy and data protection – considering\\r\\ndata privacy and security from the design phase, for data usage that is\\r\\nsecure, and legally compliant with privacy regulations.\\r\\n& Sensin\\r\\ng\\r\\nMachine\\r\\nVisio\\r\\nn\\r\\nDec si\\r\\noi n\\r\\na M i k gn\\r\\nPred ci t ni g na d\\r\\nuA\\r\\not mat ni\\r\\ng\\r\\nc A t ni g and\\r\\nProcessing\\r\\nN\\r\\natural Language\\r\\nLEARNING\\r\\nSYSTEMS\\r\\nAugmented\\r\\nCreativity\\r\\nAutonomous\\r\\nVehicles\\r\\nAI APPLICATIONS\\r\\nSpeech Synthesis Image Recognition\\r\\nAudio & Video\\r\\nAnalytics\\r\\nNatural\\r\\nLanguage\\r\\nUnderstanding\\r\\nConversational\\r\\nAgents\\r\\nPrescriptive\\r\\nModelling\\r\\nSmart Automation\\r\\nAdvanced\\r\\nSimulation\\r\\nReinforcement\\r\\nLearning\\r\\nDeep Learning\\r\\nAI Acceleration\\r\\nHardware\\r\\nAI ENABLERS\\r\\nAnalytics &\\r\\nPredictions\\r\\nBig Data\\r\\nSystems\\r\\nINTELLIGENCE\\r\\nCAPABILITIES\\r\\nAI Taxonomy\\r\\nBuilding on our core Values,\\r\\nwe believe that the design\\r\\nand delivery of AI solutions\\r\\nshould be guided by these\\r\\nseven principles, aligned\\r\\nwith the “Ethics Guidelines\\r\\nfor Trustworthy AI” issued\\r\\nin 2019 by the independent\\r\\nHigh-Level Expert Group\\r\\non AI set up by the European\\r\\nCommission.\\r\\n3\\r\\n ETHICAL CHALLENGE\\r\\nThe very first and fundamental ethical question to be considered\\r\\nis the intended purpose of the AI solution and its impact\\r\\non humans. Like any general-purpose technology, AI solutions\\r\\ncan equally enable and negatively affect human fundamental\\r\\nrights.\\r\\n1. AI with carefully\\r\\ndelimited impact\\r\\n CAPGEMINI’S RESPONSE\\r\\nCapgemini cares about the intended\\r\\npurpose of AI solutions; our solutions\\r\\nmust be mindful of the impact on humans\\r\\nand respect universal fundamental rights,\\r\\nprinciples and values, in particular the\\r\\nUniversal Declaration of Human Rights\\r\\nand the UN Global Compact. AI must\\r\\nfocus on improving life for humans and\\r\\nshould neither exacerbate existing harm\\r\\nnor create new harm for individuals.\\r\\nThe intended purpose of an AI application\\r\\n– what the AI solution will deliver,\\r\\nfor whom, and to whom – must be clearly\\r\\ndefined, and AI should be used according\\r\\nto its intended purpose. To this end,\\r\\nwe are transparent about the intended\\r\\npurpose with our various stakeholders,\\r\\nnotably the end users, and include\\r\\nappropriate provisions in our agreements,\\r\\nclearly describing the use for which the\\r\\ntechnology is intended.\\r\\nAs AI is a highly evolutive technology,\\r\\nwe believe that assessing the impact\\r\\nof AI solutions, notably on individuals,\\r\\nis important to help identify the overall\\r\\nimpact, i.e. the likely benefits against\\r\\nthe foreseeable risks, such as social impact\\r\\nor potential risk deriving from inadequate\\r\\nor inappropriate use. Assessing the\\r\\npotential impact that a new technology\\r\\ncan have before adopting it helps identify\\r\\nundesired side-effects and consequent\\r\\nethical risks and helps mitigate them.\\r\\nIn situations where there is any doubt about\\r\\na potential risk of affecting fundamental\\r\\nrights, a fundamental-rights impact\\r\\nassessment will be undertaken to ensure\\r\\nthat such a risk is eliminated.\\r\\n4 Capgemini Our Code of Ethics for AI\\r\\nCASE\\r\\nGlobal demand for food is anticipated to increase\\r\\nby 60% by 2050. Today, much of the world’s\\r\\npopulation is fed by small-scale farmers, primarily\\r\\nin developing countries, using rudimentary\\r\\nfarming practices. This agricultural inefficiency\\r\\nis exacerbated by a complex value chain and a lack\\r\\nof resources and connectivity, so there is a strong\\r\\nneed for a wider package of yield-optimizing\\r\\nand risk-decreasing services for these small-scale\\r\\nfarmers. Project FARM, created at Capgemini’s\\r\\nApplied Innovation Exchange (AIE) Collaboration\\r\\nZone (CoZone) in the Netherlands, aims to address\\r\\nthese issues.\\r\\nThe Project FARM platform uses AI to determine\\r\\nfarming patterns through big data, generating\\r\\ninsights from the data to make recommendations.\\r\\nIt uses machine learning to make the platform\\r\\napplicable at scale, by connecting it with cell\\r\\nphones. This solution has been built in collaboration\\r\\nwith Agrics, a social enterprise operating in\\r\\nEast Africa, which provides local farmers with\\r\\nagricultural products and services on credit.\\r\\n ETHICAL CHALLENGE\\r\\nBeyond the direct impact on humans and human society, other beings\\r\\nand the environment can be impacted by AI solutions. The challenge\\r\\ngoes beyond guiding “human-friendly AI”, to ensuring “Earth-friendly\\r\\nAI”. As the scale and urgency of the economic and health impacts\\r\\nfrom our deteriorating natural environment grows, we have an\\r\\nopportunity to look at how AI can help transform traditional sectors\\r\\nand systems to address climate change, deliver food and water security,\\r\\nbuild sustainable cities, and protect biodiversity and human wellbeing.\\r\\nFurthermore, AI cannot support a sustainable future if it is not itself\\r\\nsustainable by design.\\r\\n2. Sustainable AI\\r\\n CAPGEMINI’S RESPONSE\\r\\nAI systems should benefit all human\\r\\nbeings. This means that their design and\\r\\ndevelopment should take into careful\\r\\nconsideration the social and societal\\r\\nimpacts. Design and development must\\r\\nalso be mindful of future generations,\\r\\nthe environment, and all beings – human\\r\\nand nonhuman alike – that make up\\r\\nour ecosystem. They must be considered\\r\\nas stakeholders throughout the AI\\r\\nsolution’s life cycle, so that AI solutions are\\r\\nsustainable and environmentally friendly.\\r\\nWe support AI to address challenges\\r\\nin societal areas as diverse as climate\\r\\nchange and CO₂ reduction, digital literacy\\r\\nand inclusion, environmental protection,\\r\\nhealth improvement, and sustainable\\r\\nfood production.\\r\\n5\\r\\n CAPGEMINI’S RESPONSE\\r\\nWe embed diversity and inclusion principles\\r\\nthroughout the entire AI system’s life cycle:\\r\\n• AI design and development teams\\r\\nmust be built as diverse teams,\\r\\nwith diversity in terms such as gender\\r\\nand ethnicity, but also discipline,\\r\\nfor multiple perspectives during AI design,\\r\\nand sensitivity to the fullest spectrum of\\r\\nethical issues.\\r\\n• We seek to identify any unfair bias likely\\r\\nto lead to discrimination and inappropriate\\r\\nresults in the context of decision making,\\r\\nand present possible correction scenarios\\r\\nto remove them.\\r\\n• We will advise clients to put in place an\\r\\noversight process to analyze and address\\r\\nthe system’s purpose, constraints,\\r\\nrequirements, and decisions in a clear\\r\\nand transparent manner.\\r\\n• AI systems must entail and ensure equal\\r\\naccess rights and treatment by people\\r\\n(regardless of ethnicity, disability,\\r\\nage, religious belief, sexual orientation,\\r\\nor other personal characteristics).\\r\\n• As an alternative to – potentially biased –\\r\\nhistorical training data, generated\\r\\n(i.e. synthetic) data or off-the-shelf\\r\\nindustry data should be considered.\\r\\nCASE\\r\\nCapgemini Invent has developed SAIA – Sustainable\\r\\nArtificial Intelligence Assistance – a demonstrator\\r\\ndesigned to show our approach to prevent\\r\\ndiscrimination and make AI decisions transparent\\r\\nthroughout the AI life cycle. It identifies potential\\r\\nbiases and analyzes bias behavior. It also provides\\r\\nrecommendations on ways to correct algorithm\\r\\nbiases and simulates the impact of these corrections.\\r\\n ETHICAL CHALLENGE\\r\\nIn order to be effective, AI needs to learn from historical data. The more data,\\r\\nthe more accurate an AI system will be in terms of categorizing, predicting,\\r\\nprescribing, and overall decisioning. However, training data for machines,\\r\\nnotably statistics, may reflect an organizational or individual perspective\\r\\non a given subject matter, or a historical picture of reality. This perspective\\r\\nmay be biased or incorrect, as data can include various forms of bias,\\r\\nresulting in extrapolations that can conflict with or undermine current trends\\r\\nand desired evolutions, gradually building up over time. This can result in\\r\\ndiscrimination against certain population groups based on gender, ethnicity,\\r\\nor similar social factors.\\r\\nLikewise, unfair biases and discrimination can be built in the algorithms\\r\\nthemselves, by design and development teams lacking appropriate diversity.\\r\\n3. Fair AI\\r\\n6 Capgemini Our Code of Ethics for AI\\r\\nCASE\\r\\nA world-leading bank for which AI is expected\\r\\nto become a significant part of operations needed\\r\\nto understand and compare the most popular AI\\r\\nexplainability methods. Capgemini designed a 4-step\\r\\napproach, comparing the explainability models\\r\\non several axes (quality of results, smoothness,\\r\\nsource dataset impact, and consistency between the\\r\\nresults of each method). By putting explainability at\\r\\nthe heart of the project, the client will ensure that\\r\\ninnovation through AI is properly understood and\\r\\nactionable for its teams..\\r\\nWorking with technologies that we\\r\\nunderstand and control, we will provide\\r\\ndocumentation and training to users\\r\\nto explain the logic behind the functioning\\r\\nof the AI and to indicate the limits\\r\\nof understanding and testing scenarios,\\r\\nin a manner adapted to the different\\r\\nstakeholders potentially concerned.\\r\\n CAPGEMINI’S RESPONSE\\r\\nAI must be transparent: its capabilities and\\r\\npurpose should be openly communicated.\\r\\nDecisions based on AI solutions should be\\r\\nexplainable, with the degree of explicability\\r\\ndependent on the context and severity\\r\\nof the consequences if the output is\\r\\nerroneous. The data sets and processes\\r\\nused for the AI solution should also be\\r\\ndocumented to allow for traceability and,\\r\\nif required, for auditability.\\r\\nWhen interacting with an AI interface,\\r\\nindividuals should be aware that they are\\r\\ncommunicating with a machine, and should\\r\\nnot be misled into thinking otherwise, while\\r\\nbeing informed of the AI capabilities and\\r\\nlimits. Individuals interacting with AI should\\r\\nbe made clearly aware about the purposes\\r\\nof the AI system, how it works, with whom\\r\\nthe information may be shared, the impact\\r\\nof the AI solution and any potential impact\\r\\non their rights, if any, in relation to the AI\\r\\nsystem at stake.\\r\\nWe will advise our clients to ensure with us\\r\\nthat systems developed are explainable,\\r\\nespecially regarding data selection and\\r\\ntreatment, notably weightings. We will\\r\\nendeavor to indicate the limits that can exist\\r\\nin the understanding of their functioning.\\r\\n ETHICAL CHALLENGE\\r\\nThe complexity of AI may amplify the “black box” concern. A “black box” is\\r\\na device, system, or program that allows input and output to be seen but gives\\r\\nno view of the processes or workings between the two. For example, in tools\\r\\nusing artificial neural networks, hidden layers of nodes process the input\\r\\nand pass their output to subsequent layers of nodes, while in deep learning,\\r\\nan artificial neural network “learns” autonomously by pattern recognition.\\r\\nAs with a human brain, one cannot see the output between layers, how data\\r\\nhas been analyzed, or what has been “learnt” – one sees only the conclusion.\\r\\nWhere a conclusion needs to be checked and justified, because it is unexpected,\\r\\nincorrect, or problematic, it can therefore be highly challenging to understand.\\r\\nThis is of greater concern where AI functionality plays a role in high-stakes\\r\\ndecision-making areas, such as banking, justice or health, where the potential\\r\\nimpact of decisions is more serious.\\r\\n4. Transparent\\r\\nand explainable AI\\r\\n7\\r\\nCASE\\r\\nA major energy supplier wanted to improve\\r\\nthe management of inbound requests\\r\\nfrom customers to enhance its service quality.\\r\\nCapgemini developed and integrated semantic\\r\\nanalysis for emails into the information system,\\r\\nallowing to automatize dispatches to the right\\r\\nteam. Accountability was secured by a reference\\r\\nbook setting out the business rules and AI\\r\\nprocessing guidelines. It will be regularly updated\\r\\nto allow for clear accountability over time.\\r\\n ETHICAL CHALLENGE\\r\\nWhile the control responsibilities in any IT system depend on organizing\\r\\naccountability, several aspects complexify this with regard to AI. The production\\r\\nenvironment itself often involves many discrete contributors, including highly\\r\\nspecialized third parties, rendering in-built controllability and oversight\\r\\nmore difficult. Moreover, in a legal environment largely based on the\\r\\nassumption of human agents, AI systems depend to a greater or lesser degree\\r\\non AI-driven intelligence, autonomous agents, and autonomous decisionmaking. Determining responsibility for AI outcomes is further complexified\\r\\nby techniques such as deep learning, which can make systems hard to control\\r\\nand outputs difficult to explain. While individuals need to know who will\\r\\nbe answerable in case of malfunction, or should a system have unintended\\r\\nconsequences or cause harm, tying an AI’s actions or decisions to a human,\\r\\nor group of humans, can therefore present a considerable challenge.\\r\\n5. Controllable AI\\r\\nwith clear accountability\\r\\n CAPGEMINI’S RESPONSE\\r\\nHumans should keep the last say on AI,\\r\\nand we should design AI solutions in such\\r\\na way that AI cannot learn to circumvent\\r\\nthe controls and voluntary interruptions\\r\\nby humans.\\r\\nThe design of AI solutions should protect\\r\\nthe human’s autonomy and decision making.\\r\\nAs such, AI solutions should help humans\\r\\nmake more informed choices.\\r\\nTo ensure such respect, humans should\\r\\nbe part of the AI governance mechanism\\r\\nin such a way that they always keep control\\r\\nover AI. Appropriate measures should be\\r\\nimplemented from the AI solution’s design\\r\\nphase, with the appropriate level of human\\r\\noversight depending on the AI solution\\r\\napplication area and potential risks.\\r\\nAI systems cannot be the subject “per se”\\r\\nof legal responsibility for their own\\r\\nfunctioning, so the AI system design should\\r\\nembed accountability rules (to identify\\r\\nwho is responsible for what) and trackability\\r\\nprinciples, allowing the AI-based decisionmaking process to be explained and audited,\\r\\nthus helping to identify and prevent future\\r\\nmistakes or bias.\\r\\nTo achieve this objective, we will\\r\\nadvise our clients that it implies to\\r\\ndefine and clearly identify together\\r\\nroles and responsibilities amongst\\r\\nthe different actors involved in the design,\\r\\nmanufacturing, integration, deployment\\r\\nand operation chain, including the\\r\\ndesigner of the AI solution, the data\\r\\nprovider, and the company that adopts\\r\\nthe AI solution or the final user. This would\\r\\nenable appropriate allocation of liability\\r\\nand effective recourse when needed.\\r\\n8 Capgemini Our Code of Ethics for AI\\r\\nCASE\\r\\nSogeti, part of Capgemini, was requested to help\\r\\na public social insurance agency to better leverage\\r\\nthe sensitive data at its disposal in order to provide\\r\\na secure, efficient service to the agency’s clients.\\r\\nA 6-step proof of concept was implemented\\r\\nto design, train, and control machine-learning\\r\\nalgorithms. This ensured the robustness and\\r\\nsafety of the model, by allowing original data\\r\\nto be preserved and data set performances to be\\r\\nevaluated, for referential integrity.\\r\\n ETHICAL CHALLENGE\\r\\nLike any tools or systems, those utilizing AI must be fit for their\\r\\nintended purposes, and resilient and secure from a technical\\r\\nperspective. As AI uptake increases, so does the scope for potential\\r\\nimpact, and the need to also consider the broader social and\\r\\nenvironmental context in which AI-based tools and systems operate.\\r\\nFrom this arises the challenge to foresee measures to safeguard\\r\\nagainst any risks, such as unlikely mishaps or malevolent intent,\\r\\nthat might prevent the AI from delivering the desired benefits.\\r\\n6. Robust and safe AI\\r\\n CAPGEMINI’S RESPONSE\\r\\nRobustness should be embedded\\r\\nthroughout the life cycle of the AI systems,\\r\\nfrom the design and development\\r\\nto the deployment and use over their\\r\\nlifetime. AI systems should include,\\r\\nwhen achievable, fallback plans in case of\\r\\nfailure of the AI system itself (e.g. allowing\\r\\nto adjust rule-based logic or even switch\\r\\nto human control, to avoid any wrong\\r\\noutput), as well as being accurate,\\r\\nreliable and having reproducible results,\\r\\nto the extent allowed by applicable laws.\\r\\n9\\r\\n ETHICAL CHALLENGE\\r\\nWith AI taking off, the need for data is greater than ever, much of it driven by consumers.\\r\\nThe opportunity for greater freedom presented by easily accessible data brings with\\r\\nit a related risk to data protection and informational privacy. Lengthy user agreements\\r\\ncan tempt consumers to click “accept” without checking what rights they are giving\\r\\naway, while companies can be tempted to feed consumer and vendor data into advanced,\\r\\nAI-fueled algorithms, without the awareness and approval of the affected consumers\\r\\nand employees. Facial recognition, voice identification systems and smart homeappliances collect data about when we come and go; while many such functions provide\\r\\na helpful service, the potential risks they carry are not trivial: Seemingly anonymized\\r\\ndata can be de-anonymized by AI. Data collected can also enable tracking, monitoring,\\r\\npeople profiling, and behavior prediction. By raising analysis of personal information\\r\\nto new levels of power and speed, AI magnifies our ability to use – and misuse – personal\\r\\ninformation, presenting a challenge for privacy and data protection.\\r\\n7. AI respectful of privacy\\r\\nand data protection\\r\\n CAPGEMINI’S RESPONSE\\r\\nIn agreement with the client, we must\\r\\nensure, that we will put in place all\\r\\nthe necessary means for our current\\r\\nperimeter of responsibility, to contribute\\r\\nto the Clients’ global AI objectives\\r\\nin terms of compliance with privacy\\r\\nregulations, data protection and proper\\r\\ndata governance.\\r\\nAI and data protection are compatible\\r\\nas long as data protection and\\r\\ncybersecurity are taken into account\\r\\nfrom the design phase of any AI project.\\r\\nBesides ensuring full respect for privacy\\r\\nand data protection laws and regulations,\\r\\nadequate data governance mechanisms\\r\\nshould also be put in place. In practice,\\r\\nthis means that any AI project would need\\r\\nto ensure that only data that are strictly\\r\\nnecessary are collected and processed.\\r\\nIndeed, the data collected and used shall\\r\\nbe proportionate, accurate, and processed\\r\\nin a secure manner.\\r\\nIndividuals will be provided with the relevant\\r\\nlevel of information on how their data\\r\\nis processed and they should be provided\\r\\nwith appropriate means to exercise their\\r\\nrights as may be required by law.\\r\\n10 Capgemini Our Code of Ethics for AI\\r\\n1111\\r\\nThe information contained in this document is proprietary.\\r\\nCopyright ©2021 Capgemini. All rights reserved.\\r\\nGraphic design: Avant Midi. January 2021 – Version 1 – EN\\r\\nThe above ethical principles on AI aim to create an\\r\\nethical culture for AI and represent a commitment\\r\\nto trustworthiness and ethical quality in services relating\\r\\nto AI.\\r\\nAI solutions developed by Capgemini are based on\\r\\necosystems that may be composed of several\\r\\nthird parties. Capgemini is committed to selecting\\r\\nand working with third-party providers that commit\\r\\nto comply with ethical principles.\\r\\nOur Code of Ethics for AI defines areas for attention\\r\\nof ethical importance. These are being completed\\r\\noperationally by the development of tools integrated\\r\\ninto service offerings and methodologies that comply\\r\\nwith the ethical principles set out in the present\\r\\ncode. Our professionals (such as AI solution architects\\r\\nand project managers) are trained to fully apply the code\\r\\n“by design” in all their AI-related engagements.',\n",
              " 'The Cisco Responsible\\r\\nAI Framework\\r\\nSecurity by Design / Human Rights by Design / Privacy by\\r\\nDesign for personal data and consequential decisions\\r\\nAt Cisco, we appreciate that Artificial Intelligence (AI) can be leveraged to power an inclusive future for all.\\r\\nWe also recognize that by applying this technology, we have a responsibility to mitigate potential harm. That\\r\\nis why we have developed a Responsible AI Framework based on six principles of Transparency, Fairness,\\r\\nAccountability, Privacy, Security and Reliability.\\r\\nWe translate these principles into controls that can be applied to model creation and the selection of\\r\\ntraining data with Security by Design, Privacy by Design, and Human Rights by Design processes embedded\\r\\nthroughout the model’s lifecycle and its application in products, services, and enterprise operations. \\r\\nThe Responsible AI Framework\\r\\nGuidance and Oversight • Establishes a Responsible AI Committee of senior executives across Cisco business\\r\\nunits, sales, privacy, security, human rights, legal, government affairs, human\\r\\nresources, and other functions.\\r\\n• Advises Cisco on responsible AI practices and oversees Responsible AI Framework\\r\\nadoption.\\r\\n• Reviews sensitive or high-risk uses of AI proposed by our business units and\\r\\nincident reports of bias or discrimination.\\r\\nControls • Embeds security, privacy, and human rights processes into AI design as part of\\r\\nthe existing Cisco Secure Development Lifecycle.\\r\\n• Assesses AI functions for models and data directly involved in decisions that could\\r\\nhave adverse legal or human rights impact. \\r\\n• Applies controls to reduce risk of AI harm by focusing on areas like unintended bias\\r\\nmitigation, model monitoring, fairness, and transparency.\\r\\nIncident Management • Leverages security, data breach, and privacy incident response system to manage\\r\\nreported AI incidents involving bias and discrimination.\\r\\n• Assigns and reports incidents to the Responsible AI Incident Response Team to\\r\\nanalyze and engage relevant team for resolution.\\r\\n• Tracks and reports AI incidents to governance board and reports findings and\\r\\nremediation steps to the original submitter or a broader group of stakeholders,\\r\\ncustomers, employees, and partners.\\r\\n Industry Leadership • Embeds Responsible AI as a focus area for incubation of new technology across Cisco.\\r\\n• Engages with industry innovation providers focused on delivering Responsible AI.\\r\\n• Participates proactively in industry forums to advance Responsible AI, including the\\r\\nCentre for Information Policy Leadership, Equal AI, and the Business Roundtable on\\r\\nHuman Rights and AI.\\r\\n External Engagement • Works with governments to understand global perspectives on AI’s benefits and risks.\\r\\n• Monitors, tracks, and influences AI-related legislation, emerging policy, and\\r\\nregulations.\\r\\n• Partners with and sponsors cutting-edge research institutions, exploring the\\r\\nintersection of ethics and AI from technical, organizational, social, and design\\r\\nperspectives.\\r\\nCisco is committed to continuing internal focus and collaboration with our external partners and stakeholders\\r\\nto improve our collective understanding of the societal and human rights impact of AI. We work to continuously\\r\\nimprove our framework to support fair, explainable, and transparent results of the AI systems we develop and use.\\r\\nLearn more about our approach to Responsible AI at the Cisco Trust Center.',\n",
              " 'Facebook’s five pillars of Responsible AI\\r\\nAI today is a core component of a vast range of technology used by billions of people around the world. Here at Facebook, it is part of systems that do everything from ranking posts in your News Feed to tackling hate speech and misinformation to responding to the COVID-19 pandemic. But, as with other emerging technologies, AI also raises many hard questions around issues such as privacy, fairness, accountability, and transparency.\\r\\n\\r\\nThe challenges raised by AI are new territory for everyone, so standards are still emerging and we admittedly don’t yet have all the answers to these important questions. We at Facebook — and we as a society — are at the beginning rather than the end of our Responsible AI journey, and finding the right approaches and then scaling them across Facebook’s large family of products and features will take time. But we recognize how important these issues are, and we’re committed to addressing them in an open and collaborative way.\\r\\n\\r\\nThat’s why Facebook created a dedicated, cross-disciplinary Responsible AI (RAI) team within its AI organization, much like we have previously invested in a dedicated Privacy team to lead our data privacy efforts and Integrity teams to enforce the policies that keep people safe on our platforms. Along with many others working across the company’s different product organizations, the RAI team is building and testing approaches to help ensure that our machine learning (ML) systems are designed and used responsibly. We are doing so with the benefit of regular consultation and collaboration with outside experts and regulators, and in line with RAI’s overall goal of ensuring that AI at Facebook benefits people and society.\\r\\n\\r\\nIt’s encouraging to see the growing momentum among governments, industry experts, and others to answer these challenges collaboratively. Most notably, the European Commission in April debuted its proposal for a risk-based approach to regulating AI. We look forward to engaging with EU lawmakers, and we welcome this proposal as a first step toward AI regulation that we hope will protect people’s rights while ensuring continued innovation and economic growth. As with other emerging technologies, we believe proactive regulation of AI is necessary. And it’s particularly important to ensure that AI governance is based on foundational values of respect for human rights, democracy, and the rule of law, particularly when countries such as China are pursuing global AI superiority unrestrained by those rights and values.\\r\\n\\r\\nThose foundational values are at the root of the wide range of principles statements that have been released around responsible AI development, most especially the European Commission’s High-Level Expert Group’s Ethics Guidelines for Trustworthy AI and the Organization for Economic Cooperation and Development’s Principles on Artificial Intelligence, which Facebook helped develop. Facebook, in turn, has organized its Responsible AI efforts around five key pillars that were heavily influenced by those principles: Privacy & Security, Fairness & Inclusion, Robustness & Safety, Transparency & Control, and Accountability & Governance.\\r\\n\\r\\nWe’re sharing new details here on our efforts inside Facebook in this crucial area, including work from our RAI team and from across the company. While much of our work is still in early stages and we have much to do, these five pillars will guide our efforts to help ensure that Facebook uses AI responsibly.\\r\\n\\r\\nPrivacy & Security\\r\\nAt Facebook, we believe protecting the privacy and security of people’s data is the responsibility of everyone at the company.\\r\\n\\r\\nThat’s why we have built our cross-product Privacy Review process, by which we assess privacy risks that involve the collection, use, or sharing of people’s information. The process is also designed to help identify and mitigate the privacy risks we identify, including features and products driven by AI. We recently published an in-depth progress update on our company-wide privacy efforts, going into greater detail about our review process and the eight core Privacy Expectations that serve as its foundation.\\r\\n\\r\\nOver the next several years, we’ll leverage the centralized tools we’ve developed to help manage the Privacy Review process to support continued investment in infrastructure improvements that will systematize the process of enforcing our privacy decisions. These changes will move more human-driven processes to automated ones that will make it easier to consistently enforce our privacy commitments across our products and services, including those driven by AI.\\r\\n\\r\\nAdditionally, in order to standardize the data flow and the development of AI for production purposes, we are unifying AI systems into one platform and ecosystem that will support all teams across the company. With a single platform, we can quickly and responsibly evolve models that perform countless inference operations every day for the billions of people that use our technologies.\\r\\n\\r\\nOf course, AI can raise novel privacy and security concerns that go beyond questions of data infrastructure. In particular, face and speech recognition and other AI-driven technologies that make use of sensitive information have driven a great deal of privacy concern among policymakers and the public. That’s why we ask people eligible for Face Recognition to affirmatively turn it on before we recognize them, and provide clear controls to turn it off. Similarly, we allow people to turn off storage of voice interactions on their Portal devices. When stored, voice interactions improve our speech recognition algorithms, and we take strict steps to protect people’s privacy when our review team transcribes those interactions.\\r\\n\\r\\nAI can also create new opportunities for protecting privacy, which is why we are heavily investing in research around privacy-preserving machine learning technologies like differential privacy, federated learning, and encrypted computation — and teaching the AI community how to deploy them. Our goal is to leverage this research to make our products work better for people while collecting less data and better protecting the data that we do collect. For example, the computer vision processing that allows Portal’s Smart Camera to accurately focus on people in the camera frame happens on the device. We are also publicly sharing the fruits of our research in the form of AI privacy resources like CrypTen, a tool to help researchers who aren’t cryptography experts easily experiment with ML models using secure computing techniques, and Opacus, an open source library for training ML models with differential privacy, to help advance the state of the art and improve AI privacy across the industry.\\r\\n\\r\\nFairness & Inclusion\\r\\nAt Facebook, we believe that our products should treat everyone fairly and work equally well for all people, which is why Fairness is one of the core Privacy Expectations that help guide the above mentioned Privacy Review process.\\r\\n\\r\\nIn the context of AI, our Responsible AI team has developed and is continually improving our Fairness Flow tools and processes to help our ML engineers detect certain forms of potential statistical bias in certain types of AI models and labels commonly used at Facebook, as described in our recent academic paper, with a goal of eventually scaling similar measurement to all our AI products.\\r\\n\\r\\nThe RAI team has also developed a framework for evaluating the fairness maturity of our products that is now beginning to be incorporated into the goals of all the product teams in our Facebook AI organization, and we aim to eventually require similar goals for all our product teams across the company. To help product teams formulate and meet those goals, we’ve established a multidisciplinary team of experts to offer targeted Fairness Consultations about specific products.\\r\\n\\r\\nOne fairness effort that we’re particularly proud of is our work to help ensure that the AI-driven Portal Smart Camera accurately focuses on people on-camera regardless of apparent skin tone or gender presentation. This foundational fairness work has helped inform computer vision efforts across a range of our products, and we recently released our Casual Conversations data set, composed of over 45,000 videos designed to similarly help researchers evaluate their computer vision and audio models for accuracy across a diverse set of ages, genders, apparent skin tones, and ambient lighting conditions.\\r\\n\\r\\nMeasurement is important, but fairness in AI can’t simply be reduced to a number or a checklist or a mathematical definition. What “fairness” means will often be unclear and contested, and can differ based on the particular product or context at issue. To help us consider these issues from a broad range of perspectives, Facebook’s Responsible Innovation team and Diversity, Equity & Inclusion team both facilitate input from a wide range of external experts and voices from underrepresented communities. They similarly solicit input from employees with diverse lived experiences, through both a Diversity Advisory Council and an Inclusive Product Council with a diverse range of stakeholders, to advise teams about how particular communities may be impacted by their new products and how existing products might be improved.\\r\\n\\r\\nWe are also prioritizing AI Diversity & Inclusion education efforts for our AI team when hiring and training employees, and setting clear D&I expectations for our AI managers. We aim to better ensure that the people making our AI products are from as diverse a range of backgrounds and perspectives as the people using them, and that we are inclusive of a broad range of voices in our decision-making.\\r\\n\\r\\nRobustness & Safety\\r\\nAt Facebook, we believe that AI systems should meet high performance standards, and should be tested to ensure they behave safely and as intended even when they are subjected to attack.\\r\\n\\r\\nThat’s why we’ve established an AI Red Team, which partners with our product teams to test how robust our AI-powered integrity systems are against adversarial threats. We are also developing new software tools for testing and improving robustness — and then sharing them with the AI research and engineering community. For example, our open source Captum library provides state-of-the-art algorithms to understand more easily and effectively which features of an AI model built with our open source PyTorch ML framework contribute to that model’s outputs. Captum helps AI developers interpret their models, benchmark their work, and improve and troubleshoot unexpected model outputs. Captum will soon include tools to help simulate adversarial attacks, much as our Red Team adversarially tests our own models. And, just last week, we released another robustness tool called AugLy, which helps teach models to be more robust in the face of perturbations of unimportant attributes of data and focus more on the important attributes of data.\\r\\n\\r\\nIn addition to contributing to outside research on robustness, we are also learning from it. For example, leveraging methodologies proposed by external experts, we’ve developed a new framework and tools to better detect and mitigate “model drift” — that is, the degradation of a model’s predictive power due to changes in the environment.\\r\\n\\r\\nEnsuring robustness and safety is an important challenge for all companies offering AI-driven services, not just Facebook. We believe industry needs to tackle that challenge collectively, which is why we’ve released the research and tools that we have, and why we’ve invested so much in collaborative efforts around adversarial AI testing. For example, we came together with the Partnership on AI, Microsoft, leading academics, and others to create the Deepfake Detection Challenge. That competition drew more than 2,000 participants who trained their deepfake detection models using a unique new data set created by Facebook. Likewise, we created and shared a data set of more than 10,000 new multimodal examples for the Hateful Memes Challenge, a first-of-its-kind competition to help build AI models that better detect multimodal hate speech. And most recently, to further speed up innovation across the field of AI robustness testing, we hosted a virtual cross-industry event in May — the “AI Red Table” — to facilitate the sharing of best practices and learnings between AI industry leaders.\\r\\n\\r\\nTransparency & Control\\r\\nAt Facebook, we believe that the people who use our products should have more transparency and control around how data about them is collected and used, which is why it is one of our eight core Privacy Expectations.\\r\\n\\r\\nBeyond privacy, in the context of Responsible AI, we are striving to be more transparent about when and how AI systems are making decisions that impact the people who use our products, to make those decisions more explainable, and to inform people about the controls they have over how those decisions are made.\\r\\n\\r\\nThat’s why we’ve introduced a number of tools over the years to increase transparency around why people see which News Feed content and ads (Why Am I Seeing This, or WAIST, tools), and to provide additional transparency and control over the data and off-Facebook activity that may impact how we select the News Feed items and ads that we think will be most relevant to them. We are also giving people more control over how our AI systems rank content in your News Feed, including giving you more control over which favorite friends or Pages should influence your ranking, or even letting you turn off AI-driven News Feed personalization altogether.\\r\\n\\r\\nTransparency isn’t just important for the people using our products, though. We also look forward to discussing more about how our models work with outside experts and regulators — and doing so in a way that preserves privacy and doesn’t reveal trade secrets. That is why, for example, we launched in May a new online Transparency Center, where we’ll be publishing information about the News Feed ranking process, including more information about some of the important signals in that process, as well as providing updates on significant changes to those ranking algorithms.\\r\\n\\r\\nThe Responsible AI team is also collaborating with other product teams and building on previous academic research and industry efforts to develop Facebook’s own method for creating simple, standardized documentation of our models, in a form commonly known as model cards. The Instagram Equity team has made the most progress in this effort so far, already using model cards across Instagram’s integrity systems and aiming to apply model cards to all Instagram models before the end of next year.\\r\\n\\r\\nWe are developing these approaches to model documentation in dialogue with other related efforts across the industry. For example, we support the About ML initiative at the Partnership on AI, which aims at developing industry standards in this area. We also held two workshops this spring through TTC Labs to bring together AI designers and AI policy experts to help chart the future of AI model documentation.\\r\\n\\r\\nWe understand that experts, regulators, and everyday people all are eager to more easily understand why AI systems make the decisions they make. There are many challenges in explaining the predictions of complex AI systems, but we are working to push the limits, including with interpretability software such as Captum. Although work in this area is still in its infancy, our hope is that ultimately we will be able to build an integrated transparency solution that can automatically feed information from internal documentation efforts like model cards into new transparency features and controls for the people using our products.\\r\\n\\r\\nAccountability & Governance\\r\\nAt Facebook, we believe in building reliable processes to ensure accountability for our AI systems and the decisions they make.\\r\\n\\r\\nThis requires building governance systems to ensure that our AI systems are performing to high standards, to satisfy external expectations and internal best practices, and to identify and mitigate any potential negative impacts those systems might pose. It also means making sure that wherever necessary or appropriate, humans are able to monitor these systems and intervene when necessary.\\r\\n\\r\\nThat’s why we’re doing the work described in this blog post, from investing in our Privacy Review efforts, to developing approaches and tools to improve our understanding and ability to address concerns about our AI systems, to increasing transparency and control around our AI products and features. And that’s why, except for removals of some content posing extreme safety concerns, we give people a way to appeal and seek additional human review of a broad range of content-takedown decisions, which are sometimes first made with the assistance of AI systems. We hold ourselves accountable through quarterly Community Standards Enforcement Reports and through an independent Oversight Board that considers further appeals of both removal and nonremoval of content and makes binding rulings around our most difficult and important content decisions.\\r\\n\\r\\nWe also recognize that quickly evolving technologies like AI can raise new and unanticipated issues, so we must continually improve our processes for identifying and mitigating negative impacts. To help address potential harms early in the product cycle, Facebook’s Responsible Innovation team provides foresight workshops, office hours, and extended collaborations to help product teams identify and brainstorm solutions to a wide range of individual and societal risks. Meanwhile, the Responsible AI team has begun developing its own AI-specific impact assessment framework that we hope will complement our existing launch review processes. We understand the importance of thinking holistically about how our AI products affect society, and we’re continually experimenting with new ways to educate and inspire our engineers to consider the big picture and weigh the long-term impact of their work.\\r\\n\\r\\nCollaborating on the future of Responsible AI\\r\\nBecause this is still a relatively new field, there are not yet clearly defined standards and processes for AI governance and for assessing potential negative AI-related impacts. We — not just Facebook but also the tech industry, the AI research community, policymakers, advocacy groups, and others — need to collaborate on figuring out how to make AI impact assessment work at scale, based on clear and reasonable standards, so that we can identify and address potential negative AI-related impacts while still creating new AI-powered products that will benefit us all. We must similarly collaborate on developing basic practical standards around AI fairness, privacy, robustness, and transparency before they are codified into law.\\r\\n\\r\\nThat’s why we are prioritizing a broad range of collaborative research around AI best practices. For example, we’re actively participating in efforts to establish clear AI principles and best practices, including collaborating with the OECD’s new AI Observatory project to study and disseminate emerging best practices that are in line with its 2019 AI principles. Through our recently launched Open Loop partnership, we’re building innovative “policy prototyping” projects for testing new potential AI policy requirements with regulators and startups before they become law, to ensure that they are both practical and impactful; we have already launched projects in Europe, Asia, and Latin America, with more to come. We’re funding a global effort to solicit diverse academic research on AI ethics issues, supporting projects in Asia, Africa, and Latin America and providing foundational support for an independent Institute for Ethics in Artificial Intelligence at the Technical University of Munich. And we’re a founding partner in the Partnership on AI, the premier cross-industry, cross-civil-society multistakeholder forum for collaboratively developing AI best practices.\\r\\n\\r\\nOf course, the people who use our products and services are another key stakeholder in how we chart our course around Responsible AI, and their lived experiences are another critical form of feedback to factor into our thinking. That’s why we have an integrated user research practice that allows us to better understand the core needs of the people using our products and help ensure that we are building technology and experiences that benefit people, society, and the world.\\r\\n\\r\\nMaintaining a focus on the needs of the people using our products while working together across governments, industries, and broader AI expert communities in academia and civil society, Facebook aims to help chart a course for the future of Responsible AI development that leads to a safer, fairer, and more prosperous society for all. We look forward to that journey and to sharing more about our own Responsible AI practices as they evolve and grow.',\n",
              " 'Fujifilm Group AI Policy\\r\\nThe Fujifilm Group’s corporate philosophy is to contribute to the advancement of culture, science, technology and industry, as well as improved health and environmental protection in society, thereby helping enhance the quality of life of people worldwide. In line with this philosophy, we are combining AI (Artificial Intelligence) with our advanced proprietary technologies, nurtured in a variety of business fields, to create new value, bringing greater comfort and convenience to people’s lives and resolving various issues faced by society.\\r\\nWe recognize that AI, which is still under development, is capable of delivering a wide range of benefits, but contains a number of issues that must also be taken into account, including ethical issues. The rapid advancement of AI technology is also altering social norms and values. Under Fujifilm’s open, fair and clear corporate culture, as defined in the Group’s Vision, we continue to focus on earnest discussions and collaborations with internal and external stakeholders to build mutual trust, and will take on challenges to deliver values that can be created with AI. The Fujifilm Group aims to leverage AI technology to contribute to building a better society, based on the policy set forth below.\\r\\n1．Accelerating New Value Creation\\r\\nIn its efforts to bring about safe, robust and easy-to-use products and services, the Fujifilm Group will actively promote the use of AI to ensure that more and more people can enjoy the benefits of AI technology. By promoting the integration of AI into the advanced proprietary technologies the Group has developed, and through co-creation with external partners, the Fujifilm Group will generate new values in a speedy manner to lead the world in resolving societal issues in healthcare and a wide range of other fields. We will also actively utilize AI in various work processes including R&D and production in company-wide efforts to streamline operations and rapidly deliver values to meet societal expectations.\\r\\n2．Respecting Human Rights\\r\\nThe Fujifilm Group recognizes not only the potential benefits that the advancement of AI technology could bring but also bias, lack of fairness, discrimination and other risks that could occur through the use of AI. We believe that the use of AI be strictly monitored to ensure that dependence on AI does not lead to restrictions or denial of human dignity, ability and potential, or pose a threat to human lives or physical health and wellbeing. The Fujifilm Group will work on developing and supplying products and services that respect basic human rights, preventing the abuse or misuse of AI that could lead to such risks, be it intentional or incidental.\\r\\n3．Ensuring Fair and Appropriate Use of AI\\r\\nThe Fujifilm Group recognizes that the use of AI has the capacity to create bias on data and algorithms that are used as sources of information for AI. In order to ensure fairness and appropriate application of the scope and methods involved in the use of AI, we will anticipate possible scenarios that could occur through diverse use of AI in the development and supply of products and services that employ AI, while developing an internal structure for appropriate verification. We will also constantly review the scope of our use of AI in line with the advancement of AI technology.\\r\\n4．Managing Information Security\\r\\nThe Fujifilm Group recognizes the possibility that data collected and utilized in AI use could affect the rights and interests of individuals and organizations. We therefore continue to work on securing privacy and security to ensure that data is used appropriately. With respect to the handling of personal information, the Fujifilm Group will apply its privacy policy to ensure appropriate management and administration.\\r\\n5．Ensuring Transparency\\r\\nThe Fujifilm Group endeavors to maintain appropriate communication in good faith to fulfill its accountability in relation to the use of AI. Through such communications to ensure transparency, we will strive to gain a high level of trust from our stakeholders, which will lead to further enhancing the accuracy of data to be obtained and thus developing more desirable products and services that utilize AI.\\r\\n6．Developing Human Resource\\r\\nTo actively promote the use of AI, the Fujifilm Group provides in-house AI literacy education and training for human resource development, targeting researchers, developers and a wide range of other staff. We aim to foster human resources that possess a comprehensive understanding of the benefits and risks of AI use, and are capable of using AI to take on new challenges to enable the Fujifilm Group to deliver products and services that prove to be truly useful to society.',\n",
              " 'Fujitsu Group AI Commitment\\r\\nProgress and innovation in the realm of advancing information and communication technologies\\r\\n(ICT), especially artificial intelligence (AI), are dramatically changing the way and the society we\\r\\nlive in. By analyzing enormous amount of data arising continuously, we are finding new ways of\\r\\nhelping to transform industries such as manufacturing, finance, healthcare, transportation, logistics\\r\\nand agriculture, and working to resolve environmental issues such as water shortage, global\\r\\nwarming and desertification.\\r\\nMeanwhile, there are some questions and concerns about unanticipated side effects including\\r\\ndiscrimination. Fujitsu Group (Fujitsu) desires to build a more prosperous and better tomorrow\\r\\nwhere human dignity is respected. We, as a developer and provider of AI solutions, firmly believe\\r\\nit crucial to find the way to use AI not only for convenience in life but also safety and security in use.\\r\\nIn 2009, Fujitsu has promoted a concept: “Human Centric”, where ICT is centered on people.\\r\\nMoreover, in 2015, Fujitsu enhanced its concept for AI: Human Centric AI which autonomously\\r\\ncollaborates with people. The concept already included ethical value around the use of AI. In order\\r\\nto turn Fujitsu‘s vision and concept of Human Centric AI into action, we hereby establish the “Fujitsu\\r\\nGroup AI Commitment” as our core principles. With this commitment, Fujitsu, as a developer and\\r\\na provider of AI solutions, continues to be the most reliable business partner to support enterprises\\r\\nbusiness transformation. We will continue the dialogue with our customers, their customers (enduser) and other stakeholders including external experts and then make prosperity brought by AI\\r\\nspread widely to the world. Furthermore, we will establish a new special committee including\\r\\nexternal experts to survey this commitment. We believe its objective opinion will strengthen our\\r\\ncorporate governance.\\r\\n1. Provide value to customers and society with AI:\\r\\nFujitsu and its whole global group companies respect co-creation with customers by using\\r\\nemerging technologies. We are working together with customers for their prosperous tomorrow.\\r\\nAt the same time, we consider impacts to end-users and society brought by continuously\\r\\nevolving AI.\\r\\n2. Strive for Human Centric AI:\\r\\nFujitsu, advocating “Human Centric AI”, treats people as ends in themselves, not as means.\\r\\nTo achieve this objective, Fujitsu, respecting diversity and inclusion, commits to use AI as a\\r\\ntool to support people’s desire to seek prosperity and contribution for society. As part of this\\r\\neffort, Fujitsu will seek trustworthy AI through considering fairness and safety to prevent\\r\\ndiscrimination and harm.\\r\\n3. Strive for a sustainable society with AI:\\r\\nFujitsu has been strongly committed to Sustainable Development Goals (SDGs). Fujitsu\\r\\nchallenges various social issues and environmental issues and thus contribute for building\\r\\nbetter society and long term business success of our customers.\\r\\n4. Strive for AI that respects and supports people’s decision making:\\r\\nFujitsu believes it is crucial to protect the intrinsic value of human choice on suggestions and\\r\\nresults brought by AI. To this end, Fujitsu will strive for designing and developing AI that can\\r\\nexplain key reasons for why it makes specific recommendations so that humans can make\\r\\ninformed decisions based on such AI.\\r\\n5. As corporate responsibility, emphasize transparency and accountability for AI:\\r\\nAs an information and communication technology provider responsible for reliability of\\r\\ninfrastructure systems, Fujitsu understands it is critical to avoid serious consequences which\\r\\ncan be caused by AI operated in unexpected conditions. To this end, Fujitsu commits to\\r\\nleverage its accumulated experience and know how to develop and constantly improve\\r\\nreliability of AI. Moreover, preparing for the unlikely event of serious consequences occurring,\\r\\nFujitsu will seek appropriate measures to track the causes and effects of any such situation.',\n",
              " 'HPE SUPPORTS AI ETHICS FOR GOOD\\r\\nArtificial Intelligence (AI) is a powerful, transformative technology that can amplify human capabilities, but also presents risks. Investing in AI ethics and its principles is a responsibility that HPE takes seriously, and we believe in ethical and responsible AI principles:\\r\\nAI privacy-enabled security\\r\\nRespect individuals privacy, be secure and minimize the risk of errors and unintended, malicious use.\\r\\nAI human focused principle\\r\\nRespect human rights and be designed with mechanisms and safeguards, such as human oversight to prevent misuse.\\r\\nAI inclusivity principle\\r\\nBe inclusive, minimize harmful bias, ensure fair and equal treatment and access for individuals.\\r\\nAI robust principle\\r\\nBe engineered to build in quality testing, include safeguards to maintain functionality, and minimize misuse and impact of failure.\\r\\nResponsible AI\\r\\nBe designed to enable responsible and accountable use, allow an understanding of AI and for outcomes to be challenged.',\n",
              " 'HPE SUPPORTS AI ETHICS FOR GOOD\\r\\nArtificial Intelligence (AI) is a powerful, transformative technology that can amplify human capabilities, but also presents risks. Investing in AI ethics and its principles is a responsibility that HPE takes seriously, and we believe in ethical and responsible AI principles:\\r\\nAI privacy-enabled security\\r\\nRespect individuals privacy, be secure and minimize the risk of errors and unintended, malicious use.\\r\\nAI human focused principle\\r\\nRespect human rights and be designed with mechanisms and safeguards, such as human oversight to prevent misuse.\\r\\nAI inclusivity principle\\r\\nBe inclusive, minimize harmful bias, ensure fair and equal treatment and access for individuals.\\r\\nAI robust principle\\r\\nBe engineered to build in quality testing, include safeguards to maintain functionality, and minimize misuse and impact of failure.\\r\\nResponsible AI\\r\\nBe designed to enable responsible and accountable use, allow an understanding of AI and for outcomes to be challenged.',\n",
              " 'innovation and protect consumers by regulating AI based on use-cases and end users, rather than the underlying\\r\\ntechnology. We support policies that advance the five business imperatives that we have called for in our IBM Policy\\r\\nLab piece - having an AI ethics official; different rules for different risks; don’t hide your AI, and test your AI. That is\\r\\nwhy we supported the National AI Initiative Act which directs NIST to develop an accountability framework to\\r\\nadvance explainable fair and trustworthy AI.\\r\\nContact: Ryan Hagemann, Ryan.Hagemann@ibm.com & Kevin Walsh, kevin.j.walsh@ibm.com',\n",
              " 'Trend 13: AI ethics throughout the development lifecycle\\r\\nResponsible AI concepts should be factored in from the beginning to ensure the business stays out of any AI ethics and bias issues. Explainability is one such critical concept. The design and development teams should be aware and informed of every step in the AI lifecycle to answer any related questions, providing all information AI users would seek to understand how and why the system made a decision. This way, an organization can remain clear of adverse ethical issues and maintain customer trust.\\r\\n\\r\\nThese scenarios demand efficient tools to make AI systems more transparent and interpretable, ensuring trust, fairness, transparency, reliability, and auditability. AI models should adhere to the following principles:\\r\\n\\r\\nPurposeful: An AI system should be designed with empathy and follow a human-centric approach with socially responsible use cases. For example, consider user preferences and behavior to provide recommendations.\\r\\nEthical: Models should comply with legal and social structures and be designed with high-cost functions that prevent unethical behavior. There should be transparency in data and models.\\r\\nHuman reviewed: Although AI models are built to operate independently without human interference, human dependency is a necessity in some cases. For example, in fraud detection or cases where law enforcement is involved, human supervision is required to review decisions made by AI models.\\r\\nBias detection: An unbiased dataset is an important prerequisite for reliable and nondiscriminatory predictions. AI models are being used for credit scoring by banks, resume shortlisting, and in some judicial systems. However, some datasets were found with an inherent bias toward color, age, and/or sex.\\r\\nExplainable: Models should enable easy interpretation of results such as predictions, recommendations, etc. Explainable AI helps understand the decision-making process of AI systems and recognize which features of the given input are emphasized while making predictions.\\r\\nAccountable: Models should use telemetry for auditing all human and machine actions. There should be data lineage for traceability, and all models/datasets should be version controlled.\\r\\nReproductive: The ML model should be consistent when giving predictions. Many practitioners think that explainable AI (XAI) is applied only at the output stage, but the role of XAI is throughout the whole AI lifecycle.\\r\\nThus, consistent and continuous governance can make AI systems understandable and resilient in various situations.',\n",
              " 'Intel is committed to advancing AI technology responsibly. We do this by utilizing rigorous, multidisciplinary review processes throughout the development lifecycle, establishing diverse development teams to reduce biases, and collaborating with industry partners to mitigate potentially harmful uses of AI. We are committed to implementing leading processes founded on international standards and industry best practices. AI has come a long way but there is still so much more to be discovered, as technology evolves. We are continuously finding ways to use this technology to drive positive change and better mitigate risks. We continue to collaborate with academia and industry partners to advance research in this area while also evolving our platforms to make responsible AI solutions computationally tractable and efficient.',\n",
              " 'We deliver trusted artificial intelligence and machine learning (AI/ML) solutions that provide real and immediate value. Our cutting-edge technology makes AI/ML algorithms trustworthy, resilient, and secure. When combined with tools that provide transparency and eliminate bias, we offer mission-focused AI/ML solutions that are trusted and valued by customers and built on our deep experience.',\n",
              " 'Preview – Microsoft Responsible AI Standard v2 – Introduction\\r\\n1\\r\\nMicrosoft\\r\\nResponsible AI\\r\\nStandard, v2\\r\\nGENERAL REQUIREMENTS\\r\\nFOR EXTERNAL RELEASE\\r\\nJune 2022\\r\\n\\r\\nAbout this release\\r\\nWhen we embarked on our effort to operationalize Microsoft’s six AI principles, we knew there was a policy\\r\\ngap. Laws and norms had not caught up with AI’s unique risks or society’s needs. Yet, our product development\\r\\nteams needed concrete and actionable guidance as to what our principles meant and how they could uphold\\r\\nthem. We leveraged the expertise on our research, policy, and engineering teams to develop guidance on how\\r\\nto fill that gap.\\r\\nThe Responsible AI Standard is the product of a multi-year effort to define product development requirements\\r\\nfor responsible AI. We are making available this second version of the Responsible AI Standard to share what\\r\\nwe have learned, invite feedback from others, and contribute to the discussion about building better norms\\r\\nand practices around AI.\\r\\nWhile our Standard is an important step in Microsoft’s responsible AI journey, it is just one step. As we make\\r\\nprogress with implementation, we expect to encounter challenges that require us to pause, reflect, and adjust.\\r\\nOur Standard will remain a living document, evolving to address new research, technologies, laws, and\\r\\nlearnings from within and outside the company.\\r\\nThere is a rich and active global dialog about how to create principled and actionable norms to ensure\\r\\norganizations develop and deploy AI responsibly. We have benefited from this discussion and will continue to\\r\\ncontribute to it. We believe that industry, academia, civil society, and government need to collaborate to\\r\\nadvance the state-of-the-art and learn from one another. Together, we need to answer open research\\r\\nquestions, close measurement gaps, and design new practices, patterns, resources, and tools.\\r\\nAs we continue our journey, we welcome feedback on our approach and insights on other ways forward:\\r\\nhttps://aka.ms/ResponsibleAIQuestions\\r\\nMicrosoft Responsible AI Standard v2\\r\\nAccountability Goals\\r\\nGoal A1: Impact assessment\\r\\nMicrosoft AI systems are assessed using Impact Assessments.\\r\\nApplies to: All AI systems.\\r\\nRequirements\\r\\nA1.1 Assess the impact of the system on people, organizations, and society by completing an Impact Assessment\\r\\nearly in the system’s development, typically when defining the product vision and requirements. Document the\\r\\neffort using the Impact Assessment template provided by the Office of Responsible AI.\\r\\nTags: Impact Assessment.\\r\\nA1.2 Review the completed Impact Assessment with the reviewers identified according to your organization’s\\r\\ncompliance process before development starts. Secure all required approvals from those reviewers.\\r\\nTags: Impact Assessment.\\r\\nA1.3 Update and review the Impact Assessment at least annually, when new intended uses are added, and before\\r\\nadvancing to a new release stage.\\r\\nTags: Impact Assessment.\\r\\nMicrosoft Responsible AI Standard v2\\r\\nGoal A2: Oversight of significant adverse impacts\\r\\nMicrosoft AI systems are reviewed to identify systems that may have a significant adverse impact on people,\\r\\norganizations, and society, and additional oversight and requirements are applied to those systems.\\r\\nApplies to: All AI systems.\\r\\nRequirements\\r\\nA2.1 Review defined Restricted Uses to determine whether the system meets the definition of any Restricted Use.\\r\\nIf it does, document this in the Impact Assessment, and follow the requirements for the Restricted Use.\\r\\nTags: Impact Assessment.\\r\\nA2.2 Answer prompts in the Impact Assessment template to determine whether the system meets the definition of\\r\\na Sensitive Use. If it does, report it to the Office of Responsible AI, and follow any additional requirements resulting\\r\\nfrom a Sensitive Uses review.\\r\\nTags: Impact Assessment.\\r\\nA2.3 Review your systems at least annually against the definitions for Sensitive Uses and Restricted Uses. If there\\r\\nare systems that meet the criteria for Sensitive Uses, report them to the Office of Responsible AI. If there are\\r\\nsystems that meet the criteria for Restricted Uses, notify the Office of Responsible AI.\\r\\nMicrosoft Responsible AI Standard v2\\r\\nGoal A3: Fit for purpose\\r\\nMicrosoft AI systems are fit for purpose in the sense that they provide valid solutions for the problems they are\\r\\ndesigned to solve.\\r\\nApplies to: All AI systems.\\r\\nRequirements\\r\\nA3.1 Document in the Impact Assessment how the system’s use will solve the problem posed by each intended use,\\r\\nrecognizing that there may be multiple valid ways in which to solve the problem.\\r\\nTags: Impact Assessment.\\r\\nA3.2 Define and document for each model in the AI system:\\r\\n1) the model’s proposed inputs and how well they represent the concepts they are intended to represent; include\\r\\nanalysis of the limitations of this representation,\\r\\n2) the model’s proposed output and how well it represents the concept it is intended to represent; include analysis\\r\\nof the limitations of this representation, and\\r\\n3) limitations to the generalizability of the resulting model based on the training and testing data that will be used.\\r\\nA3.3 Define and document Responsible Release Criteria for this Goal. Include:\\r\\n1) a concise definition of the problem being solved in the intended use,\\r\\n2) performance metrics and their Responsible Release Criteria, and\\r\\n3) error types and their Responsible Release Criteria.\\r\\nA3.4 Document an evaluation plan for each of the performance metrics and error types.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nA3.5 Use the methods defined in requirement A3.4 to conduct evaluations. Document the pre-release results of the\\r\\nevaluations. Determine and document how often ongoing evaluation should be conducted to continue supporting this\\r\\nGoal.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nA3.6 Provide documentation to customers which describes the system’s:\\r\\n1) intended uses, and\\r\\n2) evidence that the system is fit for purpose for each intended use.\\r\\nWhen the system is a platform service made available to external customers or partners, include this information in the\\r\\nrequired Transparency Note.\\r\\nTags: Transparency Note.\\r\\nA3.7 If an intended use is not supported by evidence, or if evidence comes to light that refutes that the system is fit for\\r\\npurpose for the intended use at any point in the system’s use:\\r\\n1) remove the intended use from customer-facing materials and make current customers aware of the issue, take\\r\\naction to close the identified gap, or discontinue the system,\\r\\n2) revise documentation related to the intended use, and\\r\\n3) publish the revised documentation to customers.\\r\\nWhen the system is a platform service made available to external customers or partners, include this information in the\\r\\nrequired Transparency Note.\\r\\nA3.8 Communicate with care about system benefits; follow any applicable guidance from your attorney.\\r\\nMicrosoft Responsible AI Standard v2\\r\\nGoal A4: Data governance and management\\r\\nMicrosoft AI systems are subject to appropriate data governance and management practices.\\r\\nApplies to: All AI systems.\\r\\nRequirements\\r\\nA4.1 Define and document data requirements with respect to the system’s intended uses, stakeholders, and the\\r\\ngeographic areas where the system will be deployed. Document these requirements in the Impact Assessment.\\r\\nTags: Impact Assessment.\\r\\nA4.2 Define and document procedures for the collection and processing of data, to include annotation, labelling,\\r\\ncleaning, enrichment, and aggregation, where relevant.\\r\\nA4.3 If you plan to use existing data sets to train the system, assess the quantity and suitability of available data\\r\\nsets that will be needed by the system in relation to the data requirements defined in A4.1. Document this\\r\\nassessment in the Impact Assessment.\\r\\nTags: Impact Assessment.\\r\\nA4.4 Define and document methods for evaluating data to be used by the system against the requirements\\r\\ndefined in A4.1.\\r\\nA4.5 Evaluate all data sets using the methods defined in requirement A4.4. Document the results of the evaluation. \\r\\nMicrosoft Responsible AI Standard v2\\r\\n8\\r\\nGoal A5: Human oversight and control\\r\\nMicrosoft AI systems include capabilities that support informed human oversight and control.\\r\\nApplies to: All AI systems.\\r\\nRequirements\\r\\nA5.1 Identify the stakeholders who are responsible for troubleshooting, managing, operating, overseeing, and\\r\\ncontrolling the system during and after deployment. Document these stakeholders and their oversight and control\\r\\nresponsibilities using the Impact Assessment template.\\r\\nTags: Impact Assessment.\\r\\nA5.2 Identify the system elements (including system UX, features, alerting and reporting functions, and\\r\\neducational materials) necessary for stakeholders identified in requirement A5.1 to effectively understand their\\r\\noversight responsibilities and carry them out. Stakeholders must be able to understand:\\r\\n1) the system’s intended uses,\\r\\n2) how to effectively execute interactions with the system,\\r\\n3) how to interpret system behavior,\\r\\n4) when and how to override, intervene, or interrupt the system, and\\r\\n5) how to remain aware of the possible tendency of over-relying on outputs produced by the system\\r\\n(“automation bias”).\\r\\nDocument the system design elements that will support relevant stakeholders for each oversight and control\\r\\nfunction.\\r\\nA5.3 When possible, design the system elements identified in A5.2. When this is not possible (for example, when\\r\\nMicrosoft is not responsible for the system UX), provide guidance on human oversight considerations to the third\\r\\nparty responsible for implementing the system elements identified in A5.2.\\r\\nA5.4 Define and document the method to be used to evaluate whether each oversight or control function can be\\r\\naccomplished by stakeholders in realistic conditions of system use. Include the metrics or rubrics that will be used\\r\\nin the evaluations. When this is not possible (for example, when Microsoft is not responsible for oversight and\\r\\ncontrol functions), provide guidance on evaluating oversight and control functions to the third party responsible\\r\\nfor evaluating oversight or control functions.\\r\\nA5.5 Define and document Responsible Release Criteria to achieve this Goal.\\r\\nA5.6 Conduct evaluations defined by requirement A5.4 using a near-release version of the system. Document the\\r\\nresults.\\r\\nA5.7 If there are Responsible Release Criteria for metrics or rubrics that have not been met, consult with the\\r\\nreviewers named in the Impact Assessment, and in the case of Sensitive Uses, with the Office of Responsible AI, to\\r\\ndevelop a plan detailing how the gap will be managed until it can be closed. Document that plan.\\r\\nTools and practices\\r\\nRecommendation A5.3.1 Follow the Guidelines for Human-AI Interaction when designing the system.\\r\\nRecommendation A5.4.1 Assign user researchers to design these evaluations.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n9\\r\\nTransparency Goals\\r\\nGoal T1: System intelligibility for decision making\\r\\nMicrosoft AI systems that inform decision making by or about people are designed to support stakeholder needs for\\r\\nintelligibility of system behavior.\\r\\nApplies to: All AI systems when the intended use of the generated outputs is to inform decision making by or\\r\\nabout people.\\r\\nRequirements\\r\\nT1.1 Identify:\\r\\n1) stakeholders who will use the outputs of the system to make decisions, and\\r\\n2) stakeholders who are subject to decisions informed by the system.\\r\\nDocument these stakeholders using the Impact Assessment template.\\r\\nTags: Impact Assessment.\\r\\nT1.2 Design the system, including, when possible, the system UX, features, reporting functions, and educational\\r\\nmaterials, so that stakeholders identified in requirement T1.1 can:\\r\\n1) understand the system’s intended uses,\\r\\n2) interpret relevant system behavior effectively (i.e., in a way that supports informed decision making), and\\r\\n3) remain aware of the possible tendency of over-relying on outputs produced by the system (\"automation\\r\\nbias\").\\r\\nFor the two categories of stakeholders identified in requirement T1.1, document:\\r\\n1) how the system design will support their understanding of the system’s intended uses, and\\r\\n2) how the system aids their ability to interpret relevant system responses, and\\r\\n3) how the system design discourages automation bias.\\r\\nT1.3 Define and document the method to be used to evaluate whether each stakeholder who will make decisions\\r\\nor be subject to decisions based on the behavior of the system can interpret the relevant system responses\\r\\nreasonably well. Include the metrics or rubrics that will be used in the evaluations.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nT1.4 Define and document a Responsible Release Plan, to include Responsible Release Criteria to achieve this\\r\\nGoal.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nT1.5 Conduct evaluations defined by requirement T1.3. Document the pre-release results of the evaluations.\\r\\nDetermine and document how often ongoing evaluation should be conducted to continue supporting this Goal.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nT1.6 If there are Responsible Release Criteria for metrics or rubrics that that have not been met, consult with the\\r\\nreviewers named in the Impact Assessment, and in the case of Sensitive Uses, with the Office of Responsible AI, to\\r\\ndevelop a plan detailing how the gap will be managed until it can be closed. Document that plan.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n10\\r\\nTools and practices\\r\\nRecommendation T1.2.1 Follow the Guidelines for Human-AI Interaction when designing the system.\\r\\nRecommendation T1.2.2 Use one or more techniques available as part of the Interpret ML toolkit to understand\\r\\nthe impact of features on system behavior. This may help stakeholders who need to understand model predictions.\\r\\nRecommendation T1.3.1 Assign user researchers to define, design, and prioritize evaluations in appropriately\\r\\nrealistic contexts of use.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n11\\r\\nGoal T2: Communication to stakeholders\\r\\nMicrosoft provides information about the capabilities and limitations of our AI systems to support stakeholders in\\r\\nmaking informed choices about those systems.\\r\\nApplies to: All AI systems.\\r\\nRequirements\\r\\nT2.1 Identify:\\r\\n1) stakeholders who make decisions about whether to employ a system for particular tasks, and\\r\\n2) stakeholders who develop or deploy systems that integrate with this system.\\r\\nDocument these stakeholders in the Impact Assessment template.\\r\\nTags: Impact Assessment.\\r\\nT2.2 Publish documentation for the system so that stakeholders defined in T2.1 can understand the system.\\r\\nInclude:\\r\\n1) capabilities,\\r\\n2) intended uses,\\r\\n3) uses that require extra care or guidance,\\r\\n4) operational factors and settings that allow for effective and responsible system use,\\r\\n5) limitations, including uses for which the system was not designed or evaluated, and\\r\\n6) evidence of system accuracy and performance as well as a description of the extent to which these results\\r\\nare generalizable across use cases that were not part of the evaluation.\\r\\nWhen the system is a platform service made available to external customers or partners, a Transparency Note is\\r\\nrequired.\\r\\nTags: Transparency Note.\\r\\nT2.3 Review and update documentation annually or when any of the following events occur:\\r\\n1) new uses are added,\\r\\n2) functionality changes,\\r\\n3) the product moves to a new release stage,\\r\\n4) new information about reliable and safe performance becomes known as defined by requirement RS3.3, or\\r\\n5) new information about system accuracy and performance becomes available.\\r\\nWhen the system is a platform service made available to external customers or partners, include this information in\\r\\nthe required Transparency Note.\\r\\nTags: Transparency Note.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n12\\r\\nGoal T3: Disclosure of AI interaction\\r\\nMicrosoft AI systems are designed to inform people that they are interacting with an AI system or are using a system\\r\\nthat generates or manipulates image, audio, or video content that could falsely appear to be authentic.\\r\\nApplies to: AI systems that impersonate interactions with humans, unless it is obvious from the circumstances\\r\\nor context of use that an AI system is in use. AI systems that generate or manipulate image, audio, or video\\r\\ncontent that could falsely appear to be authentic.\\r\\nRequirements\\r\\nT3.1 Identify stakeholders who will use or be exposed to the system, in accordance with the Impact Assessment\\r\\nrequirements. Document these stakeholders using the Impact Assessment template.\\r\\nTags: Impact Assessment.\\r\\nT3.2 Design the system, including system UX, features, reporting functions, educational materials, and outputs so\\r\\nthat stakeholders identified in T3.1 will be informed of the type of AI system they are interacting with or exposed\\r\\nto. Ensure that any image, audio, or video outputs that are intended to be used outside the system are labelled as\\r\\nbeing produced by AI.\\r\\nT3.3 Define and document the method to be used to evaluate whether each stakeholder identified in T3.1 is\\r\\ninformed of the type of AI system they are interacting with or exposed to.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nT3.4 Define and document Responsible Release Criteria to achieve this Goal.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nT3.5 Conduct evaluations defined by requirement T3.3. Document the pre-release results of the evaluations.\\r\\nDetermine and document how often ongoing evaluation should be conducted to continue supporting this goal.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n13\\r\\nFairness Goals\\r\\nGoal F1: Quality of service\\r\\nMicrosoft AI systems are designed to provide a similar quality of service for identified demographic groups,\\r\\nincluding marginalized groups.\\r\\nApplies to: AI systems when system users or people impacted by the system with different demographic\\r\\ncharacteristics might experience differences in quality of service that Microsoft can remedy by building the system\\r\\ndifferently.\\r\\nRequirements\\r\\nF1.1 Identify and prioritize demographic groups, including marginalized groups, that may be at risk of\\r\\nexperiencing worse quality of service based on intended uses and geographic areas where the system will be\\r\\ndeployed. Include:\\r\\n1) groups defined by a single factor, and\\r\\n2) groups defined by a combination of factors.\\r\\nDocument the prioritized identified demographic groups using the Impact Assessment template.\\r\\nTags: Impact Assessment.\\r\\nF1.2 Evaluate all data sets to assess inclusiveness of identified demographic groups and collect data to close gaps.\\r\\nDocument this process and its results.\\r\\nF1.3 Define and document the evaluation that you will perform to support this Goal. Include:\\r\\n1) any system components to be evaluated, in addition to the whole system,\\r\\n2) the metrics to be used to evaluate the system components and the whole system, and\\r\\n3) a description of the data set to be used for this evaluation.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF1.4 Define and document Responsible Release Criteria to achieve this Goal, as follows:\\r\\nFor each metric, document:\\r\\n1) any target minimum performance level for all groups, and\\r\\n2) the target maximum (absolute or relative) performance difference between groups.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF1.5 Evaluate the system according to the defined Responsible Release Criteria.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF1.6 Reassess the system design, including the choice of training data, features, objective function, and training\\r\\nalgorithm, to pursue the goals of:\\r\\n1) improving performance for any identified demographic group that does not meet any target minimum\\r\\nperformance level, and\\r\\n2) minimizing performance differences between identified demographic groups, paying particular attention\\r\\nto those that exceed the target maximum, while recognizing that doing so may appear to affect system\\r\\nperformance and that it is seldom clear how to make such tradeoffs.\\r\\nConsult with your attorney to determine your approach to this, including how you will identify and document\\r\\ntradeoffs.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n14\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF1.7 Identify and document any justifiable factors, such as circumstantial and other operational factors (e.g.,\\r\\n“background noise” for speech recognition systems or “image resolution” for facial recognition systems), that\\r\\naccount for:\\r\\n1) any inability to meet any target minimum performance level for any identified demographic group, and\\r\\n2) any remaining performance differences between identified demographic groups.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF1.8 Document the pre-release results from requirements F1.4, F1.5, and F1.6. Determine and document how often\\r\\nongoing evaluation should be conducted to continue supporting this Goal.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF1.9 Publish information for customers about:\\r\\n1) identified demographic groups for which performance may not meet any target minimum performance\\r\\nlevel,\\r\\n2) any remaining performance disparities between identified demographic groups that may exceed the target\\r\\nmaximum, and\\r\\n3) any justifiable factors that account for these performance levels and differences.\\r\\nWhen the system is a platform service made available to external customers or partners, include this information in\\r\\nthe required Transparency Note.\\r\\nTags: Transparency Note.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n15\\r\\nTools and practices\\r\\nRecommendation F1.1.1 For identifying people by age, gender identity, and ancestry in North America, use Best\\r\\nPractices for Age, Gender Identity, and Ancestry.\\r\\nRecommendation F1.1.2 Work with user researchers to understand variations in demographic groups across\\r\\nintended uses and geographic areas.\\r\\nRecommendation F1.1.3 Work with domain-specific subject matter experts to understand the factors that impact\\r\\nperformance of your system and how they vary across identified demographic groups in this domain.\\r\\nRecommendation F1.1.4 Work with members of identified demographic groups to understand the risks of and\\r\\nimpacts associated with differences in quality of service. Consider using the Community Jury technique to conduct\\r\\nthese discussions.\\r\\nRecommendation F1.2.1 Use Analysis Platform to understand the representation of identified demographic\\r\\ngroups in data sets that you plan to use for training and evaluating your system, respecting privacy controls for\\r\\nworking with sensitive data.\\r\\nRecommendation F1.2.2 Document the representation of identified demographic groups in a Datasheet.\\r\\nRecommendation F1.5.1 Use the Fairlearn Python toolkit’s assessment and mitigation capabilities, if appropriate\\r\\nfor the system.\\r\\nRecommendation F1.5.2 Use Error Analysis to help understand factors that may account for performance levels\\r\\nand differences, if appropriate for the system.\\r\\nRecommendation F1.5.3 Use one or more techniques available as part of the Interpret ML toolkit to help\\r\\nunderstand factors that may account for performance levels and differences, if appropriate for the system.\\r\\nRecommendation F1.6.1 Use the Fairlearn Python toolkit’s assessment and mitigation capabilities, if appropriate\\r\\nfor the system.\\r\\nRecommendation F1.6.2 Be prepared to collect additional training data for identified demographic groups.\\r\\nRecommendation F1.7.1 Use Error Analysis to help understand factors that may account for performance levels\\r\\nand differences, if appropriate for the system.\\r\\nRecommendation F1.7.2 Use one or more techniques available as part of the Interpret ML toolkit to help\\r\\nunderstand factors that may account for performance levels and differences, if appropriate for the system.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n16\\r\\nGoal F2: Allocation of resources and opportunities\\r\\nMicrosoft AI systems that allocate resources or opportunities in essential domains are designed to do so in a manner\\r\\nthat minimizes disparities in outcomes for identified demographic groups, including marginalized groups.\\r\\nApplies to: AI systems that generate outputs that directly affect the allocation of resources or opportunities relating\\r\\nto finance, education, employment, healthcare, housing, insurance, or social welfare.\\r\\nRequirements\\r\\nF2.1 Identify and prioritize demographic groups, including marginalized groups, that may be at risk of being\\r\\ndifferentially impacted by the system based on intended uses and geographic areas where the system will be\\r\\ndeployed. Include:\\r\\n1) groups defined by a single factor, and\\r\\n2) groups defined by a combination of factors.\\r\\nDocument the prioritized identified demographic groups using the Impact Assessment template.\\r\\nTags: Impact Assessment.\\r\\nF2.2 Evaluate all data sets to assess inclusiveness of identified demographic groups and collect data to close any\\r\\ngaps. Document this process and its results.\\r\\nF2.3 Define and document the evaluation that you will perform to support this Goal. Include:\\r\\n1) any system components to be evaluated, in addition to the whole system,\\r\\n2) the metrics to be used to evaluate the system components and the whole system, and\\r\\n3) the data set to be used for this evaluation.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF2.4 Define and document Responsible Release Criteria to achieve this Goal, as follows:\\r\\nFor each metric, document the target maximum difference (absolute or relative) between the rates at which\\r\\nresources and opportunities are allocated to groups.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF2.5 Evaluate the system according to the defined Responsible Release Criteria.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF2.6 Reassess the system design, including the choice of training data, features, objective function, and training\\r\\nalgorithm, to pursue the goal of minimizing differences between the rates at which resources and opportunities\\r\\nare allocated to identified demographic groups, paying particular attention to those that exceed the target\\r\\nmaximum difference, while recognizing that doing so may appear to affect system performance and it is seldom\\r\\nclear how to make such trade-offs.\\r\\nConsult with your attorney to determine your approach to this, including how you will identify and document\\r\\ntrade-offs.\\r\\nTags: Ongoing Evaluation Checkpoint. \\r\\nMicrosoft Responsible AI Standard v2\\r\\n17\\r\\nF2.7 Identify and document any justifiable factors that account for any remaining differences between the rates at\\r\\nwhich resources and opportunities are allocated to identified demographic groups.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF2.8 Document the pre-release results for the evaluation described by requirements F2.4, F2.5, and F2.6.\\r\\nDetermine and document how often ongoing evaluation should be conducted to continue supporting this goal.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF2.9 Publish information for customers about:\\r\\n1) any remaining differences between the rates at which resources and opportunities are allocated to\\r\\nidentified demographic groups, and\\r\\n2) any justifiable factors that account for these differences. When the system is a platform service made\\r\\navailable to external customers or partners, include this information in the required Transparency Note.\\r\\nTags: Transparency Note.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n18\\r\\nTools and practices\\r\\nRecommendation F2.1.1 For North America, use Best Practices for Age, Gender Identity, and Ancestry to help\\r\\nidentify demographic groups and methods for collecting demographic information.\\r\\nRecommendation F2.1.2 Work with user researchers to understand variations in demographic groups across\\r\\nintended uses and geographic areas.\\r\\nRecommendation F2.1.3 Work with domain-specific subject matter experts to understand the facts that impact\\r\\nperformance of your system and how they vary across identified demographic groups in this domain.\\r\\nRecommendation F2.1.4 Work with members of identified demographic groups to understand risks of and\\r\\nimpacts associated with differences between the rates at which resources and opportunities are allocated.\\r\\nRecommendation F2.2.1 Use Analysis Platform to understand the representation of identified demographic\\r\\ngroups, respecting privacy requirements for using sensitive data.\\r\\nRecommendation F2.2.2 Document the representation of identified demographic groups in a Datasheet.\\r\\nRecommendation F2.5.1 Use the Fairlearn Python toolkit’s assessment and mitigation capabilities, if appropriate\\r\\nfor the system.\\r\\nRecommendation F2.5.2 Use Error Analysis to help understand factors that may account for differences between\\r\\nthe rates at which resources and opportunities are allocated to the identified demographic groups, if appropriate\\r\\nfor the system.\\r\\nRecommendation F2.5.3 Use one or more techniques available as part of the Interpret ML toolkit to help\\r\\nunderstand factors that may account for differences between the rates at which resources and opportunities are\\r\\nallocated to the identified demographic groups, if appropriate for the system.\\r\\nRecommendation F2.6.1 Use the Fairlearn Python toolkit’s assessment and mitigation capabilities, if appropriate\\r\\nfor the system.\\r\\nRecommendation F2.7.1 Use Error Analysis to help understand factors that may account for differences between\\r\\nthe rates at which resources and opportunities are allocated to the identified demographic groups, if appropriate\\r\\nfor the system.\\r\\nRecommendation F2.7.2 Use Interpret ML to help understand factors that may account for differences between\\r\\nthe rates at which resources and opportunities are allocated to the identified demographic groups, if appropriate\\r\\nfor the system.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n19\\r\\nGoal F3: Minimization of stereotyping, demeaning, and erasing outputs\\r\\nMicrosoft AI systems that describe, depict, or otherwise represent people, cultures, or society are designed to\\r\\nminimize the potential for stereotyping, demeaning, or erasing identified demographic groups, including\\r\\nmarginalized groups.\\r\\nApplies to: AI systems when system outputs include descriptions, depictions, or other representations of people,\\r\\ncultures, or society.\\r\\nRequirements\\r\\nF3.1 Identify and prioritize demographic groups, including marginalized groups, that may be at risk of being\\r\\nsubject to stereotyping, demeaning, or erasing outputs of the system. Include:\\r\\n1) groups defined by a single factor, and\\r\\n2) groups defined by a combination of factors.\\r\\nDocument the prioritized identified demographic groups using the Impact Assessment template.\\r\\nTags: Impact Assessment.\\r\\nF3.2 Define and document any system components to be evaluated, in addition to the whole system.\\r\\nF3.3 Define and document a plan to evaluate the system components and the whole system for risks of\\r\\nstereotyping, demeaning, and erasing the prioritized identified demographic groups.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF3.4 Evaluate the system according to the plan defined in requirement F3.3.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF3.5 Reassess the system design, including the choice of training data, features, objective function, and training\\r\\nalgorithm, to pursue the goal of minimizing the potential for stereotyping, demeaning, and erasing the identified\\r\\ndemographic groups.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF3.6 Document the pre-release results from requirements F3.4 and F3.5. Determine and document how often\\r\\nongoing evaluation should be conducted to continue supporting this goal.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nF3.7 Publish information for customers about these risks involving identified demographic groups. When the\\r\\nsystem is a platform service made available to external customers or partners, include this information in the\\r\\nrequired Transparency Note.\\r\\nTags: Transparency Note.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n20\\r\\nTools and practices\\r\\nRecommendation F3.1.1 Work with user researchers, subject matter experts, and members of identified\\r\\ndemographic groups to understand these risks and their impacts.\\r\\nRecommendation F3.4.1 Use CheckList to help evaluate these risks involving identified demographic groups, if\\r\\nappropriate for the system.\\r\\nRecommendation F3.4.2 Use red teaming exercises to evaluate these risks involving identified demographic\\r\\ngroups.\\r\\nRecommendation F3.5.1 Mitigate any risks of these types of harms that you can. In addition, establish feedback\\r\\nmechanisms and a plan for addressing problems, in alignment with Reliability and Safety Goal RS3. Note that this\\r\\napproach is recommended in acknowledgment of the fact that the state-of-the-art in mitigating these risks is less\\r\\nadvanced than the state-of-the-art in mitigating differences in quality of service or allocative harms.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n21\\r\\nReliability & Safety Goals\\r\\nGoal RS1: Reliability and safety guidance\\r\\nMicrosoft evaluates the operational factors and ranges within which AI systems are expected to perform reliably and\\r\\nsafely, remediates issues, and provides related information to customers.\\r\\nApplies to: All AI systems.\\r\\nRequirements\\r\\nRS1.1 Document how:\\r\\n1) reliable and safe behavior is defined for this system and,\\r\\n2) what acceptable error rates are for overall system performance in the context of intended uses.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nRS1.2 Evaluate training and test data sets to ensure that they include representation of the intended uses,\\r\\noperational factors, and an appropriate range of settings for each factor. Document the evaluation.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nRS1.3 Determine and document the operational factors, including quality of system input, use, and operational\\r\\ncontext that are critical to manage for reliable and safe use of the system in its deployed context.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nRS1.4 Define and document acceptable ranges for each operational factor important to support reliable and safe\\r\\nsystem use. Define and document an acceptable error rate for the system when operating within these ranges.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nRS1.5 Define intended uses, if any, where additional operational factors, more narrow or different acceptable\\r\\nranges, or lower acceptable error rates (including false positive and false negative error rates), are advised to\\r\\nensure reliability and safety. Document your conclusions.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nRS1.6 Define and document an evaluation plan based on requirements RS1.1, RS1.3, RS1.4, and RS1.5, to include\\r\\nthe environment in which the system will be evaluated.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nRS1.7 Evaluate the system according to the evaluation plan defined in requirement RS1.6 to ensure reliable and\\r\\nsafe system behavior. Document the pre-release results of the evaluation. Determine and document how often\\r\\nongoing evaluation should be conducted to continue supporting this goal.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nRS1.8 In the event of failure cases within operational factors and defined ranges, work to resolve the issues. If the\\r\\nResponsible Release Criteria established in requirements RS1.1, RS1.3, RS1.4, and RS1.5 cannot be met, a\\r\\nreassessment of intended uses and updated documentation is required.\\r\\nTags: Ongoing Evaluation Checkpoint.\\r\\nRS1.9 Provide documentation to customers and potential customers of the system that includes the outputs of\\r\\nrequirements RS1.2, RS1.7 and RS1.8, and any unsupported uses defined in the Impact Assessment and in RS1.8.\\r\\nWhen the system is a platform service made available to external customers or partners, include this information in\\r\\nthe required Transparency Note.\\r\\nTags: Impact Assessment, Transparency Note.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n22\\r\\nTools and practices\\r\\nRecommendation RS1.1.1 Interview safety experts and review relevant literature for domains where the system\\r\\nmay impact the safety of people.\\r\\nRecommendation RS1.4.1 Interview customers to understand operational factors and their variations.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n23\\r\\nGoal RS2: Failures and remediations\\r\\nMicrosoft AI systems are designed to minimize the time to remediation of predictable or known failures.\\r\\nApplies to: All AI systems.\\r\\nRequirements\\r\\nRS2.1 Define predictable failures, including false positive and false negative results for the system as a whole\\r\\nand how they would impact stakeholders for each intended use. Use the Impact Assessment template to\\r\\ndocument any adverse impacts of these failures on stakeholders.\\r\\nTags: Impact Assessment.\\r\\nRS2.2 For each case of a predictable failure likely to have an adverse impact on a stakeholder, document the\\r\\nfailure management approach:\\r\\n1) When possible, design and build the system to avoid this failure. Describe the design solution.\\r\\nEstimate the time range for resolving predictable failures for each designed solution or indicate that\\r\\nthe failure will be prevented by design.\\r\\n2) When a failure cannot be prevented by design, build a fallback option that may be used when this\\r\\nfailure occurs. Describe the fallback option and document the estimated time required to invoke and\\r\\nuse the fallback option.\\r\\n3) Provide training and documentation for stakeholders accountable for system oversight that supports\\r\\ntheir resolution of the failure. Describe the documentation and training.\\r\\nRS2.3 Document your plan for managing previously unknown failures that come to light once the system is\\r\\nin use:\\r\\n1) Describe the system’s rollback plan and document the time that may elapse until the entire system,\\r\\nacross all endpoints can be rolled back.\\r\\n2) Describe support for turning features off and document the time that may elapse until the feature can\\r\\nbe turned off across all endpoints.\\r\\n3) Describe the process for updating and releasing updates to each model and document the time that\\r\\nmay elapse until the system has been updated across all endpoints.\\r\\n4) Describe how customers, partners, and end users will be notified of changes to the system, updated\\r\\nunderstandings of failures, and their best mitigations.\\r\\nRS2.4 Provide training and documentation for system owners, developers, customer support and other\\r\\nstakeholders responsible for managing the system to support their remediation and mitigation of predictable\\r\\nfailures identified in requirement RS2.1. Document the training and documentation provided.\\r\\nTools and practices\\r\\nRecommendation RS2.1.1 Conduct Failure Mode and Effects Analysis.\\r\\nRecommendation RS2.2.1 Follow the Guidelines for Human-AI Interaction when designing the system to help\\r\\nmanage failures.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n24\\r\\nGoal RS3: Ongoing monitoring, feedback, and evaluation\\r\\nMicrosoft AI systems are subject to ongoing monitoring, feedback, and evaluation so that we can identify and review\\r\\nnew uses, identify and troubleshoot issues, manage and maintain the systems, and improve them over time.\\r\\nApplies to: All AI systems.\\r\\nRequirements\\r\\nRS3.1 Establish and document a detailed inventory of the system health monitoring methods to be used, to\\r\\ninclude:\\r\\n1) data and insights generated from data repositories, system analytics, and associated alerts,\\r\\n2) processes by which customers can submit information about failures and concerns, and\\r\\n3) processes by which the general public can submit feedback.\\r\\nRS3.2 Define and document a standard operating procedure and system health monitoring action plan for each\\r\\nmonitoring channel for the system, to include:\\r\\n1) processes for reproducing system failures to support troubleshooting and prevention of future failures,\\r\\n2) which events will be monitored,\\r\\n3) how events will be prioritized for review,\\r\\n4) the expected frequency of those reviews,\\r\\n5) how events will be prioritized for response and timing to resolution,\\r\\n6) how high priority issues related to supporting the Standard and its goals will be escalated to the Office of\\r\\nResponsible AI, and\\r\\n7) engaging customer service to ensure that they are aware of how to respond to issues for the system.\\r\\nRS3.3 When new uses, critical operational factors, or changes in the supported range of an operational factor are\\r\\nidentified, determine whether any new use or operational factor can be supported with the existing system, will be\\r\\nsupported but require additional work, or will not be supported.\\r\\n• When new uses or operational factors identified are to be supported, evaluate the updated system in\\r\\naccordance with requirement RS1.6, add the new intended use to the Impact Assessment, and publish\\r\\nupdated communication in accordance with requirement RS1.9.\\r\\n• When these new uses or operational factor range changes cannot or will not be accommodated to ensure\\r\\nreliable and safe performance of the system update customer documentation described in RS1.9 to include\\r\\nthe new use as an unsupported use.\\r\\nWhen the system is a platform service made available to external customers or partners, include this information in\\r\\nthe required Transparency Note.\\r\\nTags: Impact Assessment, Transparency Note.\\r\\nRS3.4 When a system is to be used for a Sensitive Use that imposes qualification or quality control requirements\\r\\nbeyond the intended uses and/or operational factor ranges, conduct an evaluation specific to this use. If the\\r\\nrequired Responsible Release Criteria cannot be met, the Office of Responsible AI will review the results and decide\\r\\nhow to proceed. Document any changes to the Responsible Release Criteria and document the results of\\r\\nevaluation.\\r\\nRS3.5 Conduct all evaluations tagged as Ongoing Evaluation Checkpoints in other Goals on an ongoing basis.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n25\\r\\nRS3.6 If there are targets in Ongoing Evaluation Checkpoints that are no longer satisfied, consult with named\\r\\nreviewers, and in the case of Sensitive Uses, with the Office of Responsible AI, to develop and implement a plan to\\r\\nclose any gaps. Document the process, its results, and conclusions.\\r\\nRS3.7 If evidence comes to light that refutes the system is fit for purpose for an intended use at any point in the\\r\\nsystem’s use:\\r\\n1) remove the intended use from customer-facing materials and make current customers aware of the issue,\\r\\ntake action to close the identified gap, or discontinue the system,\\r\\n2) revise documentation related to the intended use, and\\r\\n3) publish the revised documentation to customers.\\r\\nWhen the system is a platform service made available to external customers or partners, include this information in\\r\\nthe required Transparency Note.\\r\\nTags: Transparency Note.\\r\\nRS3.8: Review and update documentation required by Goal T2 when any of the following events occur:\\r\\n1) new uses are added,\\r\\n2) functionality changes,\\r\\n3) new information about reliable and safe performance becomes known as defined by requirement RS3.3, or\\r\\n4) new information about system accuracy and performance becomes available.\\r\\nWhen the system is a platform service made available to external customers or partners, include this information in\\r\\nthe required Transparency Note.\\r\\nTags: Transparency Note.\\r\\nRS3.9: Escalate unresolved issues related to supporting the Standard and its requirements to the Office of\\r\\nResponsible AI.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n26\\r\\nPrivacy & Security Goals\\r\\nGoal PS1: Privacy Standard compliance\\r\\nMicrosoft AI systems are designed to protect privacy in accordance with the Microsoft Privacy Standard.\\r\\nApplies when: Microsoft Privacy Standard applies.\\r\\nGoal PS2: Security Policy compliance\\r\\nMicrosoft AI systems are designed to be secure in accordance with the Microsoft Security Policy.\\r\\nApplies when: Microsoft Security Policy applies.\\r\\nMicrosoft Responsible AI Standard v2\\r\\n27\\r\\nInclusiveness Goal\\r\\nGoal I1: Accessibility Standards compliance\\r\\nMicrosoft AI systems are designed to be inclusive in accordance with the Microsoft Accessibility Standards.\\r\\nApplies when: Microsoft Accessibility Standards apply.\\r\\nScan this code to access responsible AI resources from Microsoft:\\r\\n© 2022 Microsoft Corporation. All rights reserved. This document is provided “as-is.” It has been edited for external release to\\r\\nremove internal links, references, and examples. Information and views expressed in this document, including URL and other\\r\\nInternet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only\\r\\nand are fictitious. No real association is intended or inferred. This document does not provide you with any legal rights to any\\r\\nintellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.',\n",
              " 'PRINCIPLES AND PRACTICES FOR THE\\r\\nRESPONSIBLE APPLICATION OF ARTIFICIAL\\r\\nINTELLIGENCE AT MOTOROLA SOLUTIONS\\r\\nWHITE PAPER\\r\\nWHITE PAPER | AI AT MOTOROLA SOLUTIONS\\r\\nArtificial Intelligence (AI) will improve the efficiency, effectiveness\\r\\nand safety of the Motorola Solutions user community. However, as a\\r\\npowerful, multi-faceted emerging technology, AI can have far-reaching\\r\\nunintended consequences if applied inappropriately, or executed with\\r\\ninsufficient rigor and discipline.\\r\\nThis paper summarizes Motorola Solutions’ policies and practices for\\r\\nresponsibly applying AI in its public safety products and applications.\\r\\nOur goal is to ensure unbiased, fair, understandable, secure and reliable\\r\\noperation. We apply a human-centered design philosophy that is informed\\r\\nby copious customer research to guide our application of AI based on the\\r\\nfoundational tenets of “human in the loop” for all consequential actions,\\r\\nand an emphasis on the use of human-centered design in the application\\r\\nof AI for focused, purpose-built solutions.\\r\\nHOW ARTIFICIAL INTELLIGENCE BENEFITS OUR\\r\\nPUBLIC SAFETY USERS\\r\\nTo a layperson, AI is generally presumed to be computer technology that\\r\\ncan broadly mimic a human in performing complex tasks. In practice,\\r\\ntoday’s technology is not that advanced. Currently, AI can only emulate\\r\\naspects of human intelligence, focused on narrowly defined tasks such as\\r\\nthe ability to recognize faces, or perform a translation of audible speech\\r\\nto text. In the context of responsibly applying AI to the public safety\\r\\nworkflow, these specific task applications are, in fact, more beneficial.\\r\\nMotorola Solutions has been creating mission critical communications\\r\\nhardware and software for nearly our entire 91-year history. Through\\r\\nour acquisitions, strategic venture investments, and evolving research\\r\\nand development profile, we are assembling a comprehensive operating\\r\\nplatform for public safety. This platform is the foundation upon which\\r\\nwe can bring AI-driven outcomes to our customers. As the world creates\\r\\nexponentially increasing quantities of structured and unstructured data\\r\\nthat characterizes conditions and events, we are applying AI to interpret\\r\\nthis data, improving the efficiency, effectiveness and safety of first\\r\\nresponders and command center personnel around focused tasks.\\r\\nOur objective is to support humans throughout the public safety incident\\r\\nworkflow by augmenting their decision-making with appropriate AI\\r\\nassistance. Some ways that we are applying AI in this regard include:\\r\\n• Using AI to transcribe, translate, interpret and summarize\\r\\nspeech and text during human interactions with a public safety\\r\\nprofessional or emergency call center.\\r\\n• Utilizing AI to power natural language interactions (voice bots),\\r\\nenabling first responders to access critical information while\\r\\nallowing them to remain aware and attentive (“eyes up and hands\\r\\nfree”) at all times. Voice applications could include running a\\r\\nlicense plate check with a voice query/response; populating a\\r\\nform verbally rather than manually; or a periodic autonomous wellbeing check by asking a first responder to verbally acknowledge\\r\\ntheir condition. A key aspect of successfully executing voice\\r\\ninteraction - informed by extensive experience building missioncritical voice solutions - is building the vocabulary of our user\\r\\ngroups, e.g. 10-codes and the NATO alphabet, into the experience.\\r\\n• Leveraging AI to “watch” the exponentially increasing video\\r\\nsources available to public safety, identifying unusual occurrences\\r\\nin real time and alerting a human analyst. These use cases could\\r\\ninclude noting the appearance of smoke, the formation of a crowd,\\r\\na person crossing a security boundary, or identifying an individual\\r\\nmatching a description such as a missing person or AMBER Alert\\r\\nsubject of interest. AI also excels at searching historical video for\\r\\nitems or persons of interest.\\r\\n• Applying AI in the form of biometric identification (such as facial\\r\\nrecognition) in public safety applications to identify individuals\\r\\nwithout identification or who are incapable of identifying\\r\\nthemselves.\\r\\nIn all cases, these are tasks that humans perform manually today. We\\r\\nare simply using AI to automate the labor-intensive aspects for each.\\r\\nWHITE PAPER | AI AT MOTOROLA SOLUTIONS\\r\\nTHE NEED FOR RESPONSIBLE AI\\r\\nArtificial Intelligence is new, complex, powerful technology. Applied\\r\\ncarelessly, it can generate surprising and unintended results. Applied\\r\\nimproperly or maliciously, the technological scaling of AI can amplify\\r\\nor institutionalize undesirable outcomes - a risk of particular import for\\r\\npublic safety, given its societal impact.\\r\\nResponsible AI simply means that when applying AI in products and\\r\\nservices, one must consider the ramifications (good, bad, or unexpected)\\r\\nand constrain the outcomes to ensure proper results. Fundamentally, AI\\r\\npowered solutions must be unbiased, secure and trustworthy. Motorola\\r\\nSolutions approaches responsible AI based on our guiding policies and\\r\\nprinciples, supplanted by the underpinning human-centered design,\\r\\ndevelopment, validation, and performance monitoring processes that\\r\\nensure our AI solutions adhere to the policies.\\r\\nPOLICY AND PRINCIPLES FOR APPLYING AI\\r\\nRESPONSIBLY\\r\\nOver decades of designing mission critical systems, Motorola Solutions\\r\\nhas developed substantial design practices expertise founded in user\\r\\nresearch, iterative user-inclusive design validation, and domain-specific\\r\\nhuman factors engineering. We annually accumulate thousands of hours\\r\\nof in-field and in-situ observational user research (ethnography). We\\r\\napply this intimate understanding of user needs to a human-centered\\r\\ndesign process that results in purpose-built, focused solution designs\\r\\nspecifically optimized for the tasks at hand. We then develop prototypes\\r\\nand verification frameworks that we test and validate iteratively with\\r\\nusers to perfect the design and, ultimately, its implementation. Finally, we\\r\\napply specific human factors disciplines to our products to ensure that the\\r\\ntechnology is well-tuned to the needs of first responders operating across\\r\\ntheir daily workflows.\\r\\nWe are applying the same focused research and design-led approach in the\\r\\napplication of ‘purpose built’ AI solutions to ensure that we are aligning the\\r\\ncapabilities with specific human needs (no more, no less).\\r\\nWHITE PAPER | AI AT MOTOROLA SOLUTIONS\\r\\nOur policy for applying AI starts with three foundational tenets that are\\r\\nfundamentally focused on our users and fields of use:\\r\\n• Human in the Loop\\r\\n• Focused application resulting in purpose-built solutions\\r\\n• Applying mature AI\\r\\nHUMAN IN THE LOOP\\r\\nFundamentally, our approach is to augment human decision making while\\r\\nnever displacing or disintermediating human judgement. In this sense,\\r\\nour systems are advisory and will never take AI-generated consequential\\r\\nactions on their own. In other words, there is always a “human in the loop”\\r\\nto make the final determinations on substantial decisions. Studies indicate\\r\\nthat the best results are achieved with a combination of AI and human\\r\\nexperts1.\\r\\nOur user-centered research methodologies have led us to a specific set of\\r\\nhuman factors disciplines that we refer to as High Velocity Human Factors\\r\\n(HVHF). This recognizes that the more stress and duress an individual is\\r\\nexperiencing, the less cognitive capacity they have to apply to anything\\r\\nother than the specific event.\\r\\nFor example, if a police officer has deployed his or her weapon, there is\\r\\nlittle else they can (or should be) focused on other than the threat at hand.\\r\\nThe paradox of HVHF is that the more a user could benefit from technology\\r\\n- including AI recommendations, for example - the less mental capacity they\\r\\nhave available to leverage it.\\r\\nWe are investigating the selective application of AI to detect these\\r\\nsituations and understand the context so that we can automatically\\r\\nadapt the operation of the technology on behalf of the user. For example,\\r\\ndetecting the condition via biometric data, voice inflection, and/or\\r\\nbackground audio event detection that leads to activation of streaming body\\r\\nworn video, prioritization of communications, alerting the command center\\r\\nand nearby officers, or similar. \\r\\nWHITE PAPER\\r\\n| AI AT MOTOROLA SOLUTIONS\\r\\nFOCUSED APPLICATION AND MATURE AI\\r\\nOur two remaining foundational tenets - focused\\r\\napplication, and mature AI - are connected.\\r\\nMany respected and capable companies across a\\r\\nvariety of industries have applied substantial thought\\r\\nand effort into the considerations of how to best\\r\\napply AI in a general fashion, across a diverse set of\\r\\napplications, users, and environments2,3,4. Applying\\r\\nAI in a general manner is extremely difficult, due to\\r\\nthe wide variety of possible circumstances and user\\r\\npersonas.\\r\\nGiven our emphasis on public safety, Motorola\\r\\nSolutions can focus this particular AI challenge. We\\r\\nare creating “purpose-built” AI solutions focused on\\r\\nspecific tasks within the public safety workflow. Our\\r\\nimmersive human-centered design practices result in\\r\\na thorough understanding of the end-to-end process,\\r\\nand awareness of bottlenecks within the system\\r\\nwhere AI can accelerate or improve the process.\\r\\nWe are not creating general AI utilities, tools\\r\\nand frameworks for others to leverage in building\\r\\ntheir solution. Rather, we are applying proven AI\\r\\ntechnology in a focused and collaboratively developed\\r\\nway. We are placing it in the hands of a known\\r\\npopulation (e.g., first responders, or dispatchers). We\\r\\nare augmenting their decision-making, not displacing\\r\\nthem. Finally, we are applying the technology to a\\r\\nnarrowly defined and well-understood use case.\\r\\nCritically, the narrow scope of the application allows\\r\\nus to choose the simplest, most specific, and most\\r\\nmature underlying AI technologies. We can manage\\r\\nand curate training data for the specific applications\\r\\nand environments implied by the solution, and to\\r\\nconstrain the scenarios and use cases across which\\r\\nthe solutions must be validated.\\r\\nMotorola cannot ensure or enforce our customer’s\\r\\ncompliance with legal and ethical standards in the\\r\\napplication of AI, but our products provide access\\r\\ncontrols based upon user authentication credentials\\r\\nand access permissions to allow our customers to\\r\\nenforce user compliance to operational policies.\\r\\nOur products will maintain an audit trail of user\\r\\noperations and where possible, constrain the use\\r\\nof AI powered capabilities within the context of a\\r\\nworkflow. For example, submitting an image of\\r\\na face (probe) for search against a gallery may\\r\\nonly be accomplished in the context of an active\\r\\ninvestigation by authorized individuals. When a\\r\\n‘match’ is surfaced, this must be verified and signed\\r\\noff by one (and sometimes two, depending upon local\\r\\npolicy) human experts. Audit trails will not only keep\\r\\ntrack of the specific operations and user identity but\\r\\nwill also retain any data used in the operation (e.g.,\\r\\nintroduction of a probe-image for search).\\r\\nFUNDAMENTALS OF\\r\\nRESPONSIBLE AI\\r\\nUnderlying our foundational\\r\\ntenets designed for the unique\\r\\nneeds of public safety, there are\\r\\ngenerally acknowledged universal\\r\\nconsiderations when applying AI that\\r\\nMotorola Solutions addresses:\\r\\n• Bias and fairness\\r\\n• Understandability and transparency\\r\\n• Privacy\\r\\n• Reliability and security\\r\\nBIAS AND FAIRNESS\\r\\nBias, in general, is a prejudice for or against\\r\\none thing, person, or group compared with\\r\\nanother, usually in a way considered to be unfair.\\r\\nFor humans, bias can result from a number\\r\\nof factors, including a person’s environment,\\r\\ntraining, condition, or experiences. In AI, bias\\r\\noccurs when the output of the AI process results\\r\\nin inconsistent treatment across a group. For\\r\\nexample, being able to identify faces more\\r\\naccurately for one demographic (characterized\\r\\nby race, gender, age, and physiology) than\\r\\nanother, or making decisions about how a person\\r\\nshould be treated (such as a hiring decision)\\r\\ninconsistently across demographics.\\r\\nMuch like humans learn iteratively through\\r\\nexperience, AI algorithms learn by iteratively\\r\\n‘training’ with representative data, developing\\r\\nthe ability to identify discriminating\\r\\ncharacteristics across this data set. For\\r\\nexample, in a facial recognition application, the\\r\\nAI system is exposed to a series of pictures of\\r\\ndifferent and identical faces that are tagged\\r\\nso the AI can know whether given images\\r\\nare of the same individual or not. In this way,\\r\\nthe AI effectively learns what characteristics\\r\\ndistinguish different people. (This is very\\r\\nsimilar to how humans develop their ability to\\r\\nrecognize and differentiate patterns in the senses\\r\\nof sight, sound, smell, taste, and touch.) Voice\\r\\ninteraction systems are essentially trained in\\r\\nthe same manner using tagged verbal data. The\\r\\ncompleteness and accuracy of the AI system’s\\r\\n‘learning’ is a function of the quality, diversity,\\r\\ncorrectness, and volume of the training dataset.\\r\\nIn general, data needs to be completely\\r\\nrepresentative of the context that the AI system\\r\\nwill operate within (population, vocabulary,\\r\\netc.) and there must be sufficient data available\\r\\nfor the AI to develop the ability to fully discern\\r\\nuniquely identifying characteristics.\\r\\nBias can be present within training data in two\\r\\nways: either the data is unrepresentative of\\r\\nthe population that the AI will operate within,\\r\\nor it reflects past or existing prejudices. For\\r\\ninstance, the first case might occur if an AI\\r\\nsystem trains on a set of voice samples that\\r\\nhas more females than males. The resulting\\r\\nnatural language interaction system would\\r\\nbe worse at recognizing male speech. An\\r\\nexample of the second case might be if a\\r\\nrecruiting tool dismisses a large proportion\\r\\nof female candidates because it was trained\\r\\non historical hiring decisions, which were\\r\\ndominated by males over females.\\r\\nAI is fundamentally amoral as it is not\\r\\ninfluenced by human discriminatory tendencies,\\r\\nemotions, distractions or fatigue. Thus, AI\\r\\nhas the ability to be much more deterministic,\\r\\nand less biased than human counterparts.\\r\\nFurthermore, as the AI system is used and\\r\\nexposed to more data in its population set, its\\r\\naccuracy will continually improve.\\r\\nHow Motorola Solutions Supports\\r\\nFairness in our AI\\r\\nMotorola Solutions thoroughly evaluates the\\r\\ndata employed in training our AI algorithms\\r\\nto ensure that sufficient quantity, quality and\\r\\ndiversity exists across the dataset to properly\\r\\ntrain the algorithm for its intended purpose and\\r\\noperating environment.\\r\\nWe thoroughly validate the operation of the\\r\\ntrained algorithms with a representative and\\r\\ndiverse set of test cases that are applied\\r\\nacross a range of operational conditions.\\r\\nWe also test and retest our products in actual\\r\\ncustomer environments.\\r\\nIn fielded operation, our systems generate\\r\\ntelemetry that we can continuously monitor\\r\\nto identify performance issues, as well as\\r\\nany inconsistent or undesirable behaviors.\\r\\nFor example, as we discover optimizations\\r\\nand refinements that improve accuracy we\\r\\ncan retrain the model with an enhanced data\\r\\nset (e.g., a new set of slang terminology or\\r\\nspeech accent that was not initially known/\\r\\nanticipated) and deploy that universally across\\r\\nour entire user base.\\r\\nWHITE PAPER | AI AT MOTOROLA SOLUTIONS\\r\\nUNDERSTANDABILITY AND\\r\\nTRANSPARENCY\\r\\nUnderstandability simply means that the\\r\\nbehavior and outputs of an AI system must\\r\\nbe readily explainable by those who provide\\r\\nit. This ‘why’ component is an essential\\r\\ncharacteristic of a system in order for users\\r\\nto interpret and trust the outputs produced,\\r\\nand for society to trust the tools that first\\r\\nresponders are applying on their behalf.\\r\\nIn traditional systems, explaining behavior\\r\\nis generally a straightforward exercise of\\r\\nreading the software programs to understand\\r\\nwhat the programmer instructed the system to\\r\\ndo. Trained AI systems do not fundamentally\\r\\nhave programmed or codified behaviors. Their\\r\\noperations are a function of the training\\r\\ndataset used. Variations in output may come\\r\\nfrom the system’s training or may be the\\r\\nmanifestation of traditional software bugs in\\r\\nthe underlying implementation of the model.\\r\\nHow Motorola Solutions Supports\\r\\nUnderstandability in our AI\\r\\n• Motorola Solutions maximizes our\\r\\nability to explain the operation of our\\r\\nsystems by adopting mature, testable\\r\\nAI components that are as simple\\r\\nas possible for the task at hand.\\r\\nFor example, by knowing the verbal\\r\\ncommands needed for an interaction\\r\\nthat the system will support, we can\\r\\ngreatly constrain the verbal phrases\\r\\nand words (intents) that need to be\\r\\nunderstood.\\r\\n• By leveraging mature, well characterized\\r\\nand understood implementations\\r\\nand model frameworks with proven\\r\\nreliability, we increase the likelihood\\r\\nthat the simplest explanation is most\\r\\nlikely to be correct. Simple systems\\r\\nare much easier to test, understand\\r\\nand explain than complex ones, and\\r\\nalso boost the overall reliability of the\\r\\nsystem. Often, these components will\\r\\ncome from services in widely deployed\\r\\ncloud service platforms operating at\\r\\nscale.\\r\\n• We ensure that our systems generate\\r\\noperational performance data that we\\r\\ncan monitor and review on a regular\\r\\nbasis to assess efficacy of operation.\\r\\nPRIVACY\\r\\nIn the context of AI applications, the issue of\\r\\nprivacy primarily involves securing and managing\\r\\nthe data associated with the system, including\\r\\ntraining data (input to the system) and any\\r\\noutputs that the system produces in operation.\\r\\nTraining data may contain Personally Identifiable\\r\\nInformation (PII), which has to be managed\\r\\nin accordance with appropriate information\\r\\nsecurity policies. As established in the bias\\r\\nand fairness discussion, it is essential to work\\r\\nwith representative data when training AI.\\r\\nConsequently, Motorola Solutions will almost\\r\\ncertainly be leveraging samples of live data\\r\\n(containing PII) in some instances.\\r\\nHow Motorola Solutions Supports\\r\\nPrivacy in our AI\\r\\n• Where possible, Motorola Solutions\\r\\nwill work with anonymized data (no\\r\\nassignable PII content) at the source,\\r\\nsynthesized data (by machine methods or\\r\\nthrough controlled customer interactions\\r\\nsuch as training exercises), or accumulate\\r\\nour training data from publicly available\\r\\nsources.\\r\\n• We utilize tools and frameworks that\\r\\nfacilitate privacy-sensitive training by\\r\\nencoding general patterns rather than\\r\\nfacts about specific training examples\\r\\nwherever possible. Techniques such\\r\\nas differential privacy and federated\\r\\nlearning offer strong mathematical\\r\\nguarantees that models do not learn or\\r\\nremember the details about any specific\\r\\nuser. Furthermore, no training data is\\r\\ndistributed with our products.\\r\\n• In the USA, Motorola Solutions employs\\r\\nCriminal Justice Information System\\r\\n(CJIS) compliant data facilities, approved\\r\\npersonnel, and stringent practices to\\r\\nmanage the data that we house for\\r\\ntraining and testing algorithm purposes.\\r\\nOutside of the USA, where it is necessary\\r\\nto apply localized data for this purpose,\\r\\nwe will adhere to all country specific\\r\\nregulations and practices that might\\r\\nentail establishment of local presence\\r\\nand a secure data repository that respects\\r\\nconsiderations such as data sovereignty.\\r\\n• When securing usage or deploymentgenerated operational outputs of AI\\r\\nenabled solutions, we rely upon the same\\r\\nWHITE PAPER | AI AT MOTOROLA SOLUTIONS\\r\\nstringent cybersecurity practices that we\\r\\napply to all of our products5\\r\\n. We build\\r\\nin all of the necessary security controls,\\r\\nauditing, and practices necessary to\\r\\nenable our users to secure and manage\\r\\nsensitive data (e.g., Law Enforcement\\r\\nRecords) and this same fabric applies to\\r\\noutputs generated by AI. Where possible\\r\\nwe leverage industry best practices and\\r\\nscale (e.g., government cloud based\\r\\nplatforms and environments).\\r\\nRELIABILITY AND SECURITY\\r\\nAs with any item that public safety users\\r\\nleverage for day-to-day operation, AI based\\r\\nsolutions must be as reliable as possible.\\r\\nMotorola Solutions has a history of delivering\\r\\nmission critical solutions and a culture of\\r\\nensuring quality, performance, and integrity in\\r\\nour products. This is accomplished by following\\r\\ndisciplined development, validation, deployment\\r\\nand life-cycle management processes and\\r\\nemploying comprehensive product testing\\r\\npractices.\\r\\nWhen introducing changes to solutions\\r\\nin the public safety space, we employ an\\r\\nincremental discipline. Starting with a thorough\\r\\nunderstanding of users and their environment,\\r\\nwe progress through a collaborative creation\\r\\nprocess where we iteratively develop, test and\\r\\nrefine capabilities incrementally with continual\\r\\ncustomer feedback.\\r\\nHow Motorola Solutions Supports\\r\\nReliability and Security in our AI\\r\\n• We ensure that new capabilities don’t\\r\\neliminate, impair or alter existing\\r\\nfunctions so that in the moment of need\\r\\nnothing previously relied upon or learned\\r\\nas ‘muscle memory’ is compromised.\\r\\n• We introduce new capabilities in\\r\\ncontrolled and tightly monitored\\r\\ncircumstances (e.g., training facilities\\r\\nand environments for models, select\\r\\ntest users, etc.) and employ focus group\\r\\nfeedback. In selecting these target\\r\\nenvironments, we consider the nature of\\r\\nthe AI capability being applied and ensure\\r\\nthat we choose diverse environments to\\r\\nmaximize the testing surface variation.\\r\\n• We employ standard incremental rollout\\r\\nmethods across our user base such as\\r\\nBeta testing.\\r\\n• We leverage continuous integration / continuous deployment development\\r\\npractices that allows us to deploy changes, fixes, and new capabilities quickly and\\r\\nincrementally. This is especially important in the context of retraining AI functionality.\\r\\n• We build in system monitoring and telemetry that we can threshold or monitor\\r\\nregularly to ensure that system is operating as intended.\\r\\n• Our products build in compliance mechanisms to apply role-based access controls\\r\\nand create audits, allowing our customers to enforce compliance and accountability\\r\\nacross their users.\\r\\nSUMMARY\\r\\nMotorola Solutions is responsibly and incrementally employing AI to assist and\\r\\naugment our users to help them be more efficient, effective and safe. We are\\r\\ndoing this by leveraging proven mission critical research and design principles\\r\\nguided by the fundamental tenets of human in the loop for consequential\\r\\ndecisions and focused solutions that leverage mature AI.\\r\\nWe are developing AI aimed at customer outcomes that are familiar and\\r\\nconsistent with outcomes we’ve previously enabled in a more manual fashion\\r\\nwith other technologies. AI simply enables these outcomes in a way that is more\\r\\nefficient, more effective, and safer. We can easily measure the effectiveness and\\r\\naccuracy of AI-driven solutions relative to more traditional methods of achieving\\r\\nthe same outcomes. In this way, our AI solutions are anchored in and measured\\r\\nagainst widely accepted, culturally and ethically appropriate methods that\\r\\nsupport fairness, understandability, privacy, and security.\\r\\nREFERENCES\\r\\n1. Face recognition accuracy of forensic examiners, superrecognizers, and face recognition algorithms; Philips, Yates, Hu, Hahn, Noyes, Jackson, Cavazos, Jeckeln, Ranjan, Sankaranarayanan, Chen,\\r\\nCastillo, Chellappa, White, O’Toole; Proceedings of the National Academy of Sciences; June 2018\\r\\n2. The Future Computed – Artificial Intelligence and its Role in Society; Microsoft Corporation; 2018\\r\\n3. Responsible AI Practices; Google\\r\\n4. Everyday Ethics for Artificial Intelligence; IBM Corporation; September 2018\\r\\n5. Motorola Solutions Products, Solutions, and Services Foundational Cybersecurity Policies; Motorola Solutions; Version 1.0; October 2017\\r\\nTo learn more about empowering public safety with AI, visit:\\r\\nmotorolasolutions.com/viqi\\r\\nTHE UNDERSTANDABILITY POTENTIAL OF\\r\\nMACHINE LEARNING\\r\\nWhere possible and applicable, we apply\\r\\nsolutions that provide more interpretability for\\r\\nour customers. There are exciting advances\\r\\nbeing made rapidly in the arena of AI\\r\\ninterpretability via machine learning.\\r\\nFundamentally, traditional software uses human\\r\\nreadable rules to map inputs to outputs. It is\\r\\npossible to use machine learning to learn these\\r\\nrules directly or to decipher the implicit logic\\r\\nused by the trained AI model to map inputs to\\r\\noutputs. We ensure that our machine learning\\r\\nmodels are deterministic and where possible\\r\\ninterpretable as rules operating on the input.\\r\\nWhen machine learning is applied, we will\\r\\ndeploy trained and tested implementations\\r\\nrather than field-based machine learning\\r\\nwherever possible. When we do apply machine\\r\\nlearning it will be for low-level cases such as\\r\\nallowing the solution to adapt to its environment\\r\\n(e.g., using machine learning on a video camera\\r\\nto estimate scene geometry and constant\\r\\nconditions). This allows us to curate the training\\r\\ndata set to the specific objective at hand and\\r\\nvalidate the operation of the system through\\r\\nin-house testing prior to deployment.\\r\\nMotorola Solutions, Inc. 500 West Monroe Street, Chicago, Il 60661 U.S.A. motorolasolutions.com\\r\\nMOTOROLA, MOTO, MOTOROLA SOLUTIONS and the Stylized M Logo are trademarks or registered trademarks of Motorola Trademark Holdings, LLC and\\r\\nare used under license. All other trademarks are the property of their respective owners. © 2019Motorola Solutions, Inc. All rights reserved. 10-2019',\n",
              " \"NEC Group AI and Human Rights Principles\\r\\nEnacted: April, 2019\\r\\nThe NEC Group (hereinafter referred to as “NEC”), as a “Social Value Creator,” is pursuing\\r\\n“for all people, an abundant society” by aiming to make lasting contributions toward the\\r\\nresolution of social issues leveraging new technologies that promote the safety, security,\\r\\nefficiency, and equality of society. However, we also understand that while social\\r\\nimplementation of Artificial Intelligence (AI) and utilization of biometrics and other data\\r\\n(hereinafter referred to as “AI utilization”) enriches our lives, it may also lead to human\\r\\nrights issues such as the invasion of privacy and/or discrimination depending on how it is\\r\\nutilized.\\r\\nNEC is enacting these principles in order to prevent and address human rights issues arising\\r\\nfrom AI utilization. In addition to facilitating compliance with relevant laws and regulations\\r\\naround the globe, these principles will guide our employees to recognize respect for human\\r\\nrights as the highest priority in each and every stage of our business operations in relation\\r\\nto AI utilization and enable them to take action accordingly.\\r\\n(1) Fairness\\r\\nNEC will ensure that individuals are not unfairly discriminated in AI utilization keeping\\r\\nalways aware that results of decision-making by AI are possible to be biased.\\r\\n(2) Privacy\\r\\nNEC will respect and protect individual's privacy in AI utilization.\\r\\n(3) Transparency\\r\\nNEC will strive to create a framework that enables appropriate explanations regarding the\\r\\nresults of decision-making in our AI utilization.\\r\\n(4) Responsibility to Explain\\r\\nNEC will provide all stakeholders with a comprehensive explanation of the effects, value,\\r\\nand impacts of AI utilization to gain their understandings.\\r\\n(5) Proper Utilization\\r\\nNEC will ensure proper AI utilization in a way that respects human rights.\\r\\nFurthermore, NEC will ensure that our AI products and services will be utilized by our\\r\\ncustomers and partners in accordance with respect for human rights.\\r\\n(6) AI and Talent Development\\r\\nNEC will develop useful and cutting-edge technologies and talent in an effort to promote AI\\r\\nutilization.\\r\\n(7) Dialogue with Multiple Stakeholders\\r\\nNEC will incorporate the viewpoints and opinions of third parties in addition to internal\\r\\nstakeholders to prevent NEC’s AI products and services from violating human rights. To this\\r\\nend, NEC will build partnerships and collaborate closely with external experts and various\\r\\nother relevant stakeholders.\\r\\nNEC will not ignore new social issues generated by future AI utilization, but rather face them\\r\\nhead on by leveraging our technologies to realize advance societies worldwide toward\\r\\ndeepened mutual understanding and the fulfillment of human potential.\",\n",
              " 'Six pillars of Responsible AI\\r\\nFairness\\r\\nAI systems must be designed in ways that maximize fairness, non-discrimination and accessibility. All AI designs should promote inclusivity by correcting both unwanted data biases and unwanted algorithmic biases.\\r\\n\\r\\nReliability, Safety and Security\\r\\nAI systems should cause no direct harm and always aim to minimize indirect harmful behavior. They should always perform as intended and remain resilient against threats and outside tampering.\\r\\n\\r\\nPrivacy\\r\\nAI systems must respect privacy by providing individuals with agency over their data and the decisions made with it. AI systems must also respect the integrity of the data they use.\\r\\n\\r\\nTransparency\\r\\nAI systems must be explainable and understandable. They should allow for human agency and oversight by producing outputs that are comprehensible to the average person. AI decisions should be auditable and traceable, which will ultimately instill the trust AI needs for broad acceptance.\\r\\n\\r\\nSustainability\\r\\nAI systems should attempt to be both societally sustainable, by empowering society and democracy, and environmentally sustainable, by reducing the amount of power required to train and run these systems.\\r\\n\\r\\nAccountability\\r\\nAI systems should be developed and deployed through consultation and collaboration in order to create true accountability. The long-term effects of an AI application must be understandable by all stakeholders, and those stakeholders must be empowered to act if any proposed change produces adverse effects.',\n",
              " 'It’s hard to deny that artificial intelligence has come so far so fast. It’s working its way into our lives in ways that seem so natural that we find ourselves taking it for granted almost immediately. It’s helping us get around town, keeping us fit and always inventing new ways to help around the house. The ever-increasing availability of affordable sensors and computing elements will only accelerate this trend. Thinking of what’s next, which was once the domain of science fiction writers, is now our day-to-day reality. In fact, Deloitte has estimated that more than 750 million AI chips will be sold in 2020 and by 2024 they expect this will exceed 1.5 billion.\\r\\nAs with any useful advancement, AI will be a little bit of a mixed bag. Most AI applications will be designed for the greater good, but there will always be outlying cases. The increase in autonomous applications that carry with them the potential to put humans in danger drives home the need for a universal code of conduct for AI development. A few years ago this would have sounded preposterous, but things are changing fast.\\r\\nThe industry, together with governments of several of the world’s leading nations, are already developing policies that would govern AI, even going so far as discussing a “code of conduct” for AI that focuses on safety and privacy. But how does one make AI ethical? First, you have to define what is ethical and the definition isn’t as cut and dry as we may hope. Without even considering the vast cultural and societal differences that could impact any such code, in practical terms, AI devices require complicated frameworks in order to carry out their decision-making processes.\\r\\nThe integrity of an AI system is just as important as the ethical programming because once a set of underlying principles is decided on, we need to be sure they’re not compromised. Machine learning can be utilized to monitor data streams to detect anomalies, but it can also be used by hackers to further enhance the effectiveness of their cyberattacks. AI systems also have to process input data without compromising privacy. Encrypting all communications will maintain confidentiality of data, and Edge AI systems are starting to use some of the most advanced cryptography techniques available.\\r\\nBright minds. Bright futures. NXP team members create breakthrough technologies that advance our world. The future starts here.\\r\\nPerhaps the biggest challenge is that the AI ecosystem is made up of contributions from various creators. Accountability and levels of trust between these contributors are not uniformly shared, and any breach could have far reaching implications if systematic vulnerabilities are exploited. Therefore, it’s the responsibility of the entire industry to work towards interoperable and assessable security.\\r\\nAgreement on a universal code of ethics will be difficult and some basic provisions need to be resolved around safety and security. In the meantime, certification of silicon, connectivity and transactions should be a focus for stakeholders as we collaborate to build trustworthy AI systems of the future.\\r\\nAt NXP, we believe that upholding ethical AI principles, including non-maleficence, human autonomy, explicability, continued attention and vigilance, privacy and security by design is important. It is our responsibility to encourage our customers, partners and stakeholders to support us in this endeavor.\\r\\nYou can read more it in our whitepaper, The Morals of Algorithms – A contribution to the ethics of AI systems.\\r\\n',\n",
              " 'OKI Group AI Principles \\r\\nIn recent years, the development of AI technologies has been remarkable, and they have solved problems that were once thought insoluble and have huge potential as one of the means to make people’s lives richer and happier. The OKI Group aims to realize harmonious coexistence of human beings and AI by providing socially accepted AI products, etc. (i.e., products and services utilizing AI as well as provided AI technologies) in order to contribute to the improvement of the quality of life for people around the world. On the other hand, because of its nature, currently there are challenges and limitations for AI, and this may cause legal and ethical issues or safety related issues due to inappropriate development and/or utilization, etc. The OKI Group has established the OKI Group Charter of Corporate Conduct and the OKI Group Code of Conduct, and has been striving to fulfill our social responsibility based on our corporate philosophy. Based on the spirit of the OKI Group Charter of Corporate Conduct and the OKI Group Code of Conduct, in order to prevent or cope with the above issues when conducting research and development of AI technologies, the sale and provision of AI products, etc., and any other corporate activities related to AI (collectively, the “AI Business”), we established the OKI Group AI Principles. The OKI Group will, in addition to complying with laws and regulations, continue to develop its AI Business in compliance with these OKI Group AI Principles. (1) Respect for Human Rights AI should be developed and utilized for the realization of a human-centric society. The OKI Group respects fundamental human rights in the promotion of the AI Business, and as part of that, we will strive to prevent any unfair discrimination. Further, we will consider privacy, and will comply with applicable rules including laws and regulations concerning the handling of personal information. (2) Explanation and Transparency In the AI Business, the OKI Group will endeavor to gain the understanding of our customers and other stakeholders by explaining the envisaged purposes and methods of use of AI products, etc., as well as the effects and impacts of the utilization of AI products, etc., and their limitations, depending on the nature and utilization situation of the subject AI products, etc. In particular, we will consider the transparency of how AI reaches conclusions with respect to individual AI products, etc. and endeavor to provide information that contributes to an understanding of how AI reaches conclusions. Also, we will continue to examine better ways of providing information from various aspects based on the accumulation of individual examples and the progress of future discussions. (3) Dialogue and Collaboration In order to deepen our customers’ and other stakeholders’ understanding of AI products, etc., the OKI Group will have a necessary dialogue with them, and strive to establish relationships with them whereby they will become more convinced in using AI products, etc. Further, we recognize that there are challenges concerning collaboration between human beings and AI and AI-to-AI, and will continue to consider these challenges so as to realize a better human-centric society. (4) Safety and Handling of Data The OKI Group will strive to ensure the safety of provided AI products, etc. for our customers and other stakeholders. Further, with regard to data, while protecting personal information and respecting privacy, we will ensure the proper acquisition, use, management and security of data and will strive not to cause any undue damage to our customers and other stakeholders. (5) Development of Human Resources In order to contribute to the improvement of the quality of life for people around the world, the OKI Group thinks that, in the performance of the AI Business, it is essential to have human resources who have a proper understanding of AI technologies and the nature, issues and limitations of AI, and who may appropriately provide AI products, etc. to society. Therefore, we will actively develop diverse human resources that are needed in the era of AI utilization. The OKI Group will, in promoting the AI Business in accordance with the OKI Group AI Principles, develop the capability to share and resolve issues through the AI Business in collaboration between related group companies and departments. In addition to this, if we deem it necessary taking into account domestic and international trends and social demands, etc., we will amend these OKI Group AI Principles',\n",
              " 'Trusted AI\\r\\nWe deliver tools to our employees, customers and partners for developing and using AI responsibly, accurately and ethically.\\r\\n\\r\\nOur Commitment\\r\\nWe believe the benefits of AI should be accessible to everyone. But it is not enough to deliver only the technological capabilities of AI – we also have an important responsibility to ensure that AI is safe and inclusive for all. We take that responsibility seriously and are committed to providing our employees, customers, and partners with the tools they need to develop and use AI safely, accurately, and ethically.\\r\\n\\r\\n\\r\\nResponsible\\r\\nTo safeguard human rights and protect the data we are entrusted with, we work with human rights experts, and educate, empower and share our research with customers and partners.\\r\\n\\r\\n\\r\\nInclusive\\r\\nAI should respect the values of all those impacted, not just those of its creators. To achieve this, we test models with diverse data sets, seek to understand their impact, and build inclusive teams.\\r\\n\\r\\n\\r\\nAccountable\\r\\nTo create AI accountability we seek stakeholders feedback, take guidance from the Ethical Use Advisory Council, and conduct our own data science review board.\\r\\n\\r\\n\\r\\nTransparent\\r\\nWe strive for model explainability and clear usage terms, and ensure customers control their own data & models.\\r\\n\\r\\n\\r\\nEmpowering\\r\\nAccessible AI promotes growth and increased employment, and benefits society as a whole.',\n",
              " \"AI Ethics Principles\\r\\nAI is a rapidly developing area and its global social, economic impact is also growing.\\r\\nWhile recognizing that technologies promote innovation, the same technologies raise important challenges to be addressed. Samsung SDS acknowledges the use of AI technologies should aspire to human dignity and human rights as well as to a sustainable environmental ecosystem. Therefore, Samsung SDS will establish corporate AI ethics principles based on UNESCO's recommendations for the ethics of artificial intelligence. We believe all use of AI technologies must respect the rule of law, human rights, and values of equity, privacy, and fairness. These principles set out our commitment to develop, deploy, and use technology responsibly.\\r\\n\\r\\n1. Respect for Human Rights\\r\\nBased on the Samsung Spirit, our top priority is to benefit humanity and society. We respect and comply with international human rights laws and values in general and also in relation to AI technologies. Further, we will work to limit any potentially harmful or abusive application that can negatively affect human beings and their rights as we develop and deploy AI technologies.\\r\\n\\r\\n2. Diversity and Inclusion\\r\\nWe believe that everyone should be treated fairly and equitably. We understand that defining fairness is not always simple and differs across cultures and societies. We will seek to avoid biased results and unjust impact on sensitive characteristics such as race, ethnicity, gender, nationality, income, sexual orientation, ability, and political or religious belief. We also will seek to avoid exposing children to inappropriate content.\\r\\n\\r\\n3. Data and Privacy Protection\\r\\nWe recognize the importance of protecting the privacy and security of people’s data. To minimize privacy risks, we will continue to monitor data processing processes and develop safe and secure practices.\\r\\n\\r\\n4. Conservation of Environmental Ecosystem\\r\\nWe will comply with relevant national and international regulations, standards and practices to assure that AI development and services do not adversely affect the sustainability of the environment and ecosystem.\\r\\n\\r\\n5.Communication\\r\\nWe believe in transparency and explainability. AI will be explainable for users to understand its decisions or recommendations to the extent technologically feasible and that this does not jeopardize corporate competitiveness. Samsung SDS will also devise countermeasures against the risks and negative consequences that AI technology can cause to the users.\",\n",
              " \"Global Artificial Intelligence (AI) Ethics Policy\\r\\nVersion Number: 1.0\\r\\nEffective Date: January 1, 2022\\r\\nVersion 1.0, January 1, 2022\\r\\nThe online version of this document is the officially released version.\\r\\n Any copies or print-outs are not controlled\\r\\nVersion 1.0, January 1, 2022\\r\\nThe online version of this document is the officially released version.\\r\\n Any copies or print-outs are not controlled\\r\\n1 INTRODUCTION\\r\\nSAP is committed to the ethical development, deployment, use, and sale of SAP developed Artificial\\r\\nIntelligence (AI) systems. The policy defines a group-wide minimum standard for the development,\\r\\ndeployment, use, or sale of SAP’s Artificial Intelligence systems. It defines requirements for SAP’s business\\r\\nprocesses that involve AI and assigns clear responsibilities.\\r\\nThe policy is based on, and has been built upon the foundation of the SAP Guiding Principles for Artificial\\r\\nIntelligence, established in 2018, as laid out below:\\r\\n• We are driven by our values.\\r\\n• We design for people.\\r\\n• We enable business beyond bias.\\r\\n• We strive for transparency and integrity in all that we do.\\r\\n• We uphold quality and safety standards.\\r\\n• We place data protection and privacy at our core.\\r\\n• We engage with the wider societal challenges of Artificial Intelligence.\\r\\n2 PURPOSE AND OBJECTIVES\\r\\nSAP believes that Artificial Intelligence has the potential to unlock abundant potential for businesses,\\r\\ngovernments, and society. But, like all great technological advancements, AI also has the potential to create\\r\\neconomic, political, and social challenges, depending upon how it is used and implemented. In addition, the\\r\\nspeed at which the technology has moved into common usage has outpaced guidance from governmental\\r\\npolicymakers on acceptable use. For these reasons, the development, deployment, use, and sale of AI\\r\\nsystems at SAP needs to be governed by clear rules of ethics that are aligned with SAP’s established\\r\\nguiding principles for AI and its core organizational values, supplemental to any existing or pending\\r\\nlegislation.\\r\\nFoundational to SAP’s approach to AI Ethics is the company’s commitment (through its Global Human Rights\\r\\nCommitment Statement) to uphold and support the Universal Declaration of Human Rights, and to respect,\\r\\npromote, and support internationally recognized human rights and widely accepted international norms.\\r\\nAn essential part of this commitment is the prohibition of discrimination and harassment of humans based on\\r\\npersonal factors (including for example culture, race, ethnicity, religion, age, gender, sexual orientation,\\r\\ngender identity, physical or mental disability etc.), in order to promote respect for human autonomy and to\\r\\nensure that the moral worth and dignity of all human beings is respected. In addition, SAP’s goals for AI\\r\\nsystems include protecting people from harm, looking after the well-being of others, treating all individuals\\r\\nequitably and justly, ensuring the entitlement to equal freedom and dignity under the law, the protection of\\r\\ncivil, political and social rights, the universal recognition of personhood, and the right to free and\\r\\nunencumbered participation in the life of the community.\\r\\n3 SCOPE\\r\\nThis Policy applies to SAP and its employees worldwide involved in the development, deployment, and sale\\r\\nof SAP developed AI systems. If any SAP subsidiary has its own AI Ethics Policy, upon enactment of the\\r\\nGlobal AI Ethics Policy, it immediately must align with this Policy.\\r\\nThe Policy defines SAP management’s intent and expectations for the ethical development, deployment, and\\r\\nsale by SAP employees of SAP-developed AI systems and the principles may be used to provide guidance\\r\\nand counsel to customers and partners.\\r\\nHowever, in ensuring the rights of customers to own and manage their own data SAP may have limited\\r\\nknowledge of how they ultimately use SAP AI products and services. SAP shall endeavor to educate and\\r\\nadvise customers and partners based on the principles in this policy, recognizing that specific use case\\r\\nresponsibility lies with the customer or partner, acting in good faith in accordance with local laws and\\r\\nregulations. SAP reserves the right to take appropriate action in the event of a customer violation of the\\r\\nprinciples of this policy.\\r\\nOther policies that were in place before this Policy was enacted may remain in force but must be adapted to\\r\\ncomply with the rules of this Policy upon the next review or modification cycle.\\r\\n4 TERMS AND DEFINITIONS\\r\\nSAP Group SAP SE and its subsidiaries.\\r\\nSAP Subsidiary Any legal entity where SAP SE holds more than fifty percent (50%) of\\r\\nthe shares or voting rights or such entity is controlled by the SAP SE.\\r\\nVersion 1.0, January 1, 2022\\r\\nThe online version of this document is the officially released version.\\r\\n Any copies or print-outs are not controlled\\r\\nAI Ethics A set of values, principles and techniques that employ widely\\r\\naccepted standards of right and wrong to guide moral conduct in the\\r\\ndevelopment, deployment, use and sale of AI technologies.\\r\\nArtificial Intelligence\\r\\n(AI)\\r\\nTypically defined as the ability of a machine to perform cognitive\\r\\nfunctions we associate with human minds, such as perceiving,\\r\\nreasoning, learning and problem solving.\\r\\nA system's ability to correctly interpret external data, to learn from\\r\\nsuch data and to use those learnings to achieve specific goals and\\r\\ntasks through flexible adaptation.\\r\\nSAP differentiates between two types of AI systems:\\r\\nRule-based AI systems are characterized by the fact that the\\r\\nbehavior of their components is fully defined by rules created by\\r\\nhuman experts. These systems are often described as symbolic or\\r\\nexpert systems.\\r\\nLearning-based AI systems are differentiating themselves by the\\r\\nfact that their initial configuration made by humans is only the basis\\r\\nfor the final form of their functions. With the help of data, they train\\r\\nhow to solve a problem and continuously adapt their function in this\\r\\nprocess. For learning-based AI systems, humans define the problem\\r\\nand the goal, but the behavior rules and relationships required for the\\r\\nsystem are learnt in an automized way.\\r\\nIn addition, hybrid systems containing both learning-based and rulebased methods are available.\\r\\nImpact Assessment shall be performed on such Learning-based AI\\r\\nsystems during its development phase and subsequent phases to\\r\\nensure that there are no unintended consequences.\\r\\nAI Systems Any AI based algorithm, component and/or software embedded in\\r\\nSAP products and services (e.g. AI platform, databases, embedded or\\r\\nstand-alone applications, features and solutions, Conversational AI\\r\\n(‘Chatbots’)).\\r\\nAgency The capacity of individuals to act independently and to make their\\r\\nown free choices.\\r\\nApplication Any program, or group of programs, that is designed for the end user.\\r\\nAutonomy The capacity to make an informed, uncoerced decision.\\r\\nBias An inclination or prejudice for or against one person or group,\\r\\nespecially in a way considered to be unfair.\\r\\nBlack Box Algorithms Any artificial intelligence system whose inputs and operations are not\\r\\nvisible to the user or another interested party. A black box, in a\\r\\ngeneral sense, is an impenetrable system.\\r\\nDeep learning modeling is typically conducted through black box\\r\\ndevelopment: The algorithm takes millions of data points as inputs\\r\\nand correlates specific data features to produce an output. That\\r\\nprocess is largely self-directed and is generally difficult for data\\r\\nscientists, programmers and users to interpret.\\r\\nData Subject Any individual person who can be identified, directly or indirectly, via\\r\\nan identifier such as a name, an ID number, location data, or via\\r\\nfactors specific to the person's physical, physiological, genetic,\\r\\nmental, economic, cultural or social identity.\\r\\nDeep Neural Network A technology developed to simulate the activity of the human brain –\\r\\nspecifically, pattern recognition and the passage of input through\\r\\nvarious layers of simulated neural connections.\\r\\nDeveloped (as relates to\\r\\nAI systems)\\r\\nIncludes Design, SAP Standard Development, Prototyping, Coinnovation, and Innovation and Cloud Service team’s development\\r\\nactivities.\\r\\nVersion 1.0, January 1, 2022\\r\\nThe online version of this document is the officially released version.\\r\\n Any copies or print-outs are not controlled\\r\\nExplainability The ability to explain both the technical processes of an AI system\\r\\nand the related human decisions (e.g. application areas of a system).\\r\\nTechnical explainability requires that the decisions made by an AI\\r\\nsystem can be understood and traced by human beings.\\r\\nFairness Impartial and just treatment or behavior without unjust favoritism or\\r\\ndiscrimination.\\r\\nFeature A distinguishing characteristic of a software item (e.g. performance\\r\\nor functionality).\\r\\nHuman-in-the-loop AI System functionality for human beings to intervene in every\\r\\ndecision cycle of the system\\r\\nHuman-on-the-loop Refers to the capability for human intervention during the design cycle\\r\\nof the system and monitoring the system’s operation.\\r\\nHuman-in-command Refers to the capability to oversee the overall activity of the AI system\\r\\n(including its broader economic, societal, legal and ethical impact)\\r\\nand the ability to decide when and how to use the system in any\\r\\nparticular situation. This can include the decision not to use an AI\\r\\nsystem in a particular situation, to establish levels of human discretion\\r\\nduring the use of the system or to ensure the ability to override a\\r\\ndecision made by a system.\\r\\nNormative In the context of practical ethics, ‘normative’ refers to a given concept,\\r\\nvalue, or belief that puts a moral demand on one’s practices, i.e. that\\r\\nsuch a concept, value or belief indicates what one ‘should’ or ‘ought\\r\\nto’ do in circumstances where that concept, value or belief applies.\\r\\nPlatform Any hardware or software used to host an application or service.\\r\\nSoftware A set of instructions, data or programs used to operate computers and\\r\\nexecute specific tasks.\\r\\nSolution A set of related software programs and/or services that are sold as a\\r\\npackage.\\r\\nSubordinate Treat or regard as of lesser importance than something else\\r\\nTransparency Characteristic of AI that involves the justifiability of the processes that\\r\\ngo into its development and the implementation and of its outcome –\\r\\ni.e. the soundness of the justification of its use.\\r\\nTransparency is characterized by visibility or accessibility of\\r\\ninformation or the characteristic of being easily seen through and\\r\\nexplained. The principle of transparency entails that development and\\r\\nimplementation processes are justifiable through and through. It\\r\\ndemands as well that an algorithmically influenced outcome is\\r\\ninterpretable and made understandable to affected parties.\\r\\nVersion 1.0, January 1, 2022\\r\\nThe online version of this document is the officially released version.\\r\\n Any copies or print-outs are not controlled\\r\\n5 ROLES AND RESPONSIBILITIES\\r\\nAI Ethics Office Context:\\r\\n• Different AI Ethics stakeholders need to be connected and\\r\\naligned.\\r\\nStructure:\\r\\n• Orchestration of the AI Ethics Office is the responsibility of SAP's\\r\\nChief Sustainability Officer team.\\r\\nResponsibilities:\\r\\n• Convenes the AI Ethics Steering Committee.\\r\\n• Is the contact for a L1 unit if a L1 unit is unable to make a use\\r\\ncase decision regarding AI Ethics or questions or concerns\\r\\nremain.\\r\\nAI Ethics Steering\\r\\nCommittee\\r\\nContext:\\r\\n• As an emerging technology with significant innovation potential\\r\\nwithout standardization so far, execution of ethical AI standards\\r\\nrequires a cross-company approach anchored in strong LOB\\r\\nexpertise and perspectives.\\r\\nStructure:\\r\\n• Orchestration of the AI Ethics Steering Committee is the\\r\\nresponsibility of SAP’s AI Ethics Office.\\r\\n• AI Ethics Steering Committee is a ‘sub-committee’ of the\\r\\nSustainability Council.\\r\\n• In addition to representatives with AI product, legal, governmental\\r\\nand ethical expertise, members include representatives\\r\\nresponsible for non-standard AI development and\\r\\nimplementation.\\r\\nResponsibilities:\\r\\n• Serve to advise SAP personnel on how specific use cases are\\r\\naffected by this policy and the related guiding principles\\r\\n• Monitor, evolve and update SAP’s Guiding Principles for AI.\\r\\n• Oversee ongoing updating of SAP’s AI Ethics policy to guide\\r\\nimplementation of ethical AI in all LOBs developing, deploying or\\r\\nselling AI systems.\\r\\n• Provide guidance/direction on AI use cases to LOBs .\\r\\n• Provide recommendations to SAP Executive Board on critical\\r\\nproduct/customer use cases with wider implications for SAP as a\\r\\ncompany.\\r\\n• Provide Sustainability Council with bi-annual update on\\r\\nstatus/progress (written).\\r\\n• Orchestrate AI Ethics use case review process.\\r\\n• In instances of unresolvable AI System use case conflict between\\r\\nAI Steering Committee guidance/direction and relevant Business\\r\\nUnit, prepare the case and recommendation for Sustainability\\r\\nCouncil review.\\r\\n• Align with and obtain insights from External Advisory panel on AI.\\r\\n• Steering committee members to offer employees, through some\\r\\nof the existing channels, opportunity to provide feedback and\\r\\nexchange dialog on AI Ethics on regular basis.\\r\\nSustainability Council Context:\\r\\n• Sustainability Council received mandate (2018) to act as internal\\r\\n‘Ethics Advisory Board’ for SAP, reviewing critical behavior and\\r\\nmaking recommendations to the SAP Executive Board on\\r\\nappropriate action.\\r\\nStructure:\\r\\n• Orchestration of the Sustainability Council is the responsibility\\r\\nof the Chief Sustainability Officer’s office\\r\\n• The council is the highest-level governance structure for\\r\\nsustainability at SAP\\r\\nVersion 1.0, January 1, 2022\\r\\nThe online version of this document is the officially released version.\\r\\n Any copies or print-outs are not controlled\\r\\n• Senior leaders from all board areas and specific business\\r\\nunits are members of the council\\r\\nResponsibilities:\\r\\n• Oversight of AI Ethics implementation as reported by AI Ethics\\r\\nSteering Committee on bi-annual basis.\\r\\n• Provide longer term strategic direction for AI Ethics related topics\\r\\nat SAP.\\r\\n• Review critical use cases, as elevated from AI Ethics Steering\\r\\nCommittee.\\r\\n• Provide recommendations to SAP Executive Board on critical use\\r\\ncases with wider implications for SAP as a company.\\r\\nImportant Stakeholders\\r\\nimpacted by Policy\\r\\n• Developers and Data Scientists should implement and apply\\r\\nthe requirements to the development processes.\\r\\n• Sales or Consulting/Deployment Personnel (e.g. Admin or\\r\\nData Processors) should ensure that the systems they sell and\\r\\nthe products and services they offer meet the requirements.\\r\\n• Executive Board should support appropriate level of governance\\r\\nstructure to ensure the requirements are met. In addition, where\\r\\nnecessary, the board must take a decision on whether to allow or\\r\\nprohibit a usecase as per the defined escalation process.\\r\\n• End-users, customers, partners and the broader society\\r\\nshould be informed about these requirements and be able to\\r\\nrequest that they are upheld.\\r\\nVersion 1.0, January 1, 2022\\r\\nThe online version of this document is the officially released version.\\r\\n Any copies or print-outs are not controlled\\r\\n6 ETHICAL AI AT SAP\\r\\nWhilst different groups of stakeholders have different roles to play in ensuring that the requirements are met,\\r\\nend-users, customers, partners, and the broader society should be informed about these requirements and\\r\\nbe able to request that they are upheld.\\r\\nAI systems shall only be developed, deployed, used or sold by SAP in accordance with the principles laid out\\r\\nbelow.\\r\\nHuman Agency and Oversight:\\r\\nEmployees need to consider the following when developing AI systems as they relate to Human Agency and\\r\\nOversight:\\r\\n• Before implementation, the decision-making degrees of freedom of the AI system must be defined.\\r\\n• When two or more AI systems are connected to each other or embedded within each other,\\r\\nadditional testing and control measures should be performed for the individual AI systems as well as\\r\\nfor the overall system\\r\\n• The target definition of the AI system must be given by a human.\\r\\n• AI systems shall be subject to appropriate human oversight, and the rights and freedoms of a human\\r\\nshall exceed that of AI systems.\\r\\n• Human oversight shall be achieved through an appropriate governance mechanism. This could\\r\\ninclude but not be exclusive to human-in-the-loop, human-on-the-loop, or human-in-command, and\\r\\nshall be decided on a case-by-case basis. The use cases will be first reviewed based on the impact\\r\\nassessment list. Selected use cases shall be brought to the attention of the steering committee for\\r\\nfurther deliberation and decision.\\r\\n• In alignment with the SAP Global DPP Policy and SAP DPP Guidance documents, in situations\\r\\nwhere humans may be directly impacted by a decision made by SAP’s AI system, human oversight\\r\\nshall be introduced to safeguard that AI system does not undermine human autonomy or introduce\\r\\nunintended consequences.\\r\\n• As far as is practical, a clear and simple explanation shall be provided as to how decisions were\\r\\nmade by an AI system used in automated decision processes.\\r\\n• When human-on-the-loop models are used, appropriate extensive testing and governance shall be\\r\\nconducted during development and deployment to ensure the system behaves as intended by the\\r\\ndevelopers and does not have any unintended behavior, outputs, or usage.\\r\\nAddressing Bias and Discrimination:\\r\\nAI systems often gain insights from the existing structures and behavior of the societies they analyze. As a\\r\\nresult, data-driven technologies can reproduce, reinforce, and amplify patterns of marginalization, inequality,\\r\\nand discrimination that exist in society and may be encoded into data sources used for the creation of AI.\\r\\nEqually, because many of the features, metrics, and analytic structures of the models that enable data\\r\\nmining are chosen by their developers, AI systems can potentially replicate their developers’ preconceptions\\r\\nand biases.\\r\\nFinally, data samples used to train and test algorithmic systems may be insufficiently representative of the\\r\\npopulations or the past situations from which they are drawing inferences. This may include cases where the\\r\\ntype of business, industry, or enterprise from which the original datasets were obtained are inappropriate for\\r\\nthe AI systems being developed and deployed.\\r\\nThese biases can negatively impact both the development and outputs of AI systems and, in turn, customers\\r\\nor end users. Particular care shall be taken when there is a risk of causing discrimination or of unjustly\\r\\nimpacting underrepresented groups.\\r\\nEmployees need to consider the following as they relate to addressing bias and discrimination in SAP\\r\\ndeveloped AI systems:\\r\\n• In addition to the conditions laid down in SAP’s Global DPP Policy, AI systems shall not be\\r\\ndeveloped or deployed to de-anonymize already anonymized data which may result in the\\r\\nidentification of individuals or groups.\\r\\n• SAP shall endeavor to achieve fairness; AI systems shall not intentionally generate unfairly biased\\r\\noutputs.\\r\\n• Where relevant, the data used to train AI systems shall be as inclusive as possible, representing as\\r\\ndiverse a cross-section of the population or past situations as possible, and as free as possible from\\r\\n(or accounted and mitigated for) any historic or socially constructed biases, inaccuracies, errors, and\\r\\nmistakes.\\r\\n• SAP shall endeavor to detect unfairly biased outputs and shall implement technical and/or\\r\\norganizational measures to prevent direct or indirect prejudice, discrimination, or marginalization of \\r\\nVersion 1.0, January 1, 2022\\r\\nThe online version of this document is the officially released version.\\r\\n Any copies or print-outs are not controlled\\r\\ngroups or individuals, e.g. by reducing bias in training data.\\r\\n• Wherever possible, developers shall seek to involve impacted/affected users to evaluate and check\\r\\nthat outputs are diverse and discrimination free.\\r\\n• In addition to the conditions laid down in SAP’s Global Development Policy and Product Standards,\\r\\nprocesses shall be put in place to test and monitor for potential biases during the development,\\r\\ndeployment, and use phase of AI systems.\\r\\no It shall be trained and tested on as expansive as is feasible, representative, relevant,\\r\\naccurate, and generalizable datasets.\\r\\no The model architectures shall not include target variables, features, processes, or analytical\\r\\nstructures which are unreasonable, ethically objectionable, or unable to be validated\\r\\naccording to the principles laid out in this document.\\r\\no It shall be developed and deployed so that it has no intentionally harmful impacts on users\\r\\nand/or direct and indirect affected of the system.\\r\\no Where feasible, a fairness function shall be applied to test AI systems for unbiased output.\\r\\n• AI software shall be user-centric, addressing the widest possible range of applicable end-users, and\\r\\nfollowing relevant accessibility standards, regardless of users’ age, gender, abilities, or\\r\\ncharacteristics.\\r\\n• As it pertains to the addressing of bias and discrimination, AI systems shall comply with or be in\\r\\nalignment with SAP’s Global Development Policy, Product Standards Governance, Product\\r\\nStandards, CUX Design Principles, and Global DPP Policy.\\r\\n• The use of data for the testing of AI systems shall comply with SAP’s Global DPP Policy.\\r\\nTransparency and Explainability:\\r\\nSAP’s AI systems are held to specific standards in accordance with their level of technical ability and\\r\\nintended usage. Their input, capabilities, intended purpose, and limitations shall be communicated clearly to\\r\\nour customers along with the necessary technical tools for training and prediction.\\r\\nThe problem is AI systems are not morally accountable agents and cannot be held accountable for their\\r\\nactions. It is therefore essential that mechanisms be put in place so that SAP developed AI systems are\\r\\nobjective and viable as intended by prioritizing both the transparency of the process by which the AI system\\r\\nis developed, as well as the transparency and interpretability of its decisions and behaviors.\\r\\nEmployees need to consider the following as they relate to the Transparency and Explainability of SAP\\r\\ndeveloped AI systems:\\r\\n• The data sets and the processes that produce an AI system’s decisions, including those of data\\r\\ngathering and data labelling as well as the algorithms used by the developed AI system, shall be\\r\\ndocumented to allow for traceability and transparency.\\r\\n• The capabilities and limitations shall be documented as part of the development process in a manner\\r\\nappropriate to the use case at hand. This shall include information regarding the AI system's level of\\r\\naccuracy (performance metric), as well as its limitations and capabilities.\\r\\n• In alignment and compliance with SAP’s Global DPP Policy, products that use AI systems in the\\r\\nprocessing of personal data must provide transparency to the extent possible as to how the AI\\r\\nsystem was used in clear and simple language if requested by the data subject.\\r\\n• In alignment and compliance with SAP’s Global DPP Policy, AI systems that engage in profiling or\\r\\nautomated decision-making must be able to provide explanations to the extent possible to data\\r\\nsubjects upon request, describing the data segment the subject was placed into and the reasons\\r\\nthey were placed there. In addition, the reasons as to why the decision was made shall be provided\\r\\nif requested by the data subject. The explanation must be such as to provide the data subject\\r\\ngrounds to challenge the decision.\\r\\n• The methods used for developing, testing and validating, and the outcomes of or decisions made by\\r\\nthe AI system shall be fully documented as part of the development process according to SAP’s\\r\\nGlobal Development Policy and Product Development Standards.\\r\\n• Where applicable, when interacting directly with humans (including via Conversational AI or\\r\\n‘Chatbots’):\\r\\no AI systems shall be made identifiable as such to appropriate end users.\\r\\no AI systems shall be developed such that it does not encourage humans to develop\\r\\nattachment and/or empathy of users towards the AI system.\\r\\no AI systems shall clearly signal to end users that its social interaction is simulated.\\r\\n• SAP AI system developers shall endeavor to make the decisions, proposals, and outputs of the AI\\r\\nsystem as transparent as possible, based on the use case. This can be achieved using application\\r\\nlogs or the user interface (UI) to allow for the best understanding and traceability of these. \\r\\nVersion 1.0, January 1, 2022\\r\\nThe online version of this document is the officially released version.\\r\\n Any copies or print-outs are not controlled\\r\\n• The user shall be made aware that AI systems typically work on confidence levels; the actual\\r\\nconfidence level of a particular output shall be made available if required.\\r\\n• The AI system’s purpose, constraints, requirements, and decisions shall be defined and documented\\r\\nin a clear and transparent manner to the non-technical general reader or user.\\r\\n• ‘Black Box’ and/or Deep Neural Network software/ conditions:\\r\\no Where so-called ‘black box’ algorithms have been developed by SAP, other explicability\\r\\nmeasures shall be provided. These should include traceability, auditability, and transparent\\r\\ndocumentation and communication of the software’s capabilities.\\r\\no Wherever possible an explanation of the output shall be made available; where not possible,\\r\\nusers shall be made aware that the output may not be fully explainable.\\r\\no The requirement for this information will be dependent on the context and the severity of the\\r\\nconsequences. A risk matrix approach shall be followed here.\\r\\n• AI system development shall take into account the context and environment in which the system will\\r\\noperate such that, even with good intentions, no harm or misuse is likely to occur to humans when AI\\r\\nsystems are deployed.\\r\\n• To the extent that a 3rd Party AI system (e.g. ‘TensorFlow’) is embedded in SAP solutions, this\\r\\npolicy shall apply to the overall SAP software solution.\\r\\nSociety:\\r\\nSAP developed AI systems shall be developed to augment, complement, and empower human cognitive,\\r\\nsocial, and cultural skills, and not act to prevent or restrict activities commensurate with a free society.\\r\\nEmployees need to consider the following when developing or deploying AI systems as it relates to civic\\r\\nsociety:\\r\\n• In addition to the existing provisions set out in SAP’s Global DPP Policy, AI systems shall not be\\r\\ndeveloped or deployed for human surveillance that is utilized for the targeting of individuals or\\r\\ngroups, either by biometrics, facial recognition, or other identifiable features, with the purpose of\\r\\ndisregarding or abusing the human rights of the individuals or groups.\\r\\n• AI systems shall not be developed or deployed for purposes which cause individuals or groups to be\\r\\ndiscriminated against or excluded from equal access to AI’s benefits and opportunities that may be\\r\\navailable to the wider population.\\r\\n• AI systems shall not be developed or deployed for deception or unfair manipulation of individuals or\\r\\ngroups via public forums, media, or moderation of other similar uses.\\r\\n• AI systems shall not be developed or deployed to undermine human debate or democratic electoral\\r\\nsystems.\\r\\n• AI system development or deployment shall be conducted with minimum to no explicit damage to the\\r\\nenvironment\\r\\nVersion 1.0, January 1, 2022\\r\\nThe online version of this document is the officially released version.\\r\\n Any copies or print-outs are not controlled\\r\\n7 GOVERNANCE\\r\\nUse Case Review\\r\\nAs far as SAP developed AI systems are concerned, SAP personnel shall approach ethical dilemmas and\\r\\ntrade-offs related to their use via reasoned, context-relevant, and evidence-based decision making rather\\r\\nthan intuition or random discretion.\\r\\nWhere a use case proposed for an SAP developed AI system may breach this policy at any stage of the\\r\\nlifecycle of the AI system, or in order to determine whether the application of a specific use case should or\\r\\nshould not be pursued, employees should first raise the issue for evaluation by their immediate L1 unit. This\\r\\neven applies if employees only have doubts or concerns.\\r\\nIf questions or concerns remain, or a decision is unable to be made by the appropriate L1 unit, then\\r\\nemployees should inform the AI Ethics Office via an e-mail to ai.ethics@sap.com describing the use case.\\r\\nOn receipt of the e-mail the AI Ethics Office will convene the AI Ethics Steering Committee, who will review\\r\\nthe use case and advise the employee on how a specific use case may be affected by this policy by\\r\\nassessing each use case submitted to it in the context in which the AI system is to be applied.\\r\\nWhere an employee has a concern that this policy is not being followed for a particular use case, and they\\r\\nare not able to obtain an appropriate response that addresses their concern(s), and wish to do so\\r\\nanonymously, the employee shall use the established whistleblower process to raise this issue to the\\r\\nattention of the AI Ethics Steering Committee.\\r\\nIn the exceptional situation that the AI Ethics Steering Committee cannot come to a decision on a use case\\r\\nquery referred to it, the use case may be referred to the Sustainability Council (the ‘Council’) for final\\r\\narbitration.\\r\\nWhere a use case decision may have wider implications to SAP as a company, then the recommendation\\r\\nmade by either the AI Ethics Steering Committee, or the Council may be referred to the SAP Executive\\r\\nBoard for review and ratification.\\r\\nWhere no ethically acceptable trade-offs can be identified, then the development, deployment, use or sale of\\r\\nthe AI System shall not proceed in that form.\\r\\nThe Steering Committee will continually review the appropriateness of the ethical decision made to ensure\\r\\nthat where necessary suitable changes can be made based on evolving circumstances.\\r\\nUse case Review Process: \\r\\nwww.sap.com/contactsap\\r\\n© 2021 SAP SE or an SAP affiliate company. All rights reserved.\\r\\nNo part of this publication may be reproduced or transmitted in any form or for any purpose without the express permission of SAP SE or an SAP affiliate company.\\r\\nThe information contained herein may be changed without prior notice. Some software products marketed by SAP SE and its distributors contain proprietary software components of other software vendors.\\r\\nNational product specifications may vary.\\r\\nThese materials are provided by SAP SE or an SAP affiliate company for informational purposes only, without representation or warranty of any kind, and SAP or its affiliated companies shall not be liable\\r\\nfor errors or omissions with respect to the materials. The only warranties for SAP or SAP affiliate company products and services are those that are set forth in the express warranty statements\\r\\naccompanying such products and services, if any. Nothing herein should be construed as constituting an additional warranty.\\r\\nIn particular, SAP SE or its affiliated companies have no obligation to pursue any course of business outlined in this document or any related presentation, or to develop or release any functionality\\r\\nmentioned therein. This document, or any related presentation, and SAP SE’s or its affiliated companies’ strategy and possible future developments, products, and/or platform directions and functionality are\\r\\nall subject to change and may be changed by SAP SE or its affiliated companies at any time for any reason without notice. The information in this document is not a commitment, promise, or legal obligation\\r\\nto deliver any material, code, or functionality. All forward-looking statements are subject to various risks and uncertainties that could cause actual results to differ materially from expectations. Readers are\\r\\ncautioned not to place undue reliance on these forward-looking statements, and they should not be relied upon in making purchasing decisions.\\r\\nSAP and other SAP products and services mentioned herein as well as their respective logos are trademarks or registered trademarks of SAP SE (or an SAP affiliate company) in Germany and other\\r\\ncountries. All other product and service names mentioned are the trademarks of their respective companies. See www.sap.com/trademark for additional trademark information and notices.\",\n",
              " 'AI Engagement within Sony Group\\r\\nThrough the utilization of artificial intelligence (AI), Sony aims to contribute to the development of a\\r\\npeaceful and sustainable society while delivering kando - a sense of excitement, wonder or emotion\\r\\n- to the world. Starting from the electronics business, Sony has continued to expand its business area\\r\\nand has become a diverse global company that offers entertainment such as music and movies, as\\r\\nwell as financial services. To operate these business areas based on Sony’s Purpose to ”Fill the world\\r\\nwith emotion, through the power of creativity and technology.”, Sony Group AI Ethics Guidelines are\\r\\nhereby set forth below to ensure and promote a dialogue with various stakeholders and the proper\\r\\nutilization and research and development (hereafter “R&D”) of AI within Sony Group.\\r\\nSony Group AI Ethics Guidelines\\r\\nScope of the Guidelines\\r\\nThe “Sony Group AI Ethics Guidelines” (Guidelines) set forth the guidelines that must be followed by\\r\\nall officers and employees of Sony when utilizing AI and/or conducting AI-related R&D.\\r\\n\"Utilization of AI\" within Sony means the following:\\r\\n1. The provision of products and services by Sony, including entertainment content and\\r\\nfinancial services, which utilize AI; and\\r\\n2. The usage of AI for various purposes by Sony in its business activities such as R&D,\\r\\nproduct manufacturing, service provision, and other operational activities.\\r\\nDefinitions in the Guidelines\\r\\n“AI” means any functionality or its enabling technology that performs information processing for\\r\\nvarious purposes that people perceive as intelligent, and that is embodied by machine learning\\r\\nbased on data, or by rules or knowledge extracted in some methods.\\r\\n“Sony” means Sony Group Corporation and any company where more than 50% of voting rights are\\r\\ndirectly or indirectly owned by Sony Group Corporation.\\r\\nRevision of the Guidelines\\r\\nSony will review and evolve the Guidelines as needed based on national and regional AI-related\\r\\nguidelines, changes in people’s lifestyles and environments, accumulation of practices in the relevant\\r\\nindustry, and information exchanged with its various stakeholders.\\r\\n2\\r\\n1. Supporting Creative Life Styles and Building a Better Society\\r\\nThrough advancing its AI-related R&D and promoting the utilization of AI in a manner harmonized\\r\\nwith society, Sony aims to support the exploration of the potential for each individual to empower\\r\\ntheir lives, and to contribute to enrichment of our culture and push our civilization forward by\\r\\nproviding novel and creative types of kando. Sony will engage in sustainable social development and\\r\\nendeavor to utilize the power of AI for contributing to global problem-solving and for the\\r\\ndevelopment of a peaceful and sustainable society.\\r\\n2. Stakeholder Engagement\\r\\nIn order to solve the challenges arising from use of AI while striving for better AI utilization, Sony will\\r\\nseriously consider the interests and concerns of various stakeholders including its customers and\\r\\ncreators, and proactively advance a dialogue with related industries, organizations, academic\\r\\ncommunities and more. For this purpose, Sony will construct the appropriate channels for ensuring\\r\\nthat the content and results of these discussions are provided to officers and employees, including\\r\\nresearchers and developers, who are involved in the corresponding businesses, as well as for\\r\\nensuring further engagement with its various stakeholders.\\r\\n3. Provision of Trusted Products and Services\\r\\nSony understands the need for safety when dealing with products and services utilizing AI and will\\r\\ncontinue to respond to security risks such as unauthorized access. AI systems may utilize statistical or\\r\\nprobabilistic methods to achieve results. In the interest of Sony’s customers and to maintain their\\r\\ntrust, Sony will design whole systems with an awareness of the responsibility associated with the\\r\\ncharacteristics of such methods.\\r\\n4. Privacy Protection\\r\\nSony, in compliance with laws and regulations as well as applicable internal rules and policies, seeks\\r\\nto enhance the security and protection of customers\\' personal data acquired via products and\\r\\nservices utilizing AI, and build an environment where said personal data is processed in ways that\\r\\nrespect the intention and trust of customers.\\r\\n5. Respect for Fairness\\r\\nIn its utilization of AI, Sony will respect diversity and human rights of its customers and other\\r\\nstakeholders without any discrimination while striving to contribute to the resolution of social\\r\\nproblems through its activities in its own and related industries.\\r\\n6. Pursuit of Transparency\\r\\nDuring the planning and design stages for its products and services that utilize AI, Sony will strive to\\r\\nintroduce methods of capturing the reasoning behind the decisions made by AI utilized in said\\r\\nproducts and services. Additionally, it will endeavor to provide intelligible explanations and\\r\\ninformation to customers about the possible impact of using these products and services.\\r\\n7. The Evolution of AI and Ongoing Education\\r\\nPeople’s lives have continuously changed with the advance in technology across history. Sony will be\\r\\ncognizant of the effects and impact of products and services that utilize AI on society and will\\r\\nproactively work to contribute to developing AI to create a better society and foster human talent\\r\\ncapable of shaping our collective bright future through R&D and/or utilization of AI.',\n",
              " 'Tieto’s AI ethics guidelines\\r\\nHuman rights\\r\\nEnsuring the freedom and liberty or people\\r\\nto serve the social good.\\r\\nSafety & security\\r\\nAI systems are built to prevent misuse and\\r\\nreduce the risk of being compromised.\\r\\nResponsibility\\r\\nCommitted to harness AI for good, for the\\r\\nplanet and humankind.\\r\\nFairness & equality\\r\\nUnbiased, fair and inclusive AI fostering\\r\\ndiversity and equality among people.',\n",
              " \"Workday’s Commitments to Ethical AI\\r\\nApplications that use machine learning (ML) can make predictions that help inform better decisions. Here's our six key principles for responsibly integrating ML.\\r\\n\\r\\nToday’s business environment is more complex than ever. Whether it’s new regulatory requirements or the battle for talent, customers all have a common opportunity: identifying new ways to make smarter decisions that lead to better outcomes. With this in mind, Workday is incorporating machine learning (ML) technologies—a subset of artificial intelligence (AI)—into our applications so that customers can make more informed people and business decisions, accelerate operations, and assist workers with data-driven predictions that lead to better outcomes.\\r\\n\\r\\nAt Workday, ML isn’t about supplanting human decision-makers. Rather, ML-fueled applications make predictions that, when combined with human judgment, help inform better decisions. But the success of ML, like any emerging technology, depends upon trust, and that trust will exist only if companies adhere to responsible, ethical practices.\\r\\n\\r\\nWorkday believes ML in the enterprise will fundamentally improve the way we work and live, but in the face of such a profound technological and societal change, it’s vital that we commit to an ethical compass. Ours is comprised of six key principles that guide how we develop ML for the enterprise responsibly and work to help address its broader societal impact:\\r\\n\\r\\nWe Put People First\\r\\nWorkday always respects fundamental human rights. We apply ML to deliver better business outcomes and help people in their decision-making. Our solutions provide customers control over how recommendations are used.\\r\\n\\r\\nWe Care about Our Society\\r\\nWe believe that humans will always be at the center of work. We focus on how ML can align opportunity with talent, and on contributing to the development of an ML-ready workforce.\\r\\n\\r\\nWe Act Fairly and Respect the Law\\r\\nWorkday acts responsibly in our design and delivery of ML products and services, and strives to identify, address, and mitigate bias in our ML technologies. We aim to ensure that ML recommendations are equitable. Our products and services are developed and designed to enable compliance and we are engaged in the policy dialogue around regulation of new technologies.\\r\\n\\r\\nWe Are Transparent and Accountable\\r\\nWe explain to customers how our ML technologies work, the benefits they offer, and describe the data needed to power any ML solutions we offer. We demonstrate accountability in ML solutions to customers and give them a wide range of choice in how they deploy them.\\r\\n\\r\\nWe Protect Data\\r\\nWorkday’s Privacy Principles apply to all of our products and services, including to our ML efforts. We minimize the data used, and embrace good data stewardship and governance processes.\\r\\n\\r\\nWe Deliver Enterprise-Ready ML Technologies\\r\\nWe apply our leading quality processes—with input from customers—when developing and  releasing ML technologies. We deliver meaningful ML-powered solutions that help our customers tackle real-world challenges.\\r\\n\\r\\nBut it isn’t enough just to have ethics principles: we are building them into the fabric of our product development and are ensuring we have processes that drive continued compliance with them. We have a long history of this in the privacy space, including privacy-by-design processes as well as third-party audits against our controls and standards.\\r\\n\\r\\nWe are embracing a similar set of ethics-by-design controls for ML, and already have in place robust review and approval mechanisms for release of new technologies, as well as any new uses of data. We’re committed to ongoing reviews of our processes, and evolving them to incorporate new industry best practices and regulatory guidelines.\\r\\n\\r\\nAbove all, as we look forward and as with all our product efforts, we are focused on our customers’ needs and requirements so we can provide new services and technologies that allow them, and their people, to achieve more. By partnering together, we can ensure ethical development and use of ML in the enterprise.\",\n",
              " 'Securing safe, accountable, and socially beneficial technology cannot be an afterthought.\\r\\nWith the right focus on ethical standards and safety, we have better chances of finding AI’s potential benefits. By researching the ethical and social questions involving AI, we ensure these topics remain at the heart of everything we do.\\r\\nWe start from the belief that AI should be used for socially beneficial purposes and always remain under meaningful human control. Understanding what this means in practice is essential.  \\r\\n\\r\\nFinding ways to involve the broader society in our work is fundamental to our mission, so partnerships with others in the field of AI ethics is a crucial element of our approach.\\r\\n\\r\\nWe embrace scientific values like transparency, freedom of thought, and the equality of access, and we deeply respect the independence and academic integrity of our researchers and partners.\\r\\nQuestions about AI extend far beyond its technology. Through our partnerships, we’ve created public lectures, forums, and resources to better understand the societal impacts of AI. Below are some of our recent highlights.\\r\\nLaunching public lectures\\r\\nWe partnered with the Royal Society on a free public lecture and panel series, You & AI. These lectures, featuring experts like Kate Crawford and Joseph Stiglitz, explored AI’s capabilities, future directions, and potential societal effects. Each lecture was recorded and is available to watch online.\\r\\nEngaging citizens directly\\r\\nTogether with the RSA, we created the Forum for Ethical AI, a public engagement programme for discussing the use of automated decision-making tools. During this forum, citizen participants developed a critical framework for addressing transparency, accountability, and accessibility of AI technology.\\r\\nConvening experts\\r\\nIn partnership with Princeton University, we organised a workshop to explore how criminal justice systems use AI technology. We brought together technologists and advocates to discuss solutions and create resources, directly informed by affected communities, which explore the harm that can be caused by predictive tools.\\r\\nThemes\\r\\nWe want to promote research that ensures AI works for all. Our research themes are designed to reflect the key ethical challenges that exist for us and the wider AI community. We undertake research and collaborations in each of these areas, determined by the urgent challenges ahead.\\r\\nPrivacy, transparency, and fairness\\r\\nAI systems can use large-scale and sometimes sensitive datasets, such as medical or criminal justice records. This raises important questions about protecting people’s privacy and ensuring that they understand how their data is used. Also, the data used for training automated decision-making systems can contain biases, creating systems that might discriminate against certain groups of people.\\r\\n\\r\\nHow do concepts such as consent and ownership relate to using data in AI systems?\\r\\nWhat can AI researchers do to detect and minimise the effects of bias?\\r\\nWhat policies and tools allow meaningful audits of AI systems and their data?\\r\\nAI morality and values\\r\\nAI systems could make societies fairer and more equal. But different groups of people hold different values, meaning it is difficult to agree on universal principles. Likewise, endorsing values held by a majority could lead to discrimination against minorities.\\r\\n\\r\\nHow can we ensure that the values designed for AI systems reflect society?  \\r\\nHow do we prevent AI systems from causing discrimination?\\r\\nHow do we integrate inclusive values into AI systems?\\r\\nGovernance and accountability\\r\\nThe creation and use of powerful new technologies requires effective governance and regulation, ensuring they are used safely and with accountability. In the case of AI, new standards or institutions may be needed to oversee its use by individuals, states, and the private sector - both internationally and within national borders.\\r\\n\\r\\nWhat kinds of governance makes sense for rapidly developing technologies like AI?\\r\\nCan existing institutions uphold the rights of everyone affected by AI?\\r\\nWhat can we learn from other fields like biotechnology or genetics that might influence how AI is used?\\r\\nAI and the world’s complex challenges\\r\\nBy uncovering patterns in complex datasets and suggesting promising new ideas and strategies, AI technologies may one day help solve some of humanity’s most urgent problems. But applying AI technologies to real-world problems takes careful consideration.\\r\\n\\r\\nWhich problems could AI help address?\\r\\nHow can AI research best contribute?\\r\\nWho should we be working with to help solve problems?\\r\\nMisuse and unintended consequences\\r\\nWhile AI systems have great potential, they also come with risks. For example, they might malfunction or not operate in the ways they were intended. We might also rely on them too heavily in situations that go beyond their abilities or a technology designed to help society might be repurposed in unethical or harmful ways.\\r\\n\\r\\nHow can these risks be monitored across the world?\\r\\nWhat structures can be put in place to minimise harm?\\r\\nHow do we ensure that people maintain control of AI systems?\\r\\nEconomic impact: inclusion and equality\\r\\nLike previous waves of technology, AI could contribute to a huge increase in productivity. However, it could also lead to the widespread displacement of jobs and alter economies in ways that disproportionately affect some sections of the population. This poses important questions about the kinds of societies and economies we want to build.\\r\\n\\r\\nHow can we anticipate the social or economic impacts of AI?\\r\\nWhat new opportunities are created?\\r\\nHow do we ensure AI has a net positive effect on the world?',\n",
              " \"Responsible AI\\r\\n\\r\\nArtificial intelligence (AI) has witnessed the rapid development in recent years. In response to legal and ethical controversies, the National Governance Committee for the New Generation AI published the Governance Principles for the New Generation Artificial Intelligence: Developing Responsible Artificial Intelligence, proposing the theme of “Responsible AI”. These integrate ethical thinking into AI technology to avoid risks. We actively respond to this theme, deepening the interpretation of AI ethics, actively participating in the formulation of AI ethic standards, and guiding the direction of AI technology.\\r\\n\\r\\nWe join hands with stakeholders such as netizens, industry associations, scientific research institutions, and social organizations to promote the construction of a responsible AI ecosystem.\\r\\n\\r\\nAI ethics\\r\\n\\r\\nAs AI technology develops and promotes social innovation, the ethical and moral thinking behind its technology has become a core issue of social concern.\\r\\n\\r\\nBaidu Chairman and CEO Robin Li proposed the four principles of AI ethics. AI should be “safe and controllable”, which is the highest principle; AI’s innovative vision is to promote more equal access to technologies and abilities for humanity; the value of AI is to empower mankind to learn and grow instead of surpassing and replacing mankind; the ultimate ideal of AI is to bring more freedom and possibilities to humankind. These four principles aim to establish concepts and rules that the whole society follow in terms of all new AI products and technologies, so as to enable the co-existence between AI and humankind. Only by adhering to the highest principle of “safe and controllable”, establishing sound AI ethical norms, and accelerating the implementation of AI ethical principles can we use AI technologies to achieve multi-governance, benefit more groups, and realize sustainable social development.\\r\\n\\r\\nWe strive to avoid the negative aspects of AI and the information cocoon, promoting the well-ordered development of AI.\\r\\n\\r\\nThe AI ecosystem\\r\\n\\r\\nBaidu adheres to the ethical principles of human-centered AI. Regarding AI ethics and standards, we followed proposals related to standardization and joined in the preparation of the Introduction of AI Ethics Risks by the National Artificial Intelligence Standardization General Working Group. Baidu also collaborated with CAICT in formulating the Research Report on Privacy and Security of Facial Recognition Technology in Applications and participated in the development of Internet of Vehicles Service —User Personal Information Protection Requirements.\\r\\n\\r\\nIn terms of international exchanges, we have actively joined in the AI for Sustainable Development Goals (AI4SDGs) research plans and international cooperation networks while funding their research projects, actively promoting the training of talent and the leveraging of technology in the AI area to build global consensus.\\r\\n\\r\\nBelieving “Everyone Can AI”, Baidu is encouraged to open-source AI-related technology tools, so every developer can access the world's most advanced AI technologies and the whole society can use the technology more easily. We embrace innovation with an open mind, empower industry with a win-win attitude, and continue to contribute to the flourishing development of the AI field.\\r\\n\",\n",
              " \"To implement AI governance, we have embedded our ethics\\r\\nvalues in our products through diffusing the principles into\\r\\n“ethics by design” standards and an ethics review matrix.\\r\\nSenseTime is among the first companies that have set up an\\r\\nEthics and Governance Committee in the industry and made\\r\\nAI governance a strategic priority. The committee is\\r\\ncomprised of both internal and external experts and ensures\\r\\nthat our business strictly adheres to recognized ethical\\r\\nprinciples and standards.\\r\\nIn response to ethics risks at the data, algorithm, and application levels, SenseTime has established risk control mechanisms covering the entire life cycle of our products and has\\r\\nbegun to see a close-looped AI governance system forming.\\r\\nAt the data level, we have established a Privacy Protection\\r\\nAssessment Mechanism, and a Data Security and Personal\\r\\nInformation Protection Committee to conduct the assessment covering the acquisition, storage, transmission, and\\r\\nprocessing of personal information, so as to ensure that our\\r\\nproducts comply with the design requirements for “privacy\\r\\nprotection by default”.\\r\\nAt the algorithm level, we have established an Algorithm\\r\\nSecurity Assessment Mechanism, and an Algorithm Security\\r\\nManagement Working Group. The working group is tasked\\r\\nwith classifying and managing algorithms according to their\\r\\ndata type, business scenario, ethics risk level, data quality,\\r\\nstorage status, application scale, data importance, intervention degree on the user’s behavior and conducting security\\r\\nassessments on algorithm risks arising from technical limitations, algorithm design, software defects, data security,\\r\\nframework security, and other related dimensions.\\r\\nAt the application level, we have established an Ethics Risk\\r\\nClassification Management Mechanism and an Ethics Risk\\r\\nReview Team to carry out graded and targeted ethics risk\\r\\nmanagement throughout the entire product life cycle of\\r\\ndesign, development, deployment, and operation. We have\\r\\nalso set up supporting processes for self-inspection,\\r\\nassessment and review of risks, and follow-up reviews. We\\r\\nclassify ethics risks from low to high, spanning five levels\\r\\nfrom E0 to E4, based on the impact of final product safety,\\r\\npersonal rights and interests, market fairness, public safety,\\r\\nand environmental health.\\r\\nTo promote the development of responsible and verifiable\\r\\nAI, we have developed a series of internal management\\r\\ntools and technical tools covering data governance,\\r\\nalgorithm evaluation, model examination, and ethics review.\\r\\nTo cultivate an organizational culture for AI governance, we\\r\\nhave published Ethics and Governance Policy, SenseTime AI\\r\\nEthics and Governance Committee Management Charter,\\r\\nand Guidelines for Ethics risk Review, to provide a set of\\r\\nguidelines and clearly-defined standards for aligning\\r\\nemployee’ understanding and actions on AI ethics and\\r\\ngovernance. To better educate and communicate with\\r\\nemployees regarding AI governance, we have established\\r\\nregular training and community engagement platforms.\\r\\nSo far, our AI ethics and governance system and related\\r\\ntechnical tools have been recognized by various third-party\\r\\norganizations, such as the Harvard Business Review and\\r\\nArtificial Intelligence Industry Alliance (AIIA). Our first White\\r\\nPaper on AI Sustainable Development was also included in\\r\\nthe United Nations’ Resource Guide on Artificial Intelligence\\r\\nStrategies.\\r\\nI.\\r\\nOverview of AI Development\\r\\nand Governance\\r\\n05\\r\\nSENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW\\r\\n06\\r\\nOver the past decade, driven by deep learning, big data,\\r\\nand Moore’s Law, AI has made many remarkable breakthroughs, and has been commercialized in various\\r\\nsegments such as computer vision, natural language\\r\\nprocessing, and speech recognition. Today, AI is being\\r\\nwidely deployed in city management, as well as in industries such as education, finance, medical care, retail, transportation, entertainment, and manufacturing, amongst\\r\\nothers, and is expanding into other fields of knowledge\\r\\nexploration such as scientific research and the Arts. AI is\\r\\nincreasingly being recognized and adopted as a general-purpose technology, which accelerates the arrival of the\\r\\nera of ubiquitous intelligence.\\r\\nThroughout history, general-purpose technologies have\\r\\ninevitably brought fundamental changes to the existing\\r\\nstructure of society, while revolutionizing social productivity. This law applies to AI too. In particular, when data-driven\\r\\nAI breaks the boundaries of human cognition, its impact on\\r\\nthe existing social structure will be even greater. Even at\\r\\nthe present stage of “weak (narrow) AI”, concerns about\\r\\nthe robustness and fairness of automated decision-making systems and the abuse of recommender systems, deep\\r\\nsynthesis, and biometric information identification\\r\\ntechnologies clearly indicate that if the industry and all\\r\\nstakeholders don’t act proactively to seek broad consensus on how AI should be designed, developed, and\\r\\ndeployed, the journey to the era of ubiquitous intelligence\\r\\nwill increasingly face trust challenges.\\r\\nFor this reason, AI governance has garnered attention\\r\\nfrom enterprises, government agencies, international\\r\\norganizations, social groups, and other stakeholders in\\r\\nthe past decade and made remarkable progress.\\r\\nLooking at the global AI governance process, it has so\\r\\nfar gone through three stages of development, and has\\r\\nnow entered the stage of implementation:\\r\\nSENSETIME I AI FOR A BETTER TOMORROW\\r\\n07\\r\\nFigure 1 Global AI Governance Development History\\r\\nSource: Institute for AI Industry Research, SenseTime\\r\\n2016 >>> Stage 1.0\\r\\nPrinciple Discussions\\r\\n2020 >>> Stage 2.0\\r\\nPolicy Discussion\\r\\n2022 >>> Stage 3.0\\r\\nTechnical Verification\\r\\nAfter analyzing 84 principles or guidelines\\r\\nregarding AI ethics worldwide, Anna Jobin et al.\\r\\nalso found that 88% of them had been published\\r\\nafter 2016\\r\\nIn February 2020, the European Commission\\r\\nreleased a White Paper on Artificial Intelligence , the\\r\\nfirst in the world to propose a risk-based regulatory\\r\\nframework for AI governance. Since then, major\\r\\ncountries have followed suit\\r\\nIn May 2022, Singapore launched the world’s first\\r\\nopen-source testing toolbox for AI governance,\\r\\n“AI.Verify”\\r\\nStage 1.0 of AI Governance began in 2016 with a focus\\r\\non principle discussion.\\r\\nIn their study, Jessica Fjeld et al. from Harvard University\\r\\nidentified September 2016 as the beginning of AI Governance 1.0 with the publication of the Principles of\\r\\nPartnership on AI by a group of tech giants that included\\r\\nGoogle, Facebook, IBM, Amazon, and Microsoft. 1After\\r\\nanalyzing 84 principles or guidelines regarding AI ethics\\r\\nworldwide, Anna Jobin et al. also found that 88% of them\\r\\nhad been published after 2016, and the number of\\r\\ndocuments released by private enterprises and government agencies accounted for 22.6% and 21.4%, respectively.2\\r\\n1 https://cyber.harvard.edu/publication/2020/principled-ai\\r\\n2 https://doi.org/10.1038/s42256-019-0088-2\\r\\n3 https://ec.europa.eu/info/files/white-paper-artificial-intelligence-european-approach-excellence-and-trust_en\\r\\n4 https://www.pymnts.com/news/regulation/2022/oecds-principles-can-guide-governments-to-design-ai-regulatory-frameworks/\\r\\n5 https://www2.deloitte.com/global/en/insights/industry/technology/technology-media-and-telecom-predictions/2022/ai-regulation-trends.html\\r\\nStage 2.0 of AI Governance began in 2020 with a focus\\r\\non policy discussion.\\r\\nIn February 2020, the European Commission released\\r\\nthe White Paper on Artificial Intelligence3, the first in the\\r\\nworld to propose a risk-based regulatory framework for\\r\\nAI governance. Since then, major countries have\\r\\nfollowed suit and explored regulations on AI-related\\r\\ntechnologies and applications to varying degrees. For\\r\\nthis reason, 2020 is often referred to as the starting point\\r\\nof AI regulation. According to OECD statistics, more than\\r\\n700 AI policy initiatives have been proposed by over 60\\r\\ncountries around the world, while Deloitte Global\\r\\npredicts that 2022 will see even more discussion on the\\r\\nsystematic regulation of AI.45\\r\\n· ·\\r\\nSENSETIME I AI FOR A BETTER TOMORROW\\r\\n08\\r\\nAI has entered a new stage of development\\r\\nsince 2010 where computing power and data\\r\\nbecame the main driving force. At this stage,\\r\\nAI is no longer based on just human cognition\\r\\nto some extent. In fact, its rules have been far\\r\\nbeyond current human cognition, sparking a\\r\\nwave of discussions on its governance.\\r\\n- Dr. Xu Li, Executive Chairman of the Board\\r\\n and CEO of SenseTime\\r\\nThe relationship between principles and action.\\r\\nAn IBM study found that while more than half of\\r\\nsurveyed organizations have published or publicly\\r\\nendorsed common principles of AI ethics, less than a\\r\\nquarter have actually put them into practice. 6AI ethics\\r\\nand governance still face many challenges in practice:\\r\\nFirst, the integration of AI governance into the existing\\r\\norganizational structure. AI governance involves information security, data governance and other overlapping\\r\\nareas with the existing organizational structure. Issues\\r\\nsuch as overlapping responsibilities and unclear scope\\r\\nof work have led to certain constraints on the promotion\\r\\nof implementation at the organizational level. Second, AI\\r\\ngovernance has not been truly integrated into the industry’s business value chain. When promoting AI ethics and\\r\\ngovernance, the lack of clear returns on investment may\\r\\nlead to organizations paying less attention to AI ethics\\r\\nand governance and lagging in implementation. Third,\\r\\nthere is a lack of consensual standards on how to\\r\\nconduct AI governance.\\r\\nThe relationship between policy and practice.\\r\\nPolicymakers and technology developers share different\\r\\nperspectives, positions, and understandings of policy\\r\\nimplications. In the process of promoting AI governance,\\r\\npolicy requirements need to be transformed into practical standards that can be implemented by technical and\\r\\nbusiness personnel. The following four aspects needs to\\r\\nbe kept in mind when promoting AI governance: First,\\r\\npolicy formulation needs to take into account the\\r\\ndynamics and diversity of industries and AI applications,\\r\\nto foster a benign environment conducive to industry\\r\\ndevelopment. Second, different institutions, public\\r\\nagencies, and countries need to strive to promote the\\r\\ninteroperability of AI governance standards. Third, AI\\r\\ngovernance practitioners need to be involved in policy\\r\\nformulation processes, so as to make policies more\\r\\npractical. Fourth, policy makers and industry practitioners need to seek consensus in defining issues related\\r\\nto AI governance.\\r\\nThe relationship between technology and users.\\r\\nAt present, AI governance remains within the purview of\\r\\nprofessional discussions and corporate governance, with\\r\\nend-users yet to be included in the loop of AI governance. Consequently, there are often misunderstandings\\r\\non AI governance issues within the market and society.\\r\\nFor example, some users may define temporary technical\\r\\nissues as long-term governance challenges. Research by\\r\\nIBM found that only 40% of surveyed consumers\\r\\nbelieved businesses were responsibly and ethically\\r\\ndeveloping and deploying new technologies, and users\\r\\nlacked adequate understanding on AI governance.7Therefore, the relationship between technology\\r\\nand users should be properly addressed when promoting AI governance. Technology providers need to seek to\\r\\nexplain technology from the users’ perspectives, while AI\\r\\ngovernance institutions need to clarify the real challenges facing governance, deepen users’ understanding on\\r\\nAI governance, and give users the opportunity to participate in AI governance. In addition, it is necessary for\\r\\nenterprises to increase investment in AI governance-related technical tools, to improve the verifiability of AI\\r\\ngovernance.\\r\\n6 https://www.ibm.com/downloads/cas/VQ9ZGKAE 7 https://www.ibm.com/downloads/cas/VQ9ZGKAE\\r\\n10\\r\\nFigure 2: Relationship Need to Be Properly Addressed in Stage 3.0 of AI Governance\\r\\nSource: Institute for AI Industry Research, SenseTime\\r\\nPrinciple < > Action Policy < > Practice Technology < > User\\r\\nAn IBM study found that while more than half of surveyed\\r\\norganizations have published or publicly endorsed\\r\\ncommon principles of Al ethics, less than a quarter\\r\\nhave actually put them into practice.\\r\\nIn the process of promoting Al governance, policy\\r\\nrequirements need to be transformed into practical\\r\\nstandards that can be implemented by technical and\\r\\nbusiness personnel\\r\\nAn IBM study found that only 40% of surveyed\\r\\nconsumers believed businesses were responsibly and\\r\\nethically developing and deploying new\\r\\ntechnologies, and users lacked adequate\\r\\nunderstanding on Al governance.\\r\\n09\\r\\nStage 3.0 of AI Governance began in 2022 with a focus\\r\\non technical verification.\\r\\nIn 2022, there are an increasing number of initiatives\\r\\naimed at verifying how AI governance is implemented,\\r\\nas the global AI governance process continues to move\\r\\nforward and concepts such as trustworthy and responsible AI gain greater traction. Within the public sectors,\\r\\nthe government of Singapore launched the world’s first\\r\\nopen-source testing toolbox for AI governance, “AI.Verify”, in May 2022. In June 2022, the Spanish government\\r\\nand the European Commission introduced the first pilot\\r\\nproject for an AI regulatory sandbox. On the market side,\\r\\nthe Responsible Artificial Intelligence Institute, a\\r\\nUS-based AI governance research institute has released\\r\\na Responsible AI Certification Program to provide\\r\\nresponsible AI certification services to enterprises, organizations, and institutions.\\r\\nAt stage 3.0, we are of the view that the technical\\r\\nverification of AI governance is twofold: one is to verify\\r\\nthe practicability of principles, guidelines, and policy\\r\\nrequirements through practice, while the other is to\\r\\nverify the degree to which AI ethics standards have\\r\\nbeen implemented by relevant parties through technical or management tools. Next, implementing AI\\r\\ngovernance needs to properly address challenges\\r\\naround the following three relationship groups:\\r\\n· ·\\r\\n·\\r\\n·\\r\\nSENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW\\r\\nthe process of AI development and application, which\\r\\ninvolves the entire life cycle of data, i.e. collection,\\r\\ntransmission, storage, processing, and circulation.\\r\\nAt the algorithm level, the risk mainly involves\\r\\nalgorithm decision-making, black box algorithm, and\\r\\nalgorithm security. Of these, algorithm decision-making risk refers to the inability to predict the reasons\\r\\nand effects of decisions made by AI systems, due to\\r\\nthe unpredictability of the results of algorithmic\\r\\nreasoning and the cognitive limitations of human\\r\\nbeings. For example, the problem of liability fixation is\\r\\na typical one. Black box algorithm risk refers primarily\\r\\nto interpretability risk resulting from opaque\\r\\ndecision-making and the inability to be fully explained\\r\\ndue to the complexity of neural network algorithms.\\r\\nAlgorithm security risk refers to the risk caused by the\\r\\nleakage or malicious modification of model parameters and insufficient fault tolerance and elasticity.\\r\\nAt the application level, the risk involves algorithmic\\r\\nbias, ethical conflict, and labor substitution among\\r\\nothers. For example, due to subjective factors, or bias\\r\\ncontained in training data and data input during\\r\\nself-learning process, machine-learning algorithms\\r\\nmay introduce bias into its decision-making process.\\r\\nThe risk of algorithm abuse can result from ill induction to users and excessive application of algorithms.\\r\\nAI can also impact employment negatively in the long\\r\\nterm, exacerbating unfair competition and market\\r\\ndominance and causing dilemmas and risks in definII. ing responsibility.\\r\\nHow We Think of AI Governance\\r\\n11 12\\r\\nSENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW\\r\\nSenseTime has long attached great importance to AI governance. Since 2019, it has established and maintained a\\r\\nGlobal AI Ethics Risk Registry internally, which covers\\r\\nhundreds of AI ethics best practices and alerts. While closely tracking the development of global AI governance, we\\r\\nstrive to deepen our knowledge and understanding of AI\\r\\ngovernance as we seek more systematic thinking on AI\\r\\ngovernance issues.\\r\\nBased on our in-depth analysis of global risk cases, we\\r\\nobserved that the governance challenges inherent to the\\r\\nAI era stem mainly from three levels: data, algorithm, and\\r\\napplication. Specifically:\\r\\nAt the data level, the risk primarily involves privacy\\r\\nprotection, data governance, and data quality. Of these,\\r\\nprivacy protection risk refers to issues of privacy violation\\r\\nduring AI development, testing, and operation, and it is\\r\\none of the major problems to be addressed in current AI\\r\\napplications. Data quality risk refers to flaws that may\\r\\nexist in training data sets and field data collected for AI,\\r\\nas well as the corresponding adverse effects. This is also\\r\\na type of data risk specific to AI. Data security risk refers\\r\\nto the security protection of data held by enterprises in\\r\\n·\\r\\n·\\r\\n·\\r\\nTechnology and human activities today are\\r\\ndeeply integrated. We need ethics and\\r\\nhumanism to promote the healthy development of science and technology that is beneficial to mankind. Therefore, we attach great\\r\\nimportance to AI ethics and governance.\\r\\n- Dr. Xu Li, Executive Chairman of the Board\\r\\n and CEO of SenseTime\\r\\nIII.\\r\\nAI Ethics for “Balanced Development”\\r\\nSENSETIME I AI FOR A BETTER TOMORROW\\r\\n13 14\\r\\nBased on our own experience and observation of the global\\r\\nAI governance process, we consider AI governance as a\\r\\ndynamic process driven by value, supported by technical\\r\\ntools, implemented with collaboration, and achieved through\\r\\nhierarchical progression. The hierarchy of AI governance\\r\\ncomprises four layers: Functional, Reliable, Controllable and\\r\\nTrustworthy, covering the functionality, security and robustness, controllability and ethical requirements of AI governance.\\r\\nFunctional means that an AI system can satisfy the application requirements in terms of function and performance.\\r\\nReliable means that an AI system can satisfy the requirements of the deployment environment and sustainable\\r\\noperation in terms of security and robustness.\\r\\nControllable means that an AI system can adequately\\r\\nprotect the independent will and rights of human beings\\r\\nand guarantee a human’s control over the system on the\\r\\nfunctional level.\\r\\nTrustworthy means that the design and application of an\\r\\nAI system that conforms to human values.\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\nFigure 3: Hierarchy of AI Governance\\r\\nSource: SenseTime\\r\\n√ the design and application of an AI system that conforms to human values\\r\\n√ an AI system can satisfy application requirements in terms of function and performance\\r\\n√ an AI system can adequately protect the independent will and rights of human beings and guarantee\\r\\n a human’s control over the system on the functional level\\r\\n√ an AI system can satisfy the requirements of the deployment environment and sustainable\\r\\n operation in terms of security and robustness III.\\r\\nAI Ethics for “Balanced Development”\\r\\nSENSETIME I AI FOR A BETTER TOMORROW\\r\\nTrustworthy\\r\\nControllable\\r\\nReliable\\r\\nFunctional\\r\\nSENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW\\r\\n15 16\\r\\nTo embed the values in our products, we have further\\r\\ndiffused the principles into “ethics by design” standards.\\r\\nThese high-level standards are as follows:\\r\\nRespect for human rights. Human freedom and dignity\\r\\nmust be protected along with other basic rights, as\\r\\nglobally recognized ethics standards are upheld, human\\r\\ndevelopment and life experience can be enhanced\\r\\nwithout harming human status.\\r\\nFor good. Sustainable development for humans must be\\r\\nprotected, just as the interests of vulnerable groups are\\r\\nprotected. Adhere to the basic concepts of human\\r\\nethics and morality, and the applications should be\\r\\nreasonable, legal, and compliant.\\r\\nFree from bias. The data used should overall be objective, neutral, and representative and balance universal\\r\\napplicability with the needs of special populations.\\r\\nProtection of privacy. The collection and usage of\\r\\npersonal information should adhere to the principle of\\r\\nFigure 4: Three Core Ethical Principles of Responsible AI\\r\\nSource: SenseTime\\r\\nGiven the state of its development and commercialization,\\r\\nAI technology and its applications are still in the early\\r\\nstages, and AI-related economic form, industry ecosystem,\\r\\nand business model are still in the exploratory stages. Like\\r\\nother general-purpose technologies that have emerged\\r\\nthroughout the course of history, the healthy and sustainable development of AI technology and relevant industries\\r\\nnot only requires an innovative space that is consistent with\\r\\nthe current state of its development, but also needs appropriate guidance and guardrails. Hence, we believe that AI\\r\\ndevelopment and governance should go hand-in-hand,\\r\\ntheir functions and relationships are like that of a nut and\\r\\nbolt, complementing each other and are indispensable.\\r\\nBased on our understanding of AI development and governance, we unveiled the “AI Ethics for Balanced Development” Report in 2021, to further crystalize SenseTime’s\\r\\nthree core ethical principles of responsible AI: sustainability,\\r\\nhuman-centric approach, and controllable technology.\\r\\nSpecifically, the concept of “balanced development” advocates AI development should be complemented with governance and promotes the healthy and sustainable development of the AI industry, as well as the digital transformation\\r\\nof the overall economy and society through AI governance.\\r\\n“Human-centric” advocates respecting, accommodating, and balancing differences in historical, cultural,\\r\\nAI development and governance should go\\r\\nhand-in-hand. If governance is applied\\r\\nprematurely, it may limit AI development.\\r\\nHowever, if it lags behind, the consequences\\r\\ncould be catastrophic and the cost of reparative governance will be high.\\r\\n- Dr. Xu Li, Executive Chairman of the Board\\r\\n and CEO of SenseTime\\r\\nsocial, and economic development among different\\r\\ncountries and regions, and pursuing consensus among\\r\\ndifferent cultures. Meanwhile, we should also ensure the\\r\\nprotection of human rights and privacy and deploy\\r\\ntechnology without prejudice.\\r\\n“Controllable technology” advocates that AI is developed by and for humans and therefore, should be\\r\\ncontrolled by humans. Correspondingly, its controllers,\\r\\ni.e., humans, should be responsible for its actions.\\r\\n“Sustainability” advocates the sustainable development\\r\\nof society, economy, culture, and the environment, and\\r\\npromotes openness and inclusive innovation.\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\ndata minimization. In particular, the processing of sensitive personal information must obtain the consent of\\r\\nthe information subject or specific circumstances\\r\\nprescribed by law.\\r\\nReliable and controllable. Within a certain period of\\r\\ntime, under certain conditions, specific functions can be\\r\\nimplemented without failure. Even in the case of failure,\\r\\neffective shutdown and human takeovers can be implemented.\\r\\nTransparent and explainable. Priority should be given to\\r\\nalgorithmic models that can be explained. Users should\\r\\nbe provided with clear, understandable, and satisfactory descriptions of the product operating mechanism,\\r\\nand the limitations and potential risks of the product\\r\\nshould be presented clearly.\\r\\nVerifiable. It should be possible to verify the algorithmic\\r\\nmodel and its results repeatedly under the same or\\r\\nsimilar conditions.\\r\\nAccountable. The rights and obligations of the subjects\\r\\nin R&D, design, manufacturing, operation, and service\\r\\nshould be clearly defined, and relevant mechanisms\\r\\nshould be available to trace the models and data\\r\\nbehind the output results.\\r\\n01\\r\\n02 03\\r\\nHuman-centric\\r\\nHuman-centric\\r\\nSustainability\\r\\nSustainability Controllable\\r\\nTechnology\\r\\nControllable\\r\\nTechnology\\r\\nPursue ethical consensus between different cultures\\r\\nRespect, tolerate and balance the historical, cultural, social and\\r\\neconomic development differences of different countries and\\r\\nregions around the world\\r\\nEmphasis on human rights, privacy protection and unbiased\\r\\napplication of Technology\\r\\n·\\r\\nAdvance public awareness of the\\r\\nbenefits and potential risks of\\r\\nartificial intelligence technology\\r\\nComply with applicable laws and\\r\\nregulations\\r\\nEnsure human control over\\r\\ntechnology\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\nPromote the sustainable development of social\\r\\neconomy, culture and environment\\r\\nOpen and inclusive cooperation\\r\\nActively explore the application of innovative and\\r\\nsustainable artificial intelligence governance model\\r\\n18\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\nTrust must be the core of the business community and the\\r\\nbasic premise for the widespread acceptance of emerging\\r\\ntechnologies. SenseTime, as an innovative scientific enterprise in the field of AI, has always regarded the trust of the\\r\\nmarket and users as the key to its development. Since\\r\\nSenseTime was founded, all our actions have been guided\\r\\nby responsible AI development and deployment. At the\\r\\nsame time, we believe that responsible AI is not only a\\r\\nprinciple, but also concrete and implementable. The key to\\r\\nachieving this goal is to build a holistic AI governance\\r\\nsystem.\\r\\nAfter exploring AI governance in-depth, we have realized\\r\\nthat the construction of an AI governance system should\\r\\nnot be simply verbal or a quote on paper, but should also be\\r\\ntraceable and well-documented. Therefore, for the first\\r\\nWith the rise of new technologies, the ethics\\r\\nof science and technology faces many new\\r\\ntopics that require joint research involving\\r\\nevery part of society. As an industry leader,\\r\\nSenseTime has the responsibility to adhere to\\r\\nhigh standards of AI ethics.\\r\\n- Zhang Wang, Vice President of SenseTime,\\r\\n Chairman of the AI Ethics and Governance\\r\\n Committee.\\r\\nThe boundaries and core requirement of\\r\\nmanaging AI ethics risks should be the development of responsible AI. In terms of governance and compliance, enterprises should\\r\\nthink and act ahead.\\r\\n- Yang Fan, Co-founder and Vice President of\\r\\n SenseTime, Member of the AI Ethics and\\r\\n Governance Committee.\\r\\ntime in the industry, we propose the development of\\r\\n“responsible and verifiable” AI as our vision for AI governance. Specifically, “responsible and verifiable” AI should\\r\\nmeet the following basic requirements:\\r\\nResponsible for people. AI systems should respect and\\r\\nprotect the dignity and rights of people and contribute\\r\\nto the health and well-being of people.\\r\\nResponsible for society. AI systems should respect and\\r\\nadapt to the customs and habits of different cultures\\r\\nand contribute to the healthy and sustainable development of society.\\r\\nResponsible for the environment. The development of AI\\r\\nsystems should be mindful of its environmental impact,\\r\\nand its use should be beneficial to the sustainable\\r\\ndevelopment of the environment.\\r\\nAccountability for responsible parties. Responsible\\r\\nparties should be clearly defined for the entire life cycle\\r\\nand all modules of every AI system, so as to ensure\\r\\naccountability.\\r\\nRisks evaluated. AI systems should go through adequate\\r\\nethics risk assessments before going online.\\r\\nVerifiable governance process. The entire life cycle of AI\\r\\nsystems and relevant governance processes should\\r\\nhave complete technical logs and documentation.\\r\\nVerifiable governance results. The implementation of AI\\r\\ngovernance standards over the entire life cycle of AI\\r\\nsystems should be supported by technical and administrative tools.\\r\\nIV.\\r\\nResponsible and Verifiable AI\\r\\nSENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW\\r\\n17\\r\\nV.\\r\\nHow We Implement AI Governance\\r\\nSENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW\\r\\n19 20\\r\\n(1) Organization Innovation\\r\\nSenseTime is one of the first companies in the industry to\\r\\nestablish an AI Ethics and Governance Committee and\\r\\nmake AI governance a strategic priority.\\r\\nTo systematically respond to the ethics risks of AI at different levels, such as data, algorithms, and applications,\\r\\nSenseTime officially established the AI Ethics and Governance Committee in January 2020 to develop an AI ethics\\r\\ngovernance system. The AI Ethics and Governance Committee is comprised of two external members and four internal\\r\\nmembers, who come from technical, engineering, legal,\\r\\nethics, and related professional backgrounds. The Committee is also supported by a secretariat, an expert advisory\\r\\ngroup, and an executive working group to ensure the\\r\\nindependence, transparency, professionalism, and effectiveness of ethics governance. In addition, to ensure the\\r\\nefficient operations of the AI Ethics and Governance Committee and to strengthen the compliance of ethics\\r\\nstandards by all employees, SenseTime has published the AI\\r\\nEthics and Governance Committee Management Charter,\\r\\nEthics and Governance Policy, Guidelines for Ethics risk\\r\\nReview and other ethics-related corporate policies.\\r\\nDaily Work Responsibilities\\r\\nImportant Work Content\\r\\nIndependent opinions on major issues of the company and\\r\\nAl Ethics Committee (see below for details).\\r\\nFormulation of\\r\\nethical strategic\\r\\ndevelopment plans.\\r\\nProduct ethics\\r\\nreview and risk\\r\\ncontrol.\\r\\nGeneral ethical\\r\\nknowledge\\r\\ntraining and\\r\\npromotion.\\r\\nEcological\\r\\nconstruction of\\r\\nethical\\r\\ngovernance.\\r\\nParticipation in\\r\\ndiscussion and\\r\\nformulation of\\r\\nethical standards.\\r\\nExtensive joint\\r\\nresearch.\\r\\nDecision-making on major\\r\\nissues of corporate ethical\\r\\nstrategy.\\r\\nFigure 6: Responsibilities of the AI Ethics and\\r\\nGovernance Committee\\r\\nSource: SenseTime\\r\\nFigure 5：Core Requirements for Responsible\\r\\nand Verifiable AI\\r\\nSource: SenseTime\\r\\nSetting and adjusting\\r\\ndevelopment plan of AI\\r\\nEthics Committee.\\r\\nNominating, appointing,\\r\\nand dismissing expert\\r\\nconsultants of Al Ethics\\r\\nCommittee.\\r\\nOther matters stipulated by relevant laws,\\r\\nadministrative regulations, departmental rules,\\r\\nnormative documents, and articles of association.\\r\\nResponsible\\r\\nfor People\\r\\nResponsible\\r\\nfor Society\\r\\nVerifiable\\r\\nGovernance\\r\\nProcess\\r\\nVerifiable\\r\\nGovernance\\r\\nResults\\r\\nResponsible\\r\\nfor the\\r\\nEnvironment\\r\\nResponsible\\r\\nand Verifiable\\r\\nAI\\r\\nAccountability\\r\\nfor\\r\\nResponsible\\r\\nParties\\r\\nRisk\\r\\nEvaluated\\r\\n21 22\\r\\nSENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW\\r\\nAt the same time, to strengthen AI governance at all levels,\\r\\nwe have systematically enhanced the coordination among\\r\\ninternal organization structures and workflows and provided the Information Security Management Committee with\\r\\nthe ability to conduct privacy protection assessment and\\r\\nalgorithm security assessment.\\r\\n(2) Mechanism Innovation\\r\\nIn response to ethics risks at the data, algorithm, and application levels, SenseTime has established risk control mechanisms covering the entire life cycle of our products, and\\r\\nbegun to see a close-looped AI governance system forming.\\r\\nAt the data level, we have established a Privacy Protection\\r\\nAssessment Mechanism, and a Data Security and Personal\\r\\nInformation Protection Committee to conduct the assessment covering the acquisition, storage, transmission, and\\r\\nprocessing of personal information, so as to ensure that our\\r\\nproducts comply with the design requirements for “privacy\\r\\nprotection by default”.\\r\\nAt the algorithm level, we have established an Algorithm\\r\\nSecurity Assessment Mechanism, and an Algorithm Security\\r\\nManagement Working Group. The task of the working\\r\\ngroup is to classify and manage the algorithms according to\\r\\ntheir data type, business scenario, ethics risk level, data\\r\\nquality, storage status, application scale, data importance,\\r\\nintervention degree on the user’s behavior, and to conduct\\r\\nsecurity assessments on algorithm risks arising from technical limitations, algorithm design, software defects, data\\r\\nsecurity, framework security, and other related dimensions.\\r\\nAt the application level, we have established an Ethics Risk\\r\\nClassification Management Mechanism and an Ethics Risk\\r\\nReview Team to carry out graded and targeted ethics risk\\r\\nmanagement throughout the entire product life cycle of\\r\\ndesign, development, deployment, and operation. We have\\r\\nalso set up supporting processes for self-inspection,\\r\\nassessment and review of risks, and follow-up reviews. We\\r\\nclassify ethics risks from low to high, spanning five levels\\r\\nfrom E0 to E4, based on the impact of final product safety,\\r\\npersonal rights and interests, market fairness, public safety,\\r\\nand environmental health:\\r\\nE4 products: prohibited products. These refer to AI\\r\\nproducts that deviate from SenseTime’s ethics principles and violate the requirements of laws and regulations.\\r\\nE3 products: high-risk products. These refer to products\\r\\ndirectly related to the final product’s safety, personal\\r\\nrights and interests, market fairness, public safety, and\\r\\nenvironmental health.\\r\\nE2 products: medium-risk products. These refer to\\r\\nproducts that have indirect or potentially high impact\\r\\non the final product’s safety, personal rights and interests, market fairness, public safety, and environmental\\r\\nhealth.\\r\\nE1 products: low-risk products. These refer to products\\r\\nthat have no obvious impact on the final product’s\\r\\nsafety, personal rights and interests, market fairness,\\r\\npublic safety, and ecological security.\\r\\nE0 products: risk-free products. These refer to products\\r\\nthat exclude machine learning algorithms and AI\\r\\nfunctions.\\r\\nIn addition, to ensure the effective implementation of AI\\r\\ngovernance systems and to promote an ethic-respecting\\r\\nculture in a corporate setting, we have also established\\r\\nEthics Risk Management Goal-setting Mechanism, Ethics\\r\\nIncidents Reporting and Mitigation Mechanism, and Ethics\\r\\nGovernance Quality Control Mechanism.\\r\\n(3) Tools Development\\r\\nTo promote the development of responsible and verifiable\\r\\nAI, we have developed a series of internal management\\r\\ntools and technical tools covering data governance,\\r\\nalgorithm evaluation, model examination, and ethics review.\\r\\nAt the data level, we have developed a unified data governance platform and standardized data collection processes\\r\\nto ensure accuracy, balance, and rationality. Besides, with a\\r\\ndata and privacy protection platform, we realized the privacy encryption of data, thus ensuring complete data\\r\\navailability, reliability, and security. At the same time, we\\r\\nhave designed a set of personal information protection\\r\\nassessment checklists for the whole process of product\\r\\ndevelopment, promoted functional product design oriented to personal information protection, ensured the design\\r\\nprocess of AI products, and limited the collection and\\r\\nprocessing (including usage, disclosure, retention, transmission, and disposal) to clearly defined and necessary\\r\\npurposes.\\r\\nIn addition, in the process of data processing, by developing and deploying automatic labeling tools, we reduce the\\r\\namount of data contacted manually and the risk of introducing human bias at the source of model training. Moreover, the data labeling platform has access control and\\r\\nauthentication functions and can only be accessed by\\r\\ncertified data labeling personnel.\\r\\nFigure 7: SenseTime’s Standard for Ethics Risk Classification\\r\\nSource: SenseTime\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\n·\\r\\nProhibited products. These refer to AI products that deviate from SenseTime's ethics principles and\\r\\nviolate the requirements of laws and regulations.\\r\\n·\\r\\nHigh-risk products. Products that have direct impact on the final product’s safety, personal rights\\r\\nand interests, market fairness, public safety, and environmental health.\\r\\n·\\r\\nMedium-risk products. Products that have indirect or potentially high impacts on the final product’s\\r\\nsafety, personal rights and interests, market fairness, public safety, and environmental health.\\r\\n·\\r\\nLow-risk products. Products that have no obvious impact on the final product’s safety,\\r\\npersonal rights and interests, market fairness, public safety, and environmental health.\\r\\n·\\r\\nRisk-free products. Products that don’t have machine learning algorithms\\r\\nor AI functions.\\r\\n·\\r\\nE4\\r\\nE3\\r\\nE2\\r\\nE1\\r\\nE0\\r\\n23\\r\\nFigure 8: External Recognition of SenseTime’s AI Governance Practices\\r\\nSource: SenseTime\\r\\n24\\r\\nAt present, we have received a number of internationally\\r\\nrecognized certifications for network and data security,\\r\\nincluding The Information Security Management System\\r\\nCertificate (ISO/IEC 27001�2013), The Privacy Information\\r\\nManagement System (PIMS) Certificate (ISO/IEC\\r\\n27701�2019), The Code of Practice for Personally Identifiable Information Protection (ISO/IEC 29151�2017), and The\\r\\nPersonal Information Security Management System Certificate (BS10012). Products sold have also obtained Level 3\\r\\nCertification for Important Information System Classified\\r\\nProtection, and The Trusted Face Certification Special Test\\r\\nCertificate among others.\\r\\nAt the algorithm level, the black box algorithm poses a\\r\\nsignificant risk of discrediting the algorithm and hindering\\r\\nits explanation. When designing a model, we output various\\r\\ntypes of information in the code to enable fast traceability\\r\\nin case of an algorithm decision error.\\r\\nAt the same time, by establishing a model inspection\\r\\nplatform to test the model for inference attacks and reverse\\r\\nattacks, we can check and score the test factors of the\\r\\nalgorithm model against digital world white-box confrontation, digital world black-box query attack robust accuracy, digital world migration attack robust accuracy, physical\\r\\nworld sample attack success rate, and model backdoor\\r\\nattack success rate among others to determine whether the\\r\\nalgorithm model meets the design requirements. When the\\r\\nalgorithm model does not meet the design requirements,\\r\\nwe enable the corresponding algorithm repair module to\\r\\nimprove security. At the same time, we build an AI firewall at\\r\\nthe system level to defend against attacks from adversarial\\r\\nexamples. When the model is released, tests are done on\\r\\nthe test set defined by the product, and manual testing is\\r\\nconducted to ensure that the scale of the test set is large\\r\\nenough and can reach the required accuracy level.\\r\\nIn addition, based on the algorithm verification and evaluation of data sets in real scenarios, we have developed an\\r\\nalgorithm evaluation tool. It is capable of fully evaluating\\r\\nalgorithms through full coverage of the algorithm evaluation in main scenarios and long-tail scenarios, diversified\\r\\nevaluation items and rich index systems, as well as sufficient\\r\\ndata sets and comprehensive evaluation schemes, for\\r\\ncredibility and control of all commercial algorithms.\\r\\nAt the application level, we have designed a set of ethics\\r\\nrisk self-examination tools and a review platform in\\r\\nconjunction with the different stages of review. At present,\\r\\nall SenseTime’s AI products must undergo ethics risk\\r\\nreviews through the review platform at different stages\\r\\nfrom project approval and release to online operation.\\r\\nBefore the review process, self-inspection tools can be\\r\\nused to prepare for the review. During the review process,\\r\\nwe may choose to reject new product proposals, suspend\\r\\nthe ongoing product development projects, or withdraw\\r\\nexisting products that do not meet our principles and\\r\\nstandards.\\r\\nDue to the above-mentioned practices, our AI ethics and\\r\\ngovernance system and related technical tools have been\\r\\nrecognized by various third-party organizations, such as the\\r\\nHarvard Business Review and Artificial Intelligence Industry\\r\\nAlliance (AIIA). Our first White Paper on AI Sustainable\\r\\nDevelopment was also included in the United Nations’\\r\\nResource Guide on Artificial Intelligence Strategies.\\r\\nSENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW\\r\\nVI.\\r\\n“Ethics by Design” in Action\\r\\nCase Study:The SenseRobot\\r\\n — An AI Chinese Chess Robot\\r\\n25 26\\r\\n(4) Fostering Culture\\r\\nWe realize that the key to developing responsible and verifiable AI is to cultivate an organizational culture for AI governance.\\r\\nWith the release of Ethics and Governance Policy, SenseTime AI Ethics and Governance Committee Management\\r\\nCharter, and Guidelines for Ethics Risk Review, we provide\\r\\na set of guidelines and clearly-defined standards for aligning employee’ understanding and actions on AI ethics and\\r\\ngovernance. At the same time, to better acquaint employees with AI governance topics, we have introduced regular\\r\\ntrainings and community engagement platforms. We send\\r\\nimportant trends related to AI governance to all employees\\r\\nweekly and regularly organize seminars, inviting internal\\r\\nand external experts to provide training on AI ethics and\\r\\ngovernance.\\r\\n(5) Developing the Ecosystem\\r\\nWe actively participate in standard-setting bodies related\\r\\nto AI ethics and governance such as the National Information Security Standardization Technical Committee and the\\r\\nInstitute of Electrical and Electronics Engineers (IEEE) and\\r\\nserve as chair or vice chair in several working groups. At the\\r\\nsame time, we have established research cooperation on AI\\r\\nethics and governance with well-known universities at\\r\\nhome and abroad, and research institutes such as Tsinghua\\r\\nUniversity, Shanghai Jiao Tong University, and Artificial\\r\\nIntelligence International Institute.\\r\\nWe jointly launched the Tech4SDG alliance with industry\\r\\npartners to promote the steady development of responsible\\r\\nand verifiable AI. Currently, the Tech4SDG alliance covers 9\\r\\ncountries and regions in Asia, with more than 40 members,\\r\\nincluding many well-known universities and think tanks\\r\\nfrom mainland China, Hong Kong, Macau, Singapore, India,\\r\\nand Saudi Arabia, among others.\\r\\nSENSETIME I AI FOR A BETTER TOMORROW SENSETIME I AI FOR A BETTER TOMORROW\\r\\ntogether through fun and games. Through AI deep learning and self-training, SenseRobot’s capabilities are at\\r\\nexpert-level, and there are suitable activities for both\\r\\nbeginners and experienced players. In addition, it also\\r\\nbrings the whole family together to come up with\\r\\nsolutions for the chess challenges, thereby strengthening\\r\\nthe bonding between children, parents, and grandparents. SenseRobot’s aim to stimulate minds and bring the\\r\\nfamily together is an example of human-centric design.\\r\\nSenseRobot has a simple and sleek appearance. It presents\\r\\nitself as a little “astronaut”, who teaches and plays Chinese\\r\\nchess with children “face-to-face”.\\r\\nDr. Xu Li, Executive Chairman of the Board and CEO of\\r\\nSenseTime, said at the product launch event, “Our goal is to\\r\\ncreate a robot that can physically ‘think’ and ‘act’ with our\\r\\nleading AI technology, bring industrial grade AI technology\\r\\ninto every family, and make real interactions with children\\r\\nand elderly. It can not only accompany the whole development period of children, but also make high technology\\r\\nintuitive, understandable and interesting for elderly. It will\\r\\nbridge the digital divide and build emotional connection with\\r\\ntechnology, while bringing overall enjoyment to the whole\\r\\nfamily.”\\r\\nSenseRobot features AI chess learning and various levels of\\r\\nchallenges, among others. It can introduce and explain\\r\\nChinese chess culture, rules and skills of each chess piece to\\r\\nchildren without previous experience. While training the\\r\\nchildren, it can also improve their cultural literacy. In addition,\\r\\nit also contains more than 100 endgames and 26 levels of\\r\\nchess competition, so that users not only experience playing\\r\\nwith actual Chinese chess pieces, but also enjoy the mental\\r\\nstimulation at various difficulty levels.\\r\\nWith SenseTime’s leading AI technology, the SenseRobot has\\r\\nremarkable coordination and can achieve millimeter-level of\\r\\noperation accuracy to ensure the game runs smoothly.\\r\\nFurthermore, it has been certified and authorized by the\\r\\nChess and Card Management Center of the General Administration of Sport and the Chinese Xiangqi Association, so\\r\\nusers can be assessed for levels 16 – 13 in the official\\r\\nChinese chess level examinations.\\r\\nBased on feedback from users of the first batch of trials, they\\r\\nsaid that SenseRobot is a product that brings the family\\r\\nSENSETIME I AI FOR A BETTER TOMORROW\\r\\n27 28\\r\\nAs mobile phones and tablets become increasingly dominant in our lives, many parents hope to see their children\\r\\nspend less time on their electronic devices. On August\\r\\n9th, 2022, SenseTime officially launched its first household\\r\\nconsumer AI product, SenseRobot, the AI Chinese chess\\r\\nrobot. Integrated with SenseTime’s leading AI technology\\r\\nand mechanical arm technology, the SenseRobot is a\\r\\nphysical robot that can be placed at homes and on the\\r\\ntable. Children interacting with SenseRobot need not look\\r\\nat its electronic screen, and are able to learn and play\\r\\nChinese chess without straining their vision.\\r\\nSENSETIME I AI FOR A BETTER TOMORROW\\r\\nFigure 9: SenseRobot Application Scenario\",\n",
              " 'Clinical AI Governance\\r\\n\\r\\n“Our Clinical Artificial Intelligence Governance is formed of seven pillars that merge statutory, regulatory and professional best-practice to ensure our AI helps make it easier for our users to take care of themselves and their loved ones.”\\r\\n\\r\\nDr Keith Grimes, Clinical AI Director\\r\\n\\r\\nAt Babylon we believe everyone, everywhere, has the right to a long, healthy and happy life. With medical professionals in short supply we are using artificial intelligence (AI) to amplify the impact of their work.\\r\\n\\r\\nWe use a robust system of governance to transparently measure and improve the AI and ensure it is used as appropriately as possible. We also adhere to the United Kingdom’s code of conduct for artificial intelligence (AI) systems used by the NHS.\\r\\n\\r\\nOur approach to AI Governance follows the NHS guidance of:\\r\\n\\r\\n\\r\\n\\r\\nQuality Audit\\r\\n\\r\\nWe have a cyclical process of improving the quality of our AI. We review the performance of our AI, compare it against standards and refine it as a result. These standards may be internally set, based on historical performance, developed through expert consultation and adaptation of existing clinical standards, or come from the growing range of national and international standards of best practice.\\r\\n\\r\\n\\r\\n\\r\\nEffectiveness\\r\\n\\r\\nWe apply knowledge derived from our Scientific Research team, external research, clinical experience and user studies, to keep improving outcomes for users whilst optimising processes. We monitor practice, inform our teams and implement changes.\\r\\n\\r\\n\\r\\n\\r\\nEducation and Training\\r\\n\\r\\nWe educate and continually train not only the staff who develop and test the AI, but also the AI itself. This helps ensure the highest quality of clinical data and knowledge is used in the development of all our AI, and that every effort is made to ensure that conscious and unconscious bias is minimised.\\r\\n\\r\\n\\r\\n\\r\\nResearch and Development\\r\\n\\r\\nOur Scientific Research team publishes peer-reviewed research and works with researchers from multiple universities to ensure best research practice. We are working with the World Health Organisation and others to establish a standardised benchmark for symptom checkers.\\r\\n\\r\\n\\r\\n\\r\\nRisk Management\\r\\n\\r\\nOur robust methodology assesses and manages risk to users, staff and the healthcare system, identifying and responding to incidents, complying with safety standards, and operating to the highest ethical standards. This includes management of any internal or external reports received. In assessing and managing risk, we comply with the NHS’ DCB0129 and DCB0160 Clinical Risk Management Standards in the UK, adhere to the National Institute for Health and Care Excellence (NICE) Guidance on the ‘Evidence Standards Framework for Digital Health Technologies’, utilise the NHS Clinical Risk Matrix throughout our testing and assurance processes, and globally apply EN ISO 14971:2012 risk management to our medical devices.\\r\\n\\r\\n\\r\\n\\r\\nInformation Management\\r\\n\\r\\nWe comply with applicable data protection laws (including the GDPR) and follow best practice in the ethical, secure, and legal use of patient data in the delivery of healthcare, whilst ensuring that the full value of this important resource is realised for the greatest benefit of the patients.\\r\\n\\r\\n\\r\\n\\r\\nOpenness\\r\\n\\r\\nWe engage with the global community of clinicians, our users and the wider public and welcome both feedback and compliments! We use extensive user research to guide changes, our users can feedback instantly using a real-time star rating system and we have multiple easy ways of getting in contact. All our staff can raise any concerns and we work closely with trusted bodies across the world including: World Health Organisation, NHSX, NHS Digital and leading academics in the US, UK and China.',\n",
              " 'DataRobot’s Framework for Trusted AI\\r\\nDataRobot’s technology was created by some of the world’s top data scientists, incorporating best practices with recent research developments. We recognize that AI trust is multidimensional and an AI creator, operator, and consumer each have different needs. Success happens when humans and AI work together.\\r\\nPerformance\\r\\nThe DataRobot platform incorporates guardrails to ensure performance and enable the democratization of AI so that knowledgeable business users across an organization can rely on it.\\r\\nValue and Risk\\r\\nWith DataRobot’s Humble AI feature, you get real-time analysis and protection for the predictions generated by any of your deployed models, making it easier to trust your model and get more value from it.\\r\\nEthical and Explainable\\r\\nTechnical and non-technical users must be able to explain AI models in a way that is transparent and reflects your company’s values. DataRobot’s platform is not a black box and will give you clear and concise explanations for the predictions it produces.',\n",
              " 'Innovation\\r\\nQuality, Continuous Improvement\\r\\n\\r\\nAt Argo, we hold ourselves to incredibly high quality standards and, in turn, expect Suppliers to provide the highest quality products and services. Argo also expects Suppliers to demonstrate their commitment to continuous improvement by providing feedback and input into opportunities for the products or services that they provide to Argo to ensure the highest quality is achieved.\\r\\nEmbracing Change and Technology\\r\\n\\r\\nArgo recognizes that technology and business priorities change rapidly. We expect our Suppliers to be responsive and adaptive to these changes as applicable.',\n",
              " 'Corporate Communications\\r\\nPress release\\r\\n12 October 2020\\r\\nBMW Group code of ethics for artificial intelligence.\\r\\nMunich. The use of artificial intelligence (AI) is a central element of the digital\\r\\ntransformation process at the BMW Group. The BMW Group already uses AI throughout\\r\\nthe value chain to generate added value for customers, products, employees and\\r\\nprocesses.\\r\\nBuilding on the fundamental requirements formulated by the EU for trustworthy AI, the\\r\\nBMW Group has worked out seven basic principles covering the use of AI within the\\r\\ncompany. These will be continuously refined and adapted as required according to the\\r\\nmulti-layered application of AI across all areas of the company. In this way, the BMW\\r\\nGroup will pave the way for extending the use of AI and increase awareness among its\\r\\nemployees of the need for sensitivity when working with AI technologies.\\r\\nCompany\\r\\nBayerische\\r\\nMotoren Werke\\r\\nAktiengesellschaft\\r\\nPostal address\\r\\nBMW AG\\r\\n80788 München\\r\\nTelephone\\r\\n+49 89 382 60340\\r\\nInternet:\\r\\nwww.bmwgroup.com\\r\\nPress release\\r\\nDate 12 October 2020\\r\\nSubject BMW Group code of ethics for artificial intelligence.\\r\\nPage 2\\r\\nCorporate Communications\\r\\nSeven principles covering the development and application of artificial\\r\\nintelligence at the BMW Group:\\r\\n• Human agency and oversight.\\r\\nThe BMW Group implements appropriate human monitoring of decisions made by\\r\\nAI applications and considers possible ways that humans can overrule algorithmic\\r\\ndecisions.\\r\\n• Technical robustness and safety.\\r\\nThe BMW Group aims to develop robust AI applications and observes the applicable\\r\\nsafety standards designed to decrease the risk of unintended consequences and\\r\\nerrors.\\r\\n• Privacy and data governance.\\r\\nThe BMW Group extends its state-of-the-art data privacy and data security\\r\\nmeasures to cover storage and processing in AI applications.\\r\\n• Transparency.\\r\\nThe BMW Group aims for explainability of AI applications and open communication\\r\\nwhere respective technologies are used.\\r\\n• Diversity, non-discrimination and fairness.\\r\\nThe BMW Group respects human dignity and therefore sets out to build fair AI\\r\\napplications. This includes preventing non-compliance by AI applications.\\r\\n• Environmental and societal well-being.\\r\\nThe BMW Group is committed to developing and using AI applications that promote\\r\\nthe well-being of customers, employees and partners. This aligns with the BMW\\r\\nGroup’s goals in the areas of human rights and sustainability, which includes climate\\r\\nchange and environmental protection.\\r\\nPress release\\r\\nDate 12 October 2020\\r\\nSubject BMW Group code of ethics for artificial intelligence.\\r\\nPage 3\\r\\nCorporate Communications\\r\\n• Accountability.\\r\\nThe BMW Group’s AI applications should be implemented so they work responsibly.\\r\\nThe BMW Group will identify, assess, report and mitigate risks, in accordance with\\r\\ngood corporate governance.\\r\\nOverall centre of competence for the company: “Project AI”.\\r\\n“Project AI” was launched in 2018 to ensure that AI technologies are used ethically and\\r\\nefficiently. As the BMW Group’s centre of competence for data analytics and machine\\r\\nlearning, it ensures rapid knowledge and technology sharing across the company.\\r\\nProject AI therefore plays a key role in the ongoing process of digital transformation at\\r\\nthe BMW Group and supports the efficient development and scaling of smart data and\\r\\nAI technologies. One of the developments to come out of Project AI is a portfolio tool\\r\\nwhich creates transparency in the company-wide application of technologies making\\r\\ndata-driven decisions. This D³ (Data Driven Decisions) portfolio currently spans 400 use\\r\\ncases, of which more than 50 are available for regular operation.\\r\\nIn the event of enquiries please contact:\\r\\nMartin Tholund, Press Spokesperson Research, New Technologies, Innovations\\r\\nTel.: +49-89-382-77126, e-mail: martin.tholund@bmwgroup.com\\r\\nBenjamin Titz, Head of BMW Group Design, Innovation & Motorsport Communications\\r\\nTel.: +49 (0)179 – 743 80 88, e-mail: benjamin.titz@bmw.de\\r\\nThe BMW Group\\r\\nWith its four brands BMW, MINI, Rolls-Royce and BMW Motorrad, the BMW Group is the world’s leading\\r\\npremium manufacturer of automobiles and motorcycles and also provides premium financial and mobility\\r\\nservices. The BMW Group production network comprises 31 production and assembly facilities in 15\\r\\ncountries; the company has a global sales network in more than 140 countries.\\r\\nIn 2019, the BMW Group sold over 2.5 million passenger vehicles and more than 175,000 motorcycles\\r\\nworldwide. The profit before tax in the financial year 2019 was € 7.118 billion on revenues amounting to\\r\\n€ 104.210 billion. As of 31 December 2019, the BMW Group had a workforce of 126,016 employees.\\r\\nThe success of the BMW Group has always been based on long-term thinking and responsible action. The\\r\\ncompany has therefore established ecological and social sustainability throughout the value chain,\\r\\ncomprehensive product responsibility and a clear commitment to conserving resources as an integral part\\r\\nof its strategy.\\r\\nPress release\\r\\nDate 12 October 2020\\r\\nSubject BMW Group code of ethics for artificial intelligence.',\n",
              " 'Presenting the Adarga Committee for Responsible AI (ACRAI) which guides Adarga’s approach to the responsible creation, good operation, human-centric governance and oversight of Artificial Intelligence (AI).\\r\\n\\r\\nArtificial Intelligence (AI) is an important and powerful 21st-century technology, and as AI becomes more prevalent in our everyday lives, so too does the topic of ethics. This responsibility is central to Adarga’s work in enhancing human ingenuity and helping our users to discover the deep and critical insights that drive faster and better decisions. \\r\\n\\r\\nAt Adarga, we address this head-on within our business: acknowledging, examining and ensuring responsibility, transparency, accountability, and ethics are ever-present in our work as a company at the cutting edge of AI technology. \\r\\n\\r\\nThe Adarga Committee for Responsible AI (ACRAI) was specifically created to meet this need. ACRAI guides Adarga’s approach in the responsible creation, good operation, and human-centric governance and oversight of AI. \\r\\n\\r\\nACRAI is composed of Adarga team members at all levels and from all departments to ensure equality and equitability in our discussions around responsible AI use. ACRAI supports Adarga employees in the creation of responsible AI software products from idea to release - advising in our use of data and informing business decisions about who we work with. The Committee creates and oversees a framework that informs the direction and purpose of our Knowledge Platform®, guides Adarga as a company in the sometimes contentious field of AI, and through which any member of the Adarga team can question or query what is right.\\r\\n\\r\\nThis starts with our Responsible AI Principles:\\r\\n \\r\\n1.    We retain human oversight and accountability \\r\\n2.    We prize transparency and explainability \\r\\n3.    We operate reliably, reproducibly and resiliently \\r\\n4.    We uphold fairness and non-discrimination \\r\\n5.    We have a positive impact across our community\\r\\n\\r\\nThese Principles form the foundation of our Responsible AI Framework, governed by the Committee’s Terms of Reference and a Code of Practice. They support Adarga team members in working with best practices in Responsible AI on a day-to-day level and at the highest business level, to ensure that we have clear accountability process and purpose.\\r\\n\\r\\nThe ACRAI is also actively engaged with the ongoing legal and governance developments around AI. Where legislation and lawmakers may be less agile in responding to the demands and opportunities afforded by this fast-moving technology, this has created a space where companies need to take responsibility for their AI design, development, implementation and usage.  \\r\\n\\r\\nThe repercussions of not doing this is causing concern in the general public, creating an environment where the benefits of AI are tarnished by the discussion of unintended algorithm performance, or around the deliberate misuse of AI like fake news, bad actors or bots. Simultaneously, there are fundamental questions to be asked of AI around trust, ethics, and understandability. The ACRAI is Adarga’s mechanism by which we can ask these same questions of our AI technology and how Adarga’s work intersects with these critical issues in order to establish how best to govern this complex environment.\\r\\n\\r\\nThrough the ongoing work of ACRAI and the values of Adarga as a company, we are continuing to drive forward the responsible and transparent use of AI. Our platform supports the human user to get to the answers they need to empower their decision-making. Our AI software does not seek to replace the human user, but to work with human teams of analysts to make data manageable and information more accessible to them. \\r\\n\\r\\nResponsible AI, ethics and transparency are becoming ever more vital, in parallel to the increasing presence of AI in our lives, work, homes and culture. Like shifting the tiller of a boat, any changes to our machine learning product design must take into consideration the wider possible impact. We believe in foregrounding Responsible AI and supporting our data scientists and engineers in making these decisions in a conscientious way to meet the challenges that this new technology necessitates, and the new responsibilities that are now at the feet of companies like Adarga working in AI. \\r\\n\\r\\n\"Embedding these Responsible AI principles in our ways of working, processes and business ethos is fundamental as we continue to develop and deploy state-of-the-art AI capability to our customers.\"',\n",
              " 'Treating Customers Fairly and Fostering AI for Good\\r\\nWe seek to maintain the highest level of professional and ethical standards when conducting business. We observe an internal Anti-Bribery and Corruption Policy and handle personal information in accordance with our internal and external Privacy policies.\\r\\n\\r\\nWe strongly believe that AI has an essential role to play in creating a more sustainable economic world. We encourage the responsible use of AI by providing the appropriate tooling, training, advisory, and incentive to all customers and employees.\\r\\nCreating a Socially and Environmentally Responsible Supply Chain\\r\\nIn our procurement operations, we ensure a spirit of fairness, impartiality, respect for laws, and conformity with social ethics. We are committed to promoting diversity in our professional dealings with suppliers. We believe establishing a relationship of trust and collaboration in Corporate Social Responsibility (CSR) strategy will accelerate both our suppliers’ and our own sustainability action’s impact.',\n",
              " '1. By People, For People\\r\\n\\r\\nWe incorporate human oversight into AI. With people at the core, AI can enhance the workforce, expand capability and benefit society as a whole.\\r\\n\\r\\n2.       Accessible and Shared\\r\\n\\r\\nWe support open-source communities whenever appropriate to further access, collaboration, standardization and participation in industry discussion.\\r\\n\\r\\n3.       Secure and Ethical\\r\\n\\r\\nWe are grounded in ethics, safety, and values at every stage of AI, including our privacy principles and security safeguards.\\r\\n\\r\\n\\xad   Design: We use varied, validated datasets and diverse human input to achieve objectives.\\r\\n\\xad   Development: We use a transparent approach to algorithms that includes safeguards.\\r\\n\\xad   Deployment: We monitor outcomes to ensure accuracy and help minimize biases.\\r\\nThese may seem simple and common sense. But as AI leaders, we believe their importance can’t be overstated. These principles will guide our decisions from design to deployment.\\r\\n\\r\\nHere’s a hypothetical example. AI can make our network more efficient. It reduces power needs and physical duplication by offering fast and smart routing decisions. But what if a new algorithm to skirt outages kept favoring urban customers at the expense of rural ones? Not on purpose – but as an unintended consequence.\\r\\n\\r\\nNo organization will be perfect, but that’s what humans must try to anticipate, catch and repair. The people who run AI decide on the data used, the goals set, the algorithms deployed, and the outcomes monitored.',\n",
              " 'AI Guidelines\\r\\nDeutsche Telekom\\r\\nPreamble\\r\\nTwo of Deutsche Telekom’s most important goals are to keep being a trusted companion and to enhance customer experience.\\r\\nWe see it as our responsibility - as one of the leading ICT companies in Europe - to foster the development of “intelligent technologies”. At least either important, these technologies, such as AI,\\r\\nmust follow predefined ethical rules.\\r\\nTo define a corresponding ethical framework, firstly it needs a common understanding on what AI means. Today there are several definitions of AI, like the very first one of John McCarthy (1956)\\r\\n“Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.” In line with other companies and main players\\r\\nin the field of AI we at DT think of AI as the imitation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, and self-correction.\\r\\nAfter several decades, Artificial Intelligence has become one of the most intriguing topics of today – and the future. It has become widespread available and is discussed not only among experts\\r\\nbut also more and more in public, politics, etc.. AI has started to influence business (new market opportunities as well as efficiency driver), society (e.g. broad discussion about autonomously\\r\\ndriving vehicles or AI as “job machine” vs. “job killer”) and the life of each individual (AI already found its way into the living room, e.g. with voice steered digital assistants like smart speakers).\\r\\nBut the use of AI and its possibilities confront us not only with fast developing technologies but as well as with the fact that our ethical roadmaps, based on human-human interactions, might not\\r\\nbe sufficient in this new era of technological influence. New questions arise and situations that were not imaginable in our daily lives then emerge.\\r\\nWe as DT also want to develop and make use of AI. This technology can bring many benefits based on improving customer experience or simplicity. We are already in the game, e.g having\\r\\nseveral AI-related projects running. With these comes an increase of digital responsibility on our side to ensure that AI is utilized in an ethical manner. So we as DT have to give answers to our\\r\\ncustomers, shareholders and stakeholders.\\r\\nThe following Digital Ethics guidelines state how we as Deutsche Telekom want to build the future with AI. For us, technology serves one main purpose: It must act supportingly. Thus AI is in any\\r\\ncase supposed to extend and complement human abilities rather than lessen them.\\r\\nRemark: The impact of AI on DT jobs – may it as a benefit and for value creation in the sense of job enrichment and enlargement or may it in the sense of efficiency - is however not focus of\\r\\nthese guidelines. \\r\\n1. We are responsible.\\r\\nThe human always remains responsible. Our solutions come with a clear definition of who is\\r\\nresponsible for which AI system or feature. We are in charge of our products and services. And,\\r\\nwe know who is in charge for partner or third party solutions.\\r\\nWith AI technology being in its infancy, we are aware of our responsibility in development – from the very beginning. We make\\r\\nsure that we clarify which initiative or product owner has which responsibilities. For partners or third parties, we define clear\\r\\nguidelines for when a partnership can be established. And, we declare which duties are connected to the respective AI parts. \\r\\n2. We care.\\r\\nWe act in tune with our company values. Our systems and solutions must subordinate to humandefined rules and laws. Therefore, in addition to our technical requirements, our systems and\\r\\nsolutions have to obey the rules and laws that we as Deutsche Telekom, our employees – and\\r\\nhuman beings as such – follow.\\r\\nAI systems have to meet the same high technical requirements as any other IT system of ours, such as security, robustness, etc.\\r\\nBut since AI will be (and already is) a great part of our everyday lives, even guiding us in several areas, AI systems and their\\r\\nusage also have to comply with our company values (Deutsche Telekom’s Guiding Principles and Code of Conduct), ethical\\r\\nvalues, and societal conventions. We have to make sure of that. \\r\\n3. We put our customers first.\\r\\nWe enrich and simplify our customers’ lives. If an AI system or the usage of customer-related\\r\\ndata helps us to benefit our customers, we embrace this opportunity to meet their demands and\\r\\nexpectations.\\r\\nThe aggregation and use of customer data – especially in AI systems – shall always be clear and serve a useful purpose towards\\r\\nour customers. Systems and processes that support in the background are as important as services that interact with our\\r\\ncustomers directly.\\r\\n4. We are transparent.\\r\\nIn no case we hide it when the customer’s counterpart is an AI. And, we are transparent about\\r\\nhow we use customer data. As Deutsche Telekom, we always have the customer’s trust in mind\\r\\n– trust is what we stand for.\\r\\nWe are acting openly to our customers. It is obvious to our customers that they are interacting with an AI when they do. In\\r\\naddition, we make clear, how and to which extent they can choose the way of further processing their personal data. \\r\\n5. We are secure.\\r\\nData security is a prime quality of Deutsche Telekom. In order to maintain this asset, we ensure\\r\\nthat our security measures are up to date while having a full overview of how customer related\\r\\ndata is used and who has access to which kind of data.\\r\\nWe never process privacy-relevant data without legal permission. This policy applies to our AI systems just as much as it does to\\r\\nall of our activities. Additionally, we limit the usage to appropriate use cases and thoroughly secure our systems to obstruct\\r\\nexternal access and ensure data privacy.\\r\\n6. We set the framework.\\r\\nOur AI solutions are developed and enhanced on grounds of deep analysis and evaluation. They\\r\\nare transparent, auditable, fair, and fully documented. We consciously initiate the AI’s\\r\\ndevelopment for the best possible outcome.\\r\\nThe essential paradigm for our AI systems’ impact analysis is “privacy und security by design”. This is accompanied e.g. by risks\\r\\nand chances scenarios or reliable disaster scenarios. We take great care in the initial algorithm of our own AI solutions to prevent\\r\\nso called “Black Boxes” and to make sure that our systems shall not unintentionally harm the users. \\r\\n7. We maintain control.\\r\\nWe are able to deactivate and stop AI systems at any time (kill switch). Additionally, we remove\\r\\ninappropriate data to avoid bias. We have an eye on the decisions made and the information fed\\r\\nto the system in order to enhance decision quality.\\r\\nWe take responsibility for a diverse and appropriate data input. In case of inconsistencies, we rather stop the AI system than\\r\\npursue with potentially manipulated data. We are also able to “reset” our AI systems in order to remove false or biased data. By\\r\\nthis, we install a lever to reduce (unintended) unsuitable decisions or actions to a minimum. \\r\\n8. We foster the cooperative model.\\r\\nWe believe that human and machine intelligence are complementary, with each bringing its own\\r\\nstrength to the table. While we believe in a people first approach of human-machine\\r\\ncollaboration, we recognize, that humans can benefit from the strength of AI to unfold a potential\\r\\nthat neither human or machine can unlock on its own.\\r\\nWe recognize the widespread fear, that AI enabled machines will outsmart the human intelligence. We as Deutsche Telekom\\r\\nthink differently. We know and believe in the human strengths like inspiration, intuition, sense making and empathy. But we also\\r\\nrecognize the strengths of AI like data recall, processing speed and analysis. By combining both, AI systems will help humans to\\r\\nmake better decisions and accomplish objectives more effective and efficient. \\r\\n9. We share and enlighten.\\r\\nWe acknowledge the transformative power of AI for our society. We will support people and\\r\\nsociety in preparing for this future world. We live our digital responsibility by sharing our\\r\\nknowledge, pointing out the opportunities of the new technology without neglecting its risks. We\\r\\nwill engage with our customers, other companies, policy makers, education institutions and all\\r\\nother stakeholders to ensure we understand their concerns and needs and can setup the right\\r\\nsafeguards. We will engage in AI and ethics education. Hereby preparing ourselves, our\\r\\ncolleagues and our fellow human beings for the new tasks ahead.\\r\\nMany tasks that are being executed by humans now will be automated in the future. This leads to a shift in the demand of skills. Jobs will be\\r\\nreshaped, rather replaced by AI. While this seems certain, the minority knows what exactly AI technology is capable of achieving. Prejudice and\\r\\nsciolism lead to either demonization of progress or to blind acknowledgment, both calling for educational work. We as Deutsche Telekom feel\\r\\nresponsible to enlighten people and help society to deal with the digital shift, so that new appropriate skills can be developed and new jobs can be\\r\\ntaken over. And we start from within – by enabling our colleagues and employees. But we are aware that this task cannot be solved by one\\r\\ncompany alone. Therefore we will engage in partnerships with other companies, offer our know-how to policy makers and education providers to\\r\\njointly tackle the challenges ahead. ',\n",
              " 'Telefónica´s Approach to the\\r\\nResponsible Use of AI\\r\\nTELEFONICA, S.A.\\r\\nIntroduction\\r\\nArtificial Intelligence (AI) is on the rise. It can be applied to many different domains such\\r\\nas content recommendations, chatbots, image recognition, machine translation, fraud\\r\\ndetection, medical diagnosis, autonomous vehicles, legal, education, transport, and\\r\\nlogistics to name just a few. It can not only be used for business, but also for social\\r\\npurposes such as better understanding and reducing the impact of climate change,\\r\\nnatural disasters, and migration. Also, in Telefonica, AI and Big Data are used\\r\\nincreasingly. There are four main areas of application: i) optimization of core business;\\r\\nii) innovation in the customer relationship using cognitive technologies for digital\\r\\nassistants in apps, webs, call centres, shops, etc; iii) offering AI and Big Data services\\r\\nto business customers through Telefónica’s B2B area; and iv) using AI and Big Data for\\r\\nsocial good such as the fight against COVID-19 and contributing to the Sustainable\\r\\nDevelopment Goals. The last two areas are always based on anonymized and\\r\\naggregated data.\\r\\nHowever, recently several concerns have been expressed about the use of AI, in\\r\\nparticular related to potential discrimination (bias, discrimination, predictive parity), lack\\r\\nof interpretability of algorithmic conclusions (explainability, black box problem), and lack\\r\\nof transparency of personal data used.\\r\\nTo deal with those potential problems, Telefonica published its AI Principles in October\\r\\n2018. This document describes Telefónica´s Responsible Use of AI, which is a part of\\r\\nthe broader Responsibility by Design-Approach of Telefónica. Given that there is still little\\r\\nindustry experience, we will adapt and adjust the approach based on experience and\\r\\nnew external developments.\\r\\nTelefónica´s Approach to the Responsible Use of AI\\r\\nThe approach for the responsible use of AI includes:\\r\\n• a strategic model: the strategic vision on how the responsible use of AI aligns\\r\\nwith wider company objectives\\r\\n• an organizational and relational model, which defines the functions needed,\\r\\naligned with the corporate structure, in addition to the roles & responsibilities, and\\r\\nrelationships\\r\\n• an operating model defining the relevant processes along with roles responsible\\r\\nfor the tasks to be carried out. This includes the Responsible AI by Design\\r\\nmethodology.\\r\\nThe strategic model: Telefonica’s Principles of\\r\\nArtificial Intelligence\\r\\nTelefonica is strongly committed to respecting Human Rights, as is stated in its Business\\r\\nPrinciples and Human Rights Policy. This includes a commitment to developing products\\r\\nand services aimed at making the world a better place to live in and mitigating any\\r\\nnegative impacts technology may have on society or the environment. Technology\\r\\nshould contribute to making society more inclusive and offer better opportunities for all,\\r\\nand AI can contribute to these goals.\\r\\nIn order to guide the organization in its uptake of AI and Big Data across the business,\\r\\nTelefonica has published its “Principles of AI”. The principles include:\\r\\n• Fair AI seeks to ensure that the applications of AI technology lead to fair results.\\r\\nThis means that they should not lead to discriminatory impacts on people in\\r\\nrelation to race, ethnic origin, religion, gender, sexual orientation, disability or any\\r\\nother personal condition. When optimizing a machine learning algorithm, we must\\r\\ntake into account not only the performance in terms of error optimization, but also\\r\\nthe impact of the algorithm in the specific domain.\\r\\n• Transparent and Explainable AI means to be explicit about the kind of personal\\r\\nand/or non-personal data the AI systems uses as well as about the purpose the\\r\\ndata is used for. When people directly interact with an AI system, it should be\\r\\nclear to the users that this is the case. When AI systems take, or support,\\r\\ndecisions, a certain level of understanding of how the conclusions are arrived at\\r\\nneeds to be ensured, by generation explanations about how they reached that\\r\\ndecision, like is illustrated in for the particular case of supervised machine\\r\\nlearning. Those explanations should always consider the user profile to adjust\\r\\nthem to the transparency level required. This also applies in case of using thirdparty AI technology.\\r\\n• Human-centric AI means that AI should be at the service of society and generate\\r\\ntangible benefits for people. AI systems should always stay under human control\\r\\nand be driven by value-based considerations. AI used in products and services\\r\\nshould in no way lead to a negative impact on human rights or the achievement\\r\\nof the UN’s Sustainable Development Goals.\\r\\n• Privacy and Security by Design means that when creating AI systems, which\\r\\nare fueled by data, privacy and security aspects are an inherent part of the\\r\\nsystem’s lifecycle. This maximizes respecting people’s right to privacy and their\\r\\npersonal data. Notice that the data used in AI systems can be personal or\\r\\nanonymous/aggregated. Notice also that this principle is broader applicable than\\r\\nonly to AI systems, and Telefonica already has processes in place to ensure\\r\\nproper privacy and security.\\r\\n• Working with partners and third parties means Telefónica is committed to\\r\\nverifying the logic and data used by the providers to ensure that its principles are\\r\\nrespected.\\r\\nThose Principles are also based on a broad consensus in expert communities, as well\\r\\nas on specific aspects of the telecommunications industry.\\r\\nThere is, however, little published experience on how such principles can be practically\\r\\nimplemented in large organizations such that they have the desired effect. In this sense,\\r\\nTelefonica is making an important step with this approach for the responsible use of\\r\\nArtificial Intelligence.\\r\\nThe organizational and relationship model\\r\\nWe are implementing responsible AI through an organizational and relationship model\\r\\nthat defines what areas of the company are involved, what their roles are and how they\\r\\nrelate to each other for the achievement of a responsible use of AI. \\r\\nWe promote a self-responsibility approach with on-demand escalation. There is a 3 step\\r\\nescalation process as illustrated in the figure below.\\r\\nProduct managers/developers who purchase, develop and/ or use Artificial Intelligence\\r\\nare to carry out a simple self-assessment of the product/service they are developing\\r\\nalready at the design phase through an online questionnaire. This self-assessment\\r\\nexplicitly covers potential human rights risks associated with the use of Artificial\\r\\nIntelligence. This self-assessment will be integrated into a 3 tiered governance model,\\r\\nsupported by a broader Community of Experts (among them a single-point-of-contact for\\r\\nquestions relating to AI & Ethics, the Responsible AI Champion). If a product\\r\\nmanager/developer (level 1) has doubts about a potential adverse impact of a given\\r\\nproduct/service after completing the self-assessment, and this doubt cannot be resolved\\r\\nwith the help of the RAI, she will be automatically directed to a group of predetermined,\\r\\nmultidisciplinary experts within the company (level 2), that together with the product\\r\\nmanager/developer try to solve the issue at hand. In case this issue turns out be a\\r\\npotential risk to the company´s reputation, the matter is elevated to the Responsible\\r\\nBusiness Office which brings together all relevant department directors at global level\\r\\n(level 3).\\r\\nThe operating model\\r\\nThe operating model describes the processes of how to implement the Responsible AI\\r\\nApproach in the organization on a day to day basis. Integrated within the broader\\r\\nResponsibility-by-Design-Approach, it includes a methodology called “Responsible AI by\\r\\nDesign”, inspired by methodologies such as Privacy and Security by Design. The\\r\\noperating model consists of, among others:\\r\\n• Training & awareness activities\\r\\n• The self-assessment on line questionnaire, where each AI principle is\\r\\noperationalized through a set of questions to answer along with\\r\\nrecommendations.\\r\\n• A set of technical tools that helps in answering the questions\\r\\nThe design process of the methodology required a cross-enterprise initiative involving\\r\\ndifferent departments such as Engineering, Corporate Ethics & Sustainability, Security, \\r\\nLegal, Business, Human Resources, Procurement, as well as an endorsement of top\\r\\nmanagement.\\r\\nTraining & awareness\\r\\nWhile Artificial Intelligence is still a rather new technology used in large organizations,\\r\\nthe ethical and societal impacts are even more recent. Therefore, it is of utmost\\r\\nimportance to explain to employees what AI is, how it works and how it might lead to\\r\\nundesired consequences.\\r\\nTelefónica has therefore developed courses related to AI & Ethics that are accessible to\\r\\nall employees through the standard corporate portals in three languages (Spanish,\\r\\nEnglish and Portuguese). Depending on the profile of the employee he or she can access\\r\\na light version of the course which is divided in three modules of each 10 min, or a more\\r\\nprofound version which takes about 1 hour to complete.\\r\\nThe full course is divided into 6 modules as illustrated in the figure below:\\r\\nApart from online courses, each business unit using AI is participating in dedicated\\r\\nworkshops explaining the governance model. Moreover, there is a mini-guide available\\r\\non the intranet where employees can get a quick overview of how to apply the AI\\r\\nPrinciples for ensuring a responsible use of AI.\\r\\nQuestionnaire with recommendations\\r\\nFor all products and services that use AI or Big Data, the responsible manager needs to\\r\\ncomplete the self-assessment questionnaire where for each principle, several questions\\r\\nmust be answered. The questionnaire is available online in Spanish and English and\\r\\nintegrated in the global “Responsibility by Design” initiative of Telefónica Group. All\\r\\ncompleted questionnaires are logged for inspection, statistics and actions if so required. \\r\\nManagers who have completed the questionnaire will receive an email with their\\r\\ncompleted questionnaire, a set of recommendations where appropriate, and an\\r\\nindication of issues to resolve or revisit later.\\r\\nThe designing of the methodology has involved a cross-enterprise initiative have\\r\\ninvolved different departments such as Engineering, Corporate Ethics & Sustainability,\\r\\nSecurity, Legal, Business, Human Resources, Procurement, as well as an endorsement\\r\\nof top management.\\r\\nThe questionnaire consists of 12 questions covering all AI Principles. An example can\\r\\nbe seen in the figure below:\\r\\nIt is important to notice that while privacy & security are part of the AI Principles of\\r\\nTelefónica, the company has specific dedicated processes and areas taking care of\\r\\nprivacy and security. Indeed, privacy and security are relevant for any digital system, and\\r\\nnot only for AI and Big Data applications. Therefore, regarding the principle on Privacy\\r\\n& Security, the questionnaire refers to the respective areas of the company: the DPO\\r\\nand the CISO.\\r\\nTechnical tools\\r\\nBecause some of the questions of the questionnaire are impossible to answer without\\r\\nspecific tools, our methodology includes both in-house tools and external tools (mostly\\r\\nopen source). Some of the in-house tools relate to privacy-enhancing technologies such\\r\\nas anonymization, a tool to detect algorithmic discrimination against protected groups,\\r\\nand a personal data transparency tool. We are constantly evaluating new external tools,\\r\\nespecially related to bias/discrimination and explainability. This is a very active area of\\r\\nresearch with new tools appearing at a rapid pace. It is the responsibility of the\\r\\nCommunity of Experts to study and recommend the inclusion of new tools. \\r\\nFurther Telefónica references\\r\\nTowards organizational guidelines for the responsible use of AI, Richard\\r\\nBenjamins, 24th European Conference on Artificial Intelligence - ECAI 2020 Santiago de Compostela, Spain, https://ecai2020.eu/papers/1347_paper.pdf\\r\\nResponsible AI by Design in Practice, Richard Benjamins, Alberto Barbado, Daniel Sierra, Proceedings of the Human-Centered AI: Trustworthiness of AI Models & Data\\r\\n(HAI) track at AAAI Fall Symposium, DC, November 7-9, 2019,\\r\\nhttps://arxiv.org/html/2001.05375, https://arxiv.org/abs/1909.12838\\r\\nExplainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and\\r\\nchallenges toward responsible AI, Alejandro Barreto Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio\\r\\nGil-López, Daniel Molina, Richard Benjamins, Raja Chatila, Francisco Herrera, Information Fusion, Volume 58, June 2020, Pages 82-115.\\r\\nTowards a framework for understanding societal and ethical implications of\\r\\nArtificial Intelligence, Richard Benjamins, Idoia Salazar, In vulnerabilidad y cultura\\r\\ndigital. Riesgos y oportunidades de la sociedad hiperconectada, Dykinson, pages:\\r\\n2020, https://www.dykinson.com/libros/vulnerabilidad-y-cultura-digital-riesgos-yoportunidades-de-la-sociedad-hiperconectada/9788413246475/',\n",
              " 'Vodafone Group Plc\\r\\nArtificial\\r\\nIntelligence\\r\\nFramework \\r\\nVodafone Group Plc AI Framework 2019\\r\\nIntroduction\\r\\nVodafone Group’s Artificial Intelligence\\r\\n(AI) Framework sets out our approach\\r\\nto working with AI technologies and\\r\\noutlines how we intend to develop\\r\\nand employ it in a responsible manner\\r\\nacross our international business;\\r\\nthis also applies to the standards we\\r\\nexpect from third parties developing\\r\\nAI systems in collaboration with and\\r\\non behalf of Vodafone. We define\\r\\nAI as the application of advanced\\r\\nanalytical techniques (such as\\r\\nMachine Learning, and Natural\\r\\nLanguage Processing) combined with\\r\\nautomation to solve problems, develop\\r\\npersonalised products and services\\r\\nand seize opportunities in new ways.\\r\\nAt Vodafone, we are using AI to help to improve\\r\\nour products and services and to run our\\r\\nbusiness as effectively as possible. For example: • AI-powered chat bots increase the speed with\\r\\nwhich customer enquiries can be resolved;\\r\\n• AI techniques in our networks are used to\\r\\nidentify where capacity is needed so that our\\r\\ncustomers can enjoy optimised data services,\\r\\n such as high quality video streaming;\\r\\n• Vodafone employees use AI tools and software\\r\\nto help them work more efficiently; and\\r\\n• We increasingly use AI to help support good\\r\\ndecision-making, utilising ‘big data’ analysis\\r\\nbased on large, anonymised data sets.\\r\\nAs AI grows in usage and impact across\\r\\ngeographies and industries, Vodafone has\\r\\na responsibility to consider how our use of\\r\\nthis technology affects our customers, our\\r\\nemployees, and wider society. We believe it\\r\\nis critical to ensure that the AI technologies\\r\\nwe create and employ are designed to respect\\r\\nthe privacy and security of the end user’s\\r\\ndata and their associated fundamental rights.\\r\\nThe customer data we use is pseudonymised\\r\\nand permissioned. We will also seek to make\\r\\nAI-driven decisions that are fair and free of any\\r\\nharmful bias. \\r\\nVodafone Group Plc AI Framework 2019 3\\r\\nVodafone’s AI Framework\\r\\nWe endeavour to develop\\r\\nAI in an ethical way so\\r\\nthat it can be trusted.\\r\\nWe endeavour to clearly\\r\\ninform our customers and\\r\\nemployees when they\\r\\ncommunicate directly\\r\\nwith AI-powered systems.\\r\\nState-of-the-art AI-based systems leverage large anonymised or pseudonymised data sets. It is\\r\\ncritical that the outputs from these data-driven systems do not inadvertently guide us to make\\r\\ndecisions that may affect any group or individual in an unfair way. Vodafone will strive to ensure\\r\\nthat there is effective oversight and a ‘human-in-control’ approach to the use of AI. Beyond that,\\r\\nit is also important that Vodafone contributes to the debate about how this technology affects\\r\\nthe societies we live in and is made as widely available as possible.\\r\\nAI-based systems, like intelligent chat bots, have the ability to seem increasingly human in\\r\\ntheir responses. We believe that people should be informed about when they interact with an\\r\\nalgorithm or some form of AI/non-human system.\\r\\nAll our AI experts and data\\r\\nscientists are subject to our\\r\\nCode of Conduct, which\\r\\nincludes explicit provisions\\r\\nfor nondiscrimination and fair\\r\\ntreatment.\\r\\nA non-human system that\\r\\nacts as the primary interface\\r\\nfor Vodafone customers or\\r\\nemployees (for example,\\r\\ncustomer care chat bots) will\\r\\nidentify itself as a non-human\\r\\noperator.\\r\\nTransparency and Accountability Ethics and Fairness\\r\\nVodafone will proactively\\r\\nparticipate in the scientific\\r\\ncommunity, industry coalitions\\r\\nand self-regulatory bodies\\r\\nworking on research, laws,\\r\\nregulations and ethical\\r\\nguidelines for AI, such as the EU\\r\\nHigh Level Expert Group on AI.\\r\\nWe strive to clearly inform our\\r\\ncustomers and employees\\r\\nabout what data we collect on\\r\\nour users and how our systems\\r\\nutilise that data.\\r\\nVodafone AI systems should\\r\\nempower human beings,\\r\\nenabling them to make\\r\\ninformed decisions.\\r\\nVodafone will seek to reduce\\r\\nany digital divide that occurs\\r\\nin our markets because of\\r\\ndifferential access to AI-based\\r\\ntechnologies. That will include\\r\\nbuilding capacity for greater\\r\\nAI usage through awarenessraising and educational\\r\\npartnerships. \\r\\nVodafone Group Plc AI Framework 2019 4\\r\\nWe will ensure that we\\r\\nrespect international\\r\\nhuman rights standards\\r\\nand best practice\\r\\naround ensuring AI\\r\\nsystems foster diversity,\\r\\naccessibility and\\r\\ninclusivity.\\r\\nWe endeavour to respect\\r\\nthe privacy and protect\\r\\nthe security of all\\r\\nindividuals served\\r\\nby the AI we develop.\\r\\nVodafone upholds international human rights across its business footprint. Many of the issues\\r\\nthat arise in the context of AI and human rights are not novel, but are exacerbated by the\\r\\nscale, proliferation, and real-life impact that AI facilitates. Because of this, the potential of\\r\\nAI to both strengthen and diminish human rights is much greater than in previous waves of\\r\\ntechnological development.\\r\\nWe will act responsibly and in\\r\\naccordance with technology\\r\\nindustry best practice to\\r\\nminimise the risks of our\\r\\nsystems being unlawfully used\\r\\nto the detriment of people’s\\r\\nhuman rights.\\r\\nPreservation of Privacy and Security Human Rights, Diversity and Inclusivity\\r\\nVodafone will proactively\\r\\nengage with industry peers\\r\\nand other relevant experts (e.g.\\r\\nacademics and civil society)\\r\\nin consultation exercises to\\r\\nensure that AI-based systems\\r\\nare human-centric and foster\\r\\ndiversity and inclusivity.\\r\\nWe will establish and maintain\\r\\na regular dialogue with our\\r\\ncustomers as to how we\\r\\nshould use data and AI tools\\r\\nto serve them.\\r\\nCustomer trust is our number one priority. Respecting the privacy of our customers is essential\\r\\nto maintaining their trust in our business. Managing privacy risks effectively, including securing\\r\\nour network and putting customers in control of their data, is core to our approach.\\r\\nWe will ensure that customer\\r\\ndata is carefully managed, in\\r\\nline with our strong privacy\\r\\ncommitments and prevailing\\r\\nlegislation and only used with\\r\\nAI systems when we have\\r\\nestablished a clear legal basis\\r\\nto do so.\\r\\nOur customer data will always\\r\\nbe securely stored under\\r\\nstrict access control and in\\r\\ncompliance with our Group data\\r\\nsecurity policies and applicable\\r\\nlocal laws.\\r\\nWe will make sure that our\\r\\nsecurity systems for AI are\\r\\ncontinually updated as the\\r\\nthreat landscape evolves. \\r\\nVodafone Group Plc AI Framework 2019 5\\r\\nVodafone is a responsible\\r\\nemployer and is\\r\\ndetermined to become\\r\\na leading, human-centric,\\r\\ndigital business.\\r\\nOver time, we expect AI to automate an increasing number of routine tasks, enabling our\\r\\nemployees to spend more time on higher value and rewarding activities, including the\\r\\ninnovation needed to underpin sustainable business growth into the future.\\r\\nWe will support our existing\\r\\nemployees to gain new skills\\r\\nso that they can apply for\\r\\nappropriate roles that are\\r\\ncreated by our improved\\r\\ndigital capability.\\r\\nMaximising the Benefits of AI While Managing\\r\\nthe Disruption of its Implementation\\r\\nWe will deploy AI-based\\r\\nsystems to provide more\\r\\neffective work environments,\\r\\nsimplifying the work of our\\r\\nemployees and improving their\\r\\nexperience. We will provide\\r\\ntraining and education to help\\r\\nour employees use AI-based\\r\\nsystems to support their work.\\r\\nWe will strive to ensure that our\\r\\nAI teams – in line with all other\\r\\nteams in Vodafone – are diverse.\\r\\n“ AI is not an end in itself, but rather a promising\\r\\nmeans to increase human flourishing, thereby\\r\\nenhancing individual and societal well-being and\\r\\nthe common good, as well as bringing progress\\r\\nand innovation.”\\r\\nEthics Guidelines for Trustworthy AI,\\r\\nEuropean Commission\\r\\nVodafone Group Plc AI Framework 2019\\r\\n6\\r\\nRight of redress\\r\\nAnyone who feels they have been unfairly treated\\r\\nas a result of a decision made by an AI system\\r\\ndeployed by Vodafone will have the opportunity\\r\\nto escalate their concerns under the published\\r\\nprocess for Vodafone complaints in their country\\r\\nof operation.\\r\\nUpdates to this document\\r\\nVodafone will regularly review and update\\r\\nthis Framework, in light of new products and\\r\\ntechnological developments. As the Framework\\r\\nevolves over time, we pledge to report on\\r\\nkey learnings and observations regarding the\\r\\ndevelopment of ethical AI and the dissemination\\r\\nof these Framework commitments across\\r\\nthe business. \\r\\n© 2019 Vodafone Group Plc\\r\\nRegistered Office:\\r\\nVodafone House\\r\\nThe Connection\\r\\nNewbury\\r\\nBerkshire\\r\\nRG14 2FN\\r\\nRegistered in England No. 1833679',\n",
              " \"1. Realizing Well-being and Sustainability of Society\\r\\nNTT DATA will focus on sustainability as well as well-being of human society. NTT DATA will promote solving social issues by using AI in recognition of diversity and in consideration with basic human rights. AI should not harm humans; instead, we will use AI to empower humans by supporting their lives and extending their capabilities.\\r\\n\\r\\n2. Co-Creating New Values by AI\\r\\nNTT DATA strives to harness the potential benefits of AI and promote innovation by interacting and co-creating with AI stakeholders, including those who engage in research, development, operation, and utilization of AI and by establishing long-term relationships with clients.\\r\\n\\r\\n3. Fair, Reliable, and Explainable AI\\r\\nFor users of AI to realize its fairness and trustworthiness, NTT DATA invests reasonable efforts in presenting the decisions based on AI in a form that can be understood by humans and doesn’t cause unfair discrimination. We will strive to minimize concerns about the AI's decision-making process to apply constant provision of appropriate AI services.\\r\\n\\r\\n4. Data Protection\\r\\nPrivacy and security is critical to NTT DATA in providing AI services. Our AI leverages the principles of security and design to protect against unauthorized access, strengthen data traceability, and develop the proper means and policy for collecting, storing, using, and providing personal information.\\r\\n\\r\\n5. Contribution to Dissemination of Sound AI\\r\\nNTT DATA will work to further the understanding of AI in order to accelerate its acceptance. NTT DATA will also contribute to the development of society with healthy and harmonious dissemination of AI by improving literacy of AI users and by service design that is universal design compliant.\",\n",
              " \"1. Principle of Human-Centeredness\\r\\nWe believe that AI exists to make people happy and should be used only as a decision-making aid for people who use it to solve social problems. We aim to create AI that makes people happier through its use.\\r\\n2. Respect for Fairness\\r\\nFair AI cannot be realized unless we humans have fair values. We respect all people regardless of race, nationality, age, religion, gender, etc. We will develop, design, provide, and use AI based on the same principle, paying attention to the existence of social minorities, such as women, racial minorities, and sexual minorities, which are not readily apparent in commonly distributed data. We aim to realize an inclusive society through technology.\\r\\n3. Pursuit of Transparency and Accountability\\r\\nIn order to coexist and develop together with AI, it is important to create an environment in which people can understand the results of AI-based judgments and verify the basis of those judgments when necessary. We will strive for transparency in explaining how we use the results of AI judgments. We aim to develop and use AI with verifiability regarding the reliability of the AI and a high level of accountability regarding the basis for the judgments.\\r\\n4. Ensuring Safety\\r\\nWe believe that AI that contributes to people's happiness can only be realized with a high level of safety. We shall strive to design and develop AI that does not threaten the life, liberty, dignity, or property of individuals, and to create a world in which all people can use AI safely.\\r\\n5. Privacy Protection and Security\\r\\nData is indispensable for the design, development, and use of AI. Without a large amount of accurate data to educate AI, its development and contribution to society cannot be realized. That is why we will appropriately manage and operate various types of data, including personal data, in accordance with internal rules, relevant laws and regulations, and social ethics to prevent unauthorized access, etc., and protect privacy so that people can use AI with peace of mind.\\r\\n6. Development of AI Human resources and Literacy\\r\\nWe have been focusing on the development of advanced technology. While technology is advancing by leaps and bounds, the people who utilize it must also be constantly evolving and capable of handling technology responsibly. In order to realize a society in which everyone can enjoy the benefits of AI to the fullest, we will actively engage in education and training to enhance the skills and literacy of both the developers and users of AI.\",\n",
              " 'Responsibility principles\\r\\nModel the world as we hope it will become.\\r\\n\\r\\nAnticipate risks and listen to those affected.\\r\\n\\r\\nBuild in mitigation efforts commensurate with expected and actual impacts.\\r\\n\\r\\nContinually assess the societal impacts of our work.\\r\\n\\r\\nOur process\\r\\nWe believe that no technology can be made absolutely safe; machine learning is no exception. This requires anticipating and accounting for risks during our development process. We run adversarial attacks, filter our training data for harmful text, and measure our models against safety research benchmarks. We also evaluate evolving risks with monitoring tools designed to identify harmful model outputs.\\r\\n\\r\\nWe recognize that misuse of powerful language models will disproportionately impact the most vulnerable, so we aim to balance safety considerations and equity of access. This is an ongoing process. As we release early versions of our technology, we’ll work closely with our partners and users to ensure its safe and responsible use.\\r\\n\\r\\nAccountability\\r\\nResponsibility means more than getting the technology right. It’s about who makes the decisions, communicating risks upfront, and the way we’re held accountable. To help API users anticipate risks, we have published data statements (public documentation about the data used to train our models) and model cards (benchmarks and information about where our technology is and is not working well) for our datasets and models in our documentation here.\\r\\n\\r\\nWe believe that good decisions don’t happen in a vacuum. Responsibility must be baked into the culture and norms of the machine learning ecosystem, and we’re committed to sharing knowledge and best practices. We’ll be supporting workshops around responsible use—if you’re developing language models or working to mitigate their risks, please reach out.\\r\\n\\r\\nOur advisory Responsibility Council is made up of external experts who will advise our business and research practices. They have visibility into our customer development pipeline and engineering processes, and agency to ask difficult questions around the permissible uses of our technologies.\\r\\n\\r\\nUsage Guidelines\\r\\nWe require our users to abide by Cohere’s Usage Guidelines. Access will be revoked if these terms are not followed. If you spot the Cohere Platform being used in a harmful or otherwise unproductive way, please report it to us.',\n",
              " 'The gold standard of security\\r\\nBuilding responsibility into our AI means implementing robust data privacy\\r\\nprotections across our entire AI platform, so that our customers’ information is\\r\\nalways secure.\\r\\nMoveworks meets the highest security standards for an enterprise SaaS company.\\r\\nWe’re ISO 27001 certified, we’re compliant with SOC 2 Type 1 and Type 2, and we\\r\\nachieve “Gold” status for CSA STAR Level 2, which is given only to organizations\\r\\nwith the most mature cloud security postures. Indeed, security is ingrained into\\r\\nevery aspect of our business:\\r\\n• Our people are trained to keep pace with the latest security best practices\\r\\n• Our processes are continuously tested to meet stringent privacy requirements\\r\\n• Our product is designed with a DevSecOps approach across the board\\r\\nTo learn more, please review our dedicated Security webpage >\\r\\n© Moveworks, Inc www.moveworks.com\\r\\n“Trust, security, and responsibility\\r\\nare at the core of our business\\r\\nat DocuSign. We only evaluate\\r\\nvendors who share those same\\r\\nvalues, and that’s what gave us\\r\\nthe confidence to partner with\\r\\nMoveworks.”\\r\\n—Saran Mandair,\\r\\n VP of Global IT, DocuSign\\r\\nOur Approach to\\r\\nResponsible AI\\r\\nMoveworks is deeply committed to the practice of responsible\\r\\nAI. This datasheet summarizes how we protect our customers’\\r\\ndata and mitigate the risk of bias. \\r\\nResponsible AI also requires ensuring our machine learning\\r\\n(ML) process is as unbiased as possible. That’s why Moveworks\\r\\nactively minimizes or eliminates potential sources of bias in our ML\\r\\nmodels with respect to all protected classes, including race, age,\\r\\ndisability, color, national origin, religion, sexual orientation, gender\\r\\nidentification, and genetic information.\\r\\nAt its core, machine learning is about extracting the signal from the\\r\\nnoise—taking the relevant features from an input, such as a piece\\r\\nof text, and making a prediction using patterns that an ML model\\r\\nhas learned from similar data. The bias and equity challenge for ML\\r\\ndevelopers is to ensure that the signals extracted from the noise\\r\\nare appropriate inferences to make based on the input, rather than\\r\\nreplications of implicit or explicit biases that exist in the training\\r\\ndata.\\r\\nMoveworks is well aware of these challenges; we believe we have\\r\\ntaken appropriate measures to minimize or eliminate sources of\\r\\nbias. Given the problem that our platform solves, Moveworks has a\\r\\nlower risk of prejudicial bias than other AI companies. For instance,\\r\\nresetting a password or provisioning a Zoom license doesn’t\\r\\nrequire our ML models to know personal information about the\\r\\nuser.\\r\\nHowever, we still take precautions to reduce the risk of bias:\\r\\n1. Moveworks’ ML models are trained primarily on data\\r\\nsampled from production usage. Using a technique known\\r\\nas Collective Learning, many of our models are trained on\\r\\nanonymized data drawn from multiple customers, allowing\\r\\nthem to learn the universal structure of requests from\\r\\nemployees with different backgrounds and characteristics.\\r\\n2. We annotate this data without exposing any user\\r\\ncharacteristics to the annotator: no names, photos, or other\\r\\ncategory-identifying features are included in the annotation\\r\\ninterface. When annotating the intention of a request, for\\r\\nexample, the annotator only sees the text of the message and\\r\\nthe name of the organization.\\r\\n3. During training, we do not include protected attributes, such\\r\\nas gender or race, in the inputs from which the models learn\\r\\nto derive signals.\\r\\n4. For the vast majority of requests sent to our bot, it would be\\r\\nextremely difficult, if not impossible, for a human annotator\\r\\nto guess any protected categories about the knowledge\\r\\nworker from the text of the request—and harder still for our ML\\r\\nmodels. This means the models are much less likely to learn\\r\\nan intermediate representation associated with a protected\\r\\nclass.\\r\\nAll of these measures reinforce our commitment to responsible\\r\\nAI at Moveworks, from securing our platform to ensuring data\\r\\nprivacy to eliminating sources of bias.\\r\\nEliminating sources of bias\\r\\n© Moveworks, Inc www.moveworks.com\\r\\nmoveworks.com/request-demo\\r\\nRequest a demo',\n",
              " 'AI Code of Conduct\\r\\nCONTENTS\\r\\nBCG Purpose + Responsible AI\\r\\nLiving Our Principles\\r\\nOur Promise to Act\\r\\nFuture Proofing Responsible AI\\r\\nBoston Consulting Group partners with leaders in business\\r\\nand society to tackle their most important challenges and\\r\\ncapture their greatest opportunities. BCG was the pioneer\\r\\nin business strategy when it was founded in 1963. Today,\\r\\nwe work closely with clients to embrace a transformational\\r\\napproach aimed at benefiting all stakeholders—empow\\r\\n-\\r\\nering organizations to grow, build sustainable competitive\\r\\nadvantage, and drive positive societal impact.\\r\\nOur diverse, global teams bring deep industry and function\\r\\n-\\r\\nal expertise and a range of perspectives that question the\\r\\nstatus quo and spark change. BCG delivers solutions through\\r\\nleading-edge management consulting, technology and\\r\\ndesign, and corporate and digital ventures. We work in a\\r\\nuniquely collaborative model across the firm and through\\r\\n-\\r\\nout all levels of the client organization, fueled by the goal of\\r\\nhelping our clients thrive and enabling them to make the\\r\\nworld a better place.\\r\\nA Message from Our CEO\\r\\nDear colleagues and friends of BCG,\\r\\nAs you know, AI and data are playing an increasingly vital role in our clients’ businesses. Clients\\r\\ntrust us to help them transform their organizations and industries through AI. And though\\r\\nAI presents another exciting avenue for us to\\r\\nlive our Purpose—to unlock the potential of\\r\\nthose who advance the world—it comes with\\r\\nrisks. We have an obligation to ensure that the\\r\\nAI solutions we create deliver transformative\\r\\nimpact without inadvertently harming people\\r\\nor communities or compromising BCG’s values.\\r\\nThis is a time for BCG to lead with integrity. We\\r\\nhave a duty to ourselves, our clients, and society to proactively ensure that the responsible\\r\\nuse of AI is core to our approach. This Code of\\r\\nConduct outlines what we’re already doing, as\\r\\nwell as our promise for the future: to continue to\\r\\nresponsibly design, develop, and deploy AI systems around the world.\\r\\nThank you for your continued support and engagement. I look forward to advancing our clients and the world through AI – responsibly.\\r\\nChristoph Schweizer\\r\\nWe have an obligation to ensure that the AI\\r\\nsolutions we create deliver transformative\\r\\nimpact without inadvertently harming\\r\\npeople or communities or compromising\\r\\nBCG’s values.\\r\\n– Christoph Schweizer, CEO\\r\\n5 AI CODE OF CONDUCT BOSTON CONSULTING GROUP 6\\r\\nBCG Purpose + Responsible AI\\r\\nIn the constantly changing field of AI, with its potential benefits and mounting risks, it’s vital that we anchor ourselves to our Purpose: to unlock the potential\\r\\nof those who advance the world.\\r\\nOur Purpose is underpinned by five pillars that capture the core facets of who we are as an organization.\\r\\nThey guide how we interact with each other, our clients, and the world, including our approach to designing, developing, deploying, and using AI. Our\\r\\nResponsible AI approach is anchored to our timeless Purpose Principles, which capture our distinctive strengths: Bring Insight to Light, Drive Inspired\\r\\nImpact, Conquer Complexity, Lead with Integrity,\\r\\nand Grow by Growing Others.\\r\\nBRING INSIGHT TO LIGHT\\r\\nWe make the unknowns of AI known, illuminating all possible outcomes of a system before and\\r\\nduring the design process. We drive transparency by explaining how an AI algorithm operates,\\r\\nwhat data it uses, how it decides, and what its intentions are.\\r\\nExample:\\r\\nBCG developed FACET, an open-source library\\r\\nfor explainable AI to support exploration and\\r\\nunderstanding of supervised machine-learning\\r\\nmodels. FACET breaks down feature interactions\\r\\ninto three key components: redundancy, synergy, and independence. By helping developers\\r\\nand business users understand how algorithms\\r\\nanalyze the data sets on which AI predictions are\\r\\nbased, FACET reestablishes human control over\\r\\nand trust in AI.\\r\\nDRIVE INSPIRED IMPACT\\r\\nWe employ AI boldly and proactively to create\\r\\ntransformative business impact, but with protections to ensure that it doesn’t harm society.\\r\\nBy creating safeguards to protect data and reduce unintended behaviors and outcomes, we\\r\\ncan be confident that the systems we devise are\\r\\nsecure, equitable, and fair.\\r\\nExample:\\r\\nBCG worked with a brick-and-mortar retailer to optimize the footprint of its locations.\\r\\nWe helped the client think beyond just cost\\r\\nand revenue forecasts to consider the socioeconomic diversity of the neighborhoods\\r\\nwhere its stores would be opened or closed.\\r\\nThe approach helped our client live up to the\\r\\nDEI commitments it had made to shareholders\\r\\nand customers.\\r\\nCONQUER COMPLEXITY\\r\\nWe bring to light the possibilities and risks\\r\\nthat machine learning presents. Our task is to\\r\\ndesign elegant AI systems to solve problems,\\r\\ncreate value, and minimize the environmental impact of their operation and the outcomes\\r\\nthey produce.\\r\\nExample:\\r\\nBCG worked with a global fashion retailer to retool its demand forecasting system in\\r\\na way that enabled human control while leveraging AI. We created a system which AI\\r\\nmade forecasts that humans could then modify based on emerging fashion trends. The\\r\\ncompany was rewarded with increased sales,\\r\\nreduced costs, less waste, and a smaller\\r\\ncarbon footprint.\\r\\n7 AI CODE OF CONDUCT BOSTON CONSULTING GROUP 8\\r\\nLEAD WITH INTEGRITY\\r\\nWe seek to set an ethical standard in our indus\\r\\n-\\r\\ntry. As a champion of accountability, we openly\\r\\ndiscuss risks with our clients. We decline projects\\r\\nthat conflict with our values and principles, we\\r\\nprotect data and personal privacy, and we take\\r\\nresponsibility for what we make and what our\\r\\nsystems make happen.\\r\\nExample:\\r\\nBCG worked with a payroll and HR services pro\\r\\n-\\r\\nvider to deploy and test methods for bias de\\r\\n-\\r\\ntection in its sales lead generation process. We\\r\\nhelped develop a B2B decision support system\\r\\nto identify sources of bias related to the demo\\r\\n-\\r\\ngraphic makeup of potential customers. We\\r\\nworked with the client to surface biases encod\\r\\n-\\r\\ned in the historical data, designed the system to\\r\\nminimize those biases, and developed an internal white paper for company leadership to educate business stakeholders and data scientists.\\r\\nGROW BY GROWING OTHERS\\r\\nWe grow our clients’ businesses by empowering\\r\\nleaders to make the right economic and ethi\\r\\n-\\r\\ncal decisions. This means finding solutions that\\r\\naugment or advance both machine learning and\\r\\nhuman progress.\\r\\nExample:\\r\\nBCG worked with Microsoft to develop a set of in\\r\\n-\\r\\ndustry-leading guidelines for product leaders to\\r\\nhelp implement AI responsibly. The resource pro\\r\\n-\\r\\nvides a clear, actionable framework for leaders to\\r\\nguide product teams in assessing, designing, and\\r\\nvalidating Responsible AI (RAI) systems within\\r\\ntheir organizations.\\r\\n9 AI CODE OF CONDUCT 10\\r\\nApplying our Responsible AI Principles requires investing in the\\r\\npeople, processes, and technology that touch every aspect of our\\r\\norganization.\\r\\nOUR PRODUCTS\\r\\nWe conduct early risk assessments to ensure that projects align with our values. We also offer BCG’s RATE.ai\\r\\nResponsible AI algorithmic impact assessment to help product managers guide their teams toward proactively identifying and addressing risks.\\r\\nOur Delivery Excellence program provides added layers of independent oversight to help us evaluate risk, develop mitigations, review execution for quality, and, when needed, escalate for senior leader alignment.\\r\\nOUR PEOPLE\\r\\nTo empower our teams to act on our Responsible AI Principles,\\r\\nraise concerns, and take responsibility for all that we build,\\r\\nwe offer a range of RAI training.\\r\\nWe’ve developed a comprehensive set of interactive technical tutorials and open-source software to support our technical teams and ensure implementation of best practices.\\r\\nWe’ve also expanded the role of our ombudspersons to allow\\r\\nstaff to easily raise RAI concerns.\\r\\nOUR LEADERS\\r\\nA multidisciplinary Responsible AI Council composed of global senior leaders monitors all RAI implementation. Officers\\r\\nleading product teams evaluate AI as a distinct category of\\r\\nrisk, and teams have clear escalation paths to firm leadership. To complement these measures, we give leaders dedicated training to build and maintain their RAI awareness.\\r\\nOUR CLIENTS\\r\\nTo aid clients on their RAI journey, we enable their product\\r\\nteams with open-source tools like FACET and Code Carbon\\r\\nand frameworks such as Ten Guidelines for Product Leaders\\r\\nto Implement AI Responsibly. In addition to publishing our\\r\\ninsights on RAI, we share our expertise in a variety of industry forums, including the Business Roundtable and the World\\r\\nEconomic Forum. Our goal is to encourage and champion\\r\\nbroad RAI adoption.\\r\\nLiving Our\\r\\nPrinciples\\r\\n12\\r\\nResponsible AI is about more than building superior\\r\\ntechnical solutions; it’s about earning the trust of our\\r\\nemployees, leaders, clients, and society. We’re com\\r\\n-\\r\\nmitted to providing transparency and holding ourselves accountable to our promise of Responsible AI\\r\\nthrough the following actions.\\r\\nINTENDED USE\\r\\nWe’ll partner with clients to clearly define the in\\r\\n-\\r\\ntended use for every AI product.\\r\\nTRANSPARENCY\\r\\nWe’ll make available to our clients the output\\r\\nfrom our algorithmic impact assessments.\\r\\nDOCUMENTATION\\r\\nWe’ll share AI product documentation (e.g.,\\r\\nmodel cards, data provenance) with our clients,\\r\\nincluding RAI risks and mitigations.\\r\\nREGULAR REPORTING\\r\\nWe’ll ensure that our Responsible AI Council\\r\\ngenerates an annual report on our RAI program.\\r\\nENABLEMENT\\r\\nWe’ll empower our clients by continuing to build\\r\\ntools, frameworks, and other artifacts that they\\r\\ncan use to implement Responsible AI practices.\\r\\nCOMMUNITY ENGAGEMENT\\r\\nWe’ll continue to proactively engage the busi\\r\\n-\\r\\nness and AI ecosystem by sharing our knowl\\r\\n-\\r\\nedge, encouraging others to act, and advancing\\r\\nResponsible AI before challenges arise.\\r\\nOur Promise to\\r\\nAct\\r\\nResponsible AI obligations do not end when an AI\\r\\nproduct is deployed. Issues can emerge when input\\r\\ndata evolves, source code is modified, or underlying\\r\\nsoftware packages or models are updated. Core to\\r\\nour approach is enabling our clients’ monitoring re\\r\\n-\\r\\ngimes to ensure that emergent issues are rapidly iden\\r\\n-\\r\\ntified and addressed.\\r\\nIn addition, Responsible AI approaches can\\r\\n-\\r\\nnot be static. They must support ongoing gov\\r\\n-\\r\\nernance and monitoring, and they must evolve\\r\\nwith AI techniques, data availability, real-world\\r\\noutcomes, and regulations. We’ve created a re\\r\\n-\\r\\nsponsive, adaptive governance system that\\r\\nscans the environment for cutting-edge re\\r\\n-\\r\\nsearch, breakthrough best practices, and emerg\\r\\n-\\r\\ning risks and opportunities. We then weave\\r\\nthose findings into how we do things. We’re com\\r\\n-\\r\\nmitted to helping others do the same.\\r\\nTo fulfill this commitment, we’ll continue to\\r\\nevolve our practices and ground them in our\\r\\nPurpose: to unlock the potential of those\\r\\nwho advance the world.\\r\\nFuture Proofing\\r\\nResponsible AI\\r\\n13 14\\r\\nTo find the latest BCG content and register to receive e-alerts on this topic or\\r\\nothers, please visit bcg.com.\\r\\nFollow Boston Consulting Group on Facebook and Twitter.\\r\\n© Boston Consulting Group, Inc. 2022. All rights reserved.',\n",
              " \"Deloitte’s Trustworthy AI™ framework\\r\\nWe put trust at the center of everything we do. We use a multidimensional AI framework to help organizations develop ethical safeguards across six key dimensions—a crucial step in managing the risks and capitalizing on the returns associated with artificial intelligence.\\r\\n\\r\\nTrustworthy AI™ requires governance and regulatory compliance throughout the AI lifecycle from ideation to design, development, deployment and machine learning operations (MLOps) anchored on the six dimensions in Deloitte's Trustworthy AI™ framework—transparent and explainable, fair and impartial, robust and reliable, respectful of privacy, safe and secure, and responsible and accountable. At its foundation, AI governance encompasses all the above stages, and is embedded across technology, processes and employee trainings. This includes adhering to applicable regulations, as it prompts risk evaluation, control mechanisms, and overall compliance. Together, governance and compliance are the means by which an organization and its stakeholders ensure AI deployments are ethical and can be trusted.\\r\\nFair and impartial\\r\\nAssess whether AI systems include internal and external checks to help enable equitable application across all participants.\\r\\nTransparent and explainable\\r\\nHelp participants understand how their data can be used and how AI systems make decisions. Algorithms, attributes, and correlations are open to inspection.\\r\\nResponsible and accountable\\r\\nPut an organizational structure and policies in place that can help clearly determine who is responsible for the output of AI system decisions.\\r\\nRobust and Reliable\\r\\nConfirm that AI systems have the ability to learn from humans and other systems and produce consistent and reliable outputs.\\r\\nRespectful of privacy\\r\\nRespect data privacy and avoid using AI to leverage customer data beyond its intended and stated use. Allow customers to opt in and out of sharing their data.\\r\\nSafe and secure\\r\\nProtect AI systems from potential risks (including cyber risks) that may cause physical and digital harm.\",\n",
              " \"PwC’s Responsible AI\\r\\nAI you can trust\\r\\nAI Risks\\r\\nAI algorithms that\\r\\ningest real-world\\r\\ndata and\\r\\npreferences as\\r\\ninputs, run a risk of\\r\\nlearning and\\r\\nimitating our biases\\r\\nand prejudices.\\r\\nFor as long as\\r\\nautomated systems\\r\\nhave existed,\\r\\nhumans have tried\\r\\nto circumvent them.\\r\\nThis is no different\\r\\nwith AI.\\r\\nSimilar to any other\\r\\ntechnology, AI\\r\\nshould have\\r\\norganisation-wide\\r\\noversight with\\r\\nclearly-identified\\r\\nrisks and controls.\\r\\nThe widespread\\r\\nadoption of\\r\\nautomation across\\r\\nall areas of the\\r\\neconomy may\\r\\nimpact jobs and\\r\\nshift demand to\\r\\ndifferent skills.\\r\\nThe widespread\\r\\nadoption of complex\\r\\nand autonomous AI\\r\\nsystems could\\r\\nresult in “echochambers”\\r\\ndeveloping between\\r\\nmachines and\\r\\nhumans.\\r\\nAI solutions are\\r\\ndesigned with\\r\\nspecific objectives in\\r\\nmind which may\\r\\ncompete with\\r\\noverarching\\r\\norganisational and\\r\\nsocietal values within\\r\\nwhich they operate.\\r\\n1. Performance 2. Security 3. Control 4. Economic 5. Societal 6. Ethical\\r\\nAI is here to stay—bringing limitless potential to push us\\r\\nforward as a society. Used wisely, it can create huge\\r\\nbenefits for businesses, governments, and individuals\\r\\nworldwide.\\r\\nHow big is the opportunity? Our research estimates that AI could\\r\\ncontribute $15.7 trillion to the global economy by 2030, as a result of\\r\\nproductivity gains and increased consumer demand driven by AIenhanced products and services. AI solutions are diffusing across\\r\\nindustries and impacting everything from customer service and sales to\\r\\nback office automation. AI’s transformative potential continues to be top\\r\\nof mind for business leaders: Our CEO survey finds that 72% of CEOs\\r\\nbelieve that AI will significantly change the way they do business in the\\r\\nnext five years.\\r\\nWith great potential comes great risk. Are your algorithms making decisions that align with your values? Do customers trust you\\r\\nwith their data? How is your brand affected if you can’t explain how AI systems work? It’s critical to anticipate problems and futureproof your systems so that you can fully realize AI’s potential. It’s a responsibility that falls to all of us — board members, CEOs,\\r\\nbusiness unit heads, and AI specialists alike.\\r\\nPerformance risks\\r\\ninclude:\\r\\n•Risk of errors\\r\\n•Risk of bias\\r\\n•Risk of opaqueness\\r\\n•Risk of instability of\\r\\nperformance\\r\\n•Lack of feedback\\r\\nprocess\\r\\nSecurity risks\\r\\ninclude:\\r\\n•Cyber intrusion\\r\\nrisks\\r\\n•Privacy risks\\r\\n•Open source\\r\\nsoftware risks\\r\\n•Adversarial attacks\\r\\nControl risks\\r\\ninclude:\\r\\n•Risk of AI going\\r\\nrogue\\r\\n•Control malevolent\\r\\nAI\\r\\nEconomic risks\\r\\ninclude:\\r\\n•Risk of job\\r\\ndisplacement\\r\\n•Risk of\\r\\nconcentration of\\r\\npower within 1 or a\\r\\nfew companes\\r\\n•Liability risk\\r\\nSocietal risks\\r\\ninclude:\\r\\n•Risk of autonomous\\r\\nweapons\\r\\nproliferation\\r\\n•Risk of an\\r\\nIntelligence divide\\r\\nEthical risks\\r\\ninclude:\\r\\n•Lack of values risk\\r\\n•Values\\r\\nmisalignment risk\\r\\nOur holistic approach helps you address the five dimensions of Responsible AI\\r\\nInnovate Responsibly\\r\\nWhether you're just getting started or are getting ready to scale, put your trust in Responsible AI. Drawing on our\\r\\nproven capability in AI innovation and deep global business expertise, we'll assess your end-to-end needs, and design\\r\\na solution to address your unique risks and challenges.\\r\\n© 2019 PwC. All rights reserved.\\r\\nPwC refers to the PwC network and/or one or more of its member firms, each of which is a separate legal entity. Please see www.pwc.com/structure for further\\r\\ndetails.\\r\\nThis content is for general information purposes only, and should not be used as a substitute for consultation with professional advisors.\\r\\nLiability limited by a scheme approved under Professional Standards Legislation.\\r\\nAt PwC, our purpose is to build trust in society and solve important problems. We’re a network of firms in 158 countries with more than 236,000 people who are\\r\\ncommitted to delivering quality in assurance, advisory and tax services. Find out more and tell us what matters to you by visiting us at www.pwc.com.\\r\\nWL 127068859\\r\\nOur Ethical AI\\r\\nFramework provides\\r\\nguidance and a\\r\\npractical approach to\\r\\nhelp your organisation\\r\\nwith the development,\\r\\nand governance of AI\\r\\nsolutions that are\\r\\nethical and moral.\\r\\nEthics &\\r\\nRegulation\\r\\nFairness is a social\\r\\nconstruct with many\\r\\ndifferent and, at\\r\\ntimes, conflicting\\r\\ndefinitions.\\r\\nResponsible AI helps\\r\\nyour organisation to\\r\\nbecome more aware\\r\\nof bias, and take\\r\\ncorrective action to\\r\\nhelp systems improve\\r\\nfairness in their\\r\\ndecision-making.\\r\\nRobustness &\\r\\nSecurity\\r\\nThe foundation for\\r\\nResponsible AI is an\\r\\nend-to-end enterprise\\r\\ngovernance\\r\\nframework. This\\r\\nfocuses on the risks\\r\\nand controls at every\\r\\nstep of your\\r\\norganisation’s AI\\r\\njourney, and at every\\r\\nlevel, from the\\r\\nenterprise to the\\r\\nindividual AI model.\\r\\nGovernance\\r\\nAn AI system that\\r\\nhuman users are\\r\\nunable to understand\\r\\ncan lead to a “black\\r\\nbox” effect. We\\r\\nprovide services to\\r\\nhelp you explain both\\r\\noverall decisionmaking and also\\r\\nindividual choices and\\r\\npredictions, tailored to\\r\\nthe perspectives of\\r\\ndifferent stakeholders.\\r\\nInterpretability &\\r\\nExplainability\\r\\nTo help make your\\r\\nsystems more\\r\\nresilient, Responsible\\r\\nAI includes services to\\r\\nidentify weaknesses\\r\\nin models, assess\\r\\nsystem safety and\\r\\nmonitor long-term\\r\\nperformance.\\r\\nBias & Fairness\\r\\nContact us\\r\\nContact us today. Learn more about how to become an industry leader in the responsible use of AI.\\r\\nAnand Rao\\r\\nGlobal & US Artificial Intelligence and US Data\\r\\n& Analytics Leader, PwC US\\r\\nEmail: anand.s.rao@pwc.com\\r\\nFlavio Palaci\\r\\nGlobal Data Analytics & Artificial Intelligence\\r\\nLeader, PwC Australia\\r\\nEmail: flavio.j.palaci@pwc.com\\r\\nPwC’s Responsible AI Toolkit\\r\\nYour stakeholders, including board members, customers, and regulators, will have many questions about your\\r\\norganisation's use of AI and data, from how it’s developed to how it’s governed. You not only need to be ready to\\r\\nprovide the answers, you must also demonstrate ongoing governance and regulatory compliance.\\r\\nOur Responsible AI Toolkit is a suite of customizable frameworks, tools and processes designed to help you harness\\r\\nthe power of AI in an ethical and responsible manner - from strategy through execution. With the Responsible AI\\r\\ntoolkit, we’ll tailor our solutions to address your organisation’s unique business requirements and AI maturity.\",\n",
              " 'ETHICAL AI \\r\\nAs AI becomes more advanced, we pledge that our technology innovation remains fair and free from bias, that data is safe and secure and used appropriately to deliver intelligent solutions that improve lives for the greater good. We may choose not to work with companies who are deemed to operate unethically or do harm to the environment.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embeddings = model.encode(sentences_1)\n",
        "sentence_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkxUiYyo6QsP",
        "outputId": "cfe4ebad-d1d6-479a-89de-97ceb40c6cf5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "cTrHHe6Z6YRr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the similarity results to a list object\n",
        "results = cosine_similarity([sentence_embeddings[0]], sentence_embeddings[1:])\n",
        "results = results.tolist()\n",
        "results = results[0]\n",
        "\n",
        "# Confirm that the list is compationable with the Original Dataset\n",
        "len(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU7BBGrH6ax8",
        "outputId": "dfdc0ee9-719e-4c27-e370-ac6b5897b557"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_data['Principle_1'] = results\n",
        "similarity_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "6AIHCINe6m9W",
        "outputId": "2e098897-7f27-4787-dac9-a7f9abd8dbd5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-fd73c820d365>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  similarity_data['Principle_1'] = results\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Company Name                                          Main Text  Principle_1\n",
              "0    Accenture  Responsible AI\\r\\nFrom principles to practice\\...     0.717751\n",
              "1        Adobe  Adobe’s Commitment to AI Ethics\\r\\nAt Adobe, o...     0.566864\n",
              "2     Alphabet  Responsible AI practices\\r\\nThe development of...     0.630594\n",
              "3       Amazon  Responsible Use of Machine Learning\\r\\nAt AWS,...     0.596675\n",
              "4         Atos  AI is a broad topic encompassing many differen...     0.709803"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be126859-c278-4735-9680-ee6761770ff0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Company Name</th>\n",
              "      <th>Main Text</th>\n",
              "      <th>Principle_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Accenture</td>\n",
              "      <td>Responsible AI\\r\\nFrom principles to practice\\...</td>\n",
              "      <td>0.717751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adobe</td>\n",
              "      <td>Adobe’s Commitment to AI Ethics\\r\\nAt Adobe, o...</td>\n",
              "      <td>0.566864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Alphabet</td>\n",
              "      <td>Responsible AI practices\\r\\nThe development of...</td>\n",
              "      <td>0.630594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Amazon</td>\n",
              "      <td>Responsible Use of Machine Learning\\r\\nAt AWS,...</td>\n",
              "      <td>0.596675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Atos</td>\n",
              "      <td>AI is a broad topic encompassing many differen...</td>\n",
              "      <td>0.709803</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be126859-c278-4735-9680-ee6761770ff0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-be126859-c278-4735-9680-ee6761770ff0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-be126859-c278-4735-9680-ee6761770ff0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principle 2"
      ],
      "metadata": {
        "id": "Rja8xz_FHxHP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3X5JTnHHw1l"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}